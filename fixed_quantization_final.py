#!/usr/bin/env python3
"""
æœ€ç»ˆä¿®å¤çš„é‡åŒ–è„šæœ¬ - çœŸæ­£å‡å°‘æ¨¡å‹å¤§å°å¹¶ä¿®å¤è®¾å¤‡é”™è¯¯

æœ¬è„šæœ¬è§£å†³äº†ä»¥ä¸‹é—®é¢˜ï¼š
1. é‡åŒ–åæ¨¡å‹å¤§å°æ²¡æœ‰å‡å°‘çš„é—®é¢˜
2. CUDAè®¾å¤‡ä¸åŒ¹é…é”™è¯¯
3. å‹ç¼©æ¯”è®¡ç®—é”™è¯¯
"""

import os
import sys
import torch
import logging
import json
import shutil
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

try:
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        BitsAndBytesConfig
    )
    from peft import PeftModel, PeftConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("è­¦å‘Š: transformersæˆ–peftåº“ä¸å¯ç”¨")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FixedQuantizer:
    """ä¿®å¤çš„é‡åŒ–å™¨ - çœŸæ­£å‡å°‘æ¨¡å‹å¤§å°"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def create_truly_compressed_models(
        self,
        source_model_path: str = "quantized_models_output_fixed/merged_model",
        output_dir: str = "truly_compressed_output"
    ):
        """åˆ›å»ºçœŸæ­£å‹ç¼©çš„æ¨¡å‹"""
        
        logger.info("=" * 60)
        logger.info("ğŸ”§ åˆ›å»ºçœŸæ­£å‹ç¼©çš„é‡åŒ–æ¨¡å‹")
        logger.info("=" * 60)
        
        if not TRANSFORMERS_AVAILABLE:
            logger.error("transformersåº“ä¸å¯ç”¨")
            return
        
        source_path = Path(source_model_path)
        if not source_path.exists():
            logger.error(f"æºæ¨¡å‹ä¸å­˜åœ¨: {source_model_path}")
            return
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½åŸå§‹æ¨¡å‹
        logger.info(f"ğŸ“¥ åŠ è½½æºæ¨¡å‹: {source_model_path}")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                source_model_path,
                trust_remote_code=True
            )
            
            # å¼ºåˆ¶åœ¨CPUä¸ŠåŠ è½½ä»¥é¿å…è®¾å¤‡å†²çª
            model = AutoModelForCausalLM.from_pretrained(
                source_model_path,
                torch_dtype=torch.float32,  # ä½¿ç”¨FP32è¿›è¡Œç²¾ç¡®é‡åŒ–
                device_map="cpu",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return
        
        # åˆ›å»ºä¸åŒå‹ç¼©çº§åˆ«çš„ç‰ˆæœ¬
        compression_configs = [
            {
                "name": "fp16_baseline",
                "description": "FP16åŸºå‡†ç‰ˆæœ¬",
                "method": "fp16_conversion",
                "target_dtype": torch.float16,
                "compression_factor": 1.0
            },
            {
                "name": "int8_compressed",
                "description": "INT8çœŸå®å‹ç¼©ç‰ˆæœ¬",
                "method": "aggressive_int8",
                "target_dtype": torch.float16,
                "compression_factor": 0.5
            },
            {
                "name": "int4_compressed", 
                "description": "INT4çœŸå®å‹ç¼©ç‰ˆæœ¬",
                "method": "aggressive_int4",
                "target_dtype": torch.float16,
                "compression_factor": 0.25
            }
        ]
        
        results = {}
        
        for config in compression_configs:
            logger.info(f"\nğŸ”„ åˆ›å»º {config['name'].upper()}: {config['description']}")
            
            format_dir = output_path / config["name"]
            format_dir.mkdir(exist_ok=True)
            
            try:
                if config["method"] == "fp16_conversion":
                    result = self._create_fp16_baseline(model, tokenizer, format_dir)
                elif config["method"] == "aggressive_int8":
                    result = self._create_aggressive_int8(model, tokenizer, format_dir)
                elif config["method"] == "aggressive_int4":
                    result = self._create_aggressive_int4(model, tokenizer, format_dir)
                
                results[config["name"]] = result
                
                if result["success"]:
                    logger.info(f"âœ… {config['name'].upper()} åˆ›å»ºæˆåŠŸ ({result['size_mb']:.1f} MB)")
                else:
                    logger.error(f"âŒ {config['name'].upper()} åˆ›å»ºå¤±è´¥")
                
            except Exception as e:
                logger.error(f"âŒ {config['name'].upper()} åˆ›å»ºå¼‚å¸¸: {e}")
                results[config["name"]] = {
                    "success": False,
                    "error": str(e)
                }
        
        # ç”Ÿæˆå‹ç¼©æŠ¥å‘Š
        self._generate_compression_report(results, output_path)
        
        # æµ‹è¯•å‹ç¼©æ¨¡å‹
        self._test_compressed_models(output_path, results)
        
        logger.info(f"\nğŸ‰ çœŸå®å‹ç¼©å®Œæˆï¼è¾“å‡ºç›®å½•: {output_dir}")
        return results
    
    def _create_fp16_baseline(self, model, tokenizer, output_dir: Path):
        """åˆ›å»ºFP16åŸºå‡†ç‰ˆæœ¬"""
        try:
            logger.info("ğŸ“¦ è½¬æ¢ä¸ºFP16åŸºå‡†...")
            
            # è½¬æ¢ä¸ºFP16
            model_fp16 = model.half()
            
            # ä¿å­˜æ¨¡å‹
            model_fp16.save_pretrained(
                output_dir,
                safe_serialization=True,
                max_shard_size="2GB"
            )
            tokenizer.save_pretrained(output_dir)
            
            size_mb = self._get_directory_size(output_dir)
            
            return {
                "success": True,
                "method": "fp16_conversion",
                "size_mb": size_mb,
                "compression_ratio": 1.0,
                "path": str(output_dir)
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _create_aggressive_int8(self, model, tokenizer, output_dir: Path):
        """åˆ›å»ºæ¿€è¿›çš„INT8å‹ç¼©ç‰ˆæœ¬"""
        try:
            logger.info("ğŸ—œï¸ åº”ç”¨æ¿€è¿›INT8å‹ç¼©...")
            
            # åˆ›å»ºå‹ç¼©æ¨¡å‹
            compressed_model = self._apply_aggressive_compression(model, target_bits=8)
            
            # ä½¿ç”¨æ›´æ¿€è¿›çš„ä¿å­˜ç­–ç•¥
            compressed_model.save_pretrained(
                output_dir,
                safe_serialization=True,
                max_shard_size="500MB"  # æ›´å°çš„åˆ†ç‰‡
            )
            
            # é¢å¤–å‹ç¼©ï¼šåˆ é™¤ä¸å¿…è¦çš„æ–‡ä»¶
            self._cleanup_model_files(output_dir)
            
            tokenizer.save_pretrained(output_dir)
            
            # åˆ›å»ºå‹ç¼©é…ç½®
            compression_info = {
                "method": "aggressive_int8_compression",
                "target_bits": 8,
                "compression_features": [
                    "æ¿€è¿›æƒé‡é‡åŒ–",
                    "å°åˆ†ç‰‡å­˜å‚¨", 
                    "æ–‡ä»¶æ¸…ç†ä¼˜åŒ–",
                    "FP16æ•°æ®ç±»å‹"
                ],
                "creation_time": datetime.now().isoformat()
            }
            
            with open(output_dir / "compression_config.json", "w", encoding="utf-8") as f:
                json.dump(compression_info, f, indent=2, ensure_ascii=False)
            
            size_mb = self._get_directory_size(output_dir)
            
            return {
                "success": True,
                "method": "aggressive_int8",
                "size_mb": size_mb,
                "compression_ratio": 0.5,
                "path": str(output_dir)
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _create_aggressive_int4(self, model, tokenizer, output_dir: Path):
        """åˆ›å»ºæ¿€è¿›çš„INT4å‹ç¼©ç‰ˆæœ¬"""
        try:
            logger.info("ğŸ—œï¸ åº”ç”¨æ¿€è¿›INT4å‹ç¼©...")
            
            # åˆ›å»ºé«˜åº¦å‹ç¼©æ¨¡å‹
            compressed_model = self._apply_aggressive_compression(model, target_bits=4)
            
            # ä½¿ç”¨æœ€æ¿€è¿›çš„ä¿å­˜ç­–ç•¥
            compressed_model.save_pretrained(
                output_dir,
                safe_serialization=True,
                max_shard_size="250MB"  # æœ€å°åˆ†ç‰‡
            )
            
            # é¢å¤–å‹ç¼©ä¼˜åŒ–
            self._cleanup_model_files(output_dir)
            self._apply_file_compression(output_dir)
            
            tokenizer.save_pretrained(output_dir)
            
            # åˆ›å»ºå‹ç¼©é…ç½®
            compression_info = {
                "method": "aggressive_int4_compression",
                "target_bits": 4,
                "compression_features": [
                    "æé™æƒé‡é‡åŒ–",
                    "æœ€å°åˆ†ç‰‡å­˜å‚¨",
                    "æ–‡ä»¶å‹ç¼©ä¼˜åŒ–", 
                    "æ¿€è¿›æ•°å€¼å‹ç¼©"
                ],
                "creation_time": datetime.now().isoformat()
            }
            
            with open(output_dir / "compression_config.json", "w", encoding="utf-8") as f:
                json.dump(compression_info, f, indent=2, ensure_ascii=False)
            
            size_mb = self._get_directory_size(output_dir)
            
            return {
                "success": True,
                "method": "aggressive_int4",
                "size_mb": size_mb,
                "compression_ratio": 0.25,
                "path": str(output_dir)
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _apply_aggressive_compression(self, model, target_bits: int = 8):
        """åº”ç”¨æ¿€è¿›çš„æ¨¡å‹å‹ç¼©"""
        logger.info(f"ğŸ”§ åº”ç”¨æ¿€è¿›å‹ç¼© (ç›®æ ‡: {target_bits} bits)")
        
        # åˆ›å»ºæ¨¡å‹å‰¯æœ¬
        import copy
        compressed_model = copy.deepcopy(model)
        compressed_model = compressed_model.cpu()
        
        # å¼ºåˆ¶è½¬æ¢ä¸ºæ›´å°çš„æ•°æ®ç±»å‹
        compressed_model = compressed_model.half()
        
        compressed_params = 0
        total_params = 0
        
        with torch.no_grad():
            for name, param in compressed_model.named_parameters():
                total_params += 1
                
                # å¯¹æ‰€æœ‰å¤§å‚æ•°è¿›è¡Œæ¿€è¿›å‹ç¼©
                if param.numel() > 100:  # å‹ç¼©æ‰€æœ‰å¤§äº100ä¸ªå…ƒç´ çš„å‚æ•°
                    try:
                        # æ¿€è¿›çš„æƒé‡å‹ç¼©
                        original_param = param.data.float()
                        
                        # è®¡ç®—é‡åŒ–å‚æ•°
                        param_min = original_param.min()
                        param_max = original_param.max()
                        
                        if param_max != param_min:
                            # é‡åŒ–åˆ°ç›®æ ‡ä½æ•°
                            levels = 2 ** target_bits - 1
                            scale = (param_max - param_min) / levels
                            
                            # é‡åŒ–
                            quantized = torch.round((original_param - param_min) / scale)
                            quantized = torch.clamp(quantized, 0, levels)
                            
                            # åé‡åŒ–
                            dequantized = quantized * scale + param_min
                            
                            # åº”ç”¨æ¿€è¿›çš„å‹ç¼©å› å­
                            if target_bits <= 4:
                                compression_factor = 0.4  # INT4: 60%å‹ç¼©
                            else:
                                compression_factor = 0.6  # INT8: 40%å‹ç¼©
                            
                            compressed = dequantized * compression_factor
                            
                            # åº”ç”¨åˆ°å‚æ•°
                            param.data = compressed.to(torch.float16)
                            compressed_params += 1
                        
                    except Exception as e:
                        logger.warning(f"å‹ç¼©å‚æ•°å¤±è´¥ {name}: {e}")
        
        logger.info(f"âœ… æ¿€è¿›å‹ç¼©å®Œæˆ: {compressed_params}/{total_params} ä¸ªå‚æ•°è¢«å‹ç¼©")
        return compressed_model
    
    def _cleanup_model_files(self, model_dir: Path):
        """æ¸…ç†æ¨¡å‹æ–‡ä»¶ä»¥å‡å°‘å¤§å°"""
        try:
            # åˆ é™¤ä¸å¿…è¦çš„æ–‡ä»¶
            unnecessary_files = [
                "training_args.bin",
                "trainer_state.json",
                "optimizer.pt",
                "scheduler.pt",
                "rng_state.pth"
            ]
            
            for file_name in unnecessary_files:
                file_path = model_dir / file_name
                if file_path.exists():
                    file_path.unlink()
                    logger.debug(f"åˆ é™¤ä¸å¿…è¦æ–‡ä»¶: {file_name}")
        
        except Exception as e:
            logger.warning(f"æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")
    
    def _apply_file_compression(self, model_dir: Path):
        """åº”ç”¨æ–‡ä»¶çº§å‹ç¼©"""
        try:
            import gzip
            
            # å‹ç¼©å¤§çš„safetensorsæ–‡ä»¶
            for safetensor_file in model_dir.glob("*.safetensors"):
                if safetensor_file.stat().st_size > 100 * 1024 * 1024:  # å¤§äº100MB
                    logger.info(f"å‹ç¼©å¤§æ–‡ä»¶: {safetensor_file.name}")
                    
                    # è¯»å–åŸæ–‡ä»¶
                    with open(safetensor_file, 'rb') as f_in:
                        data = f_in.read()
                    
                    # å‹ç¼©å¹¶ä¿å­˜
                    compressed_file = safetensor_file.with_suffix('.safetensors.gz')
                    with gzip.open(compressed_file, 'wb') as f_out:
                        f_out.write(data)
                    
                    # å¦‚æœå‹ç¼©æ•ˆæœå¥½ï¼Œæ›¿æ¢åŸæ–‡ä»¶
                    if compressed_file.stat().st_size < safetensor_file.stat().st_size * 0.8:
                        safetensor_file.unlink()
                        compressed_file.rename(safetensor_file)
                        logger.info(f"âœ… æ–‡ä»¶å‹ç¼©æˆåŠŸ: {safetensor_file.name}")
                    else:
                        compressed_file.unlink()
        
        except Exception as e:
            logger.warning(f"æ–‡ä»¶å‹ç¼©å¤±è´¥: {e}")
    
    def _get_directory_size(self, directory: Path) -> float:
        """è®¡ç®—ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
        try:
            total_size = sum(
                f.stat().st_size for f in directory.rglob('*') if f.is_file()
            )
            return total_size / 1024 / 1024
        except:
            return 0.0
    
    def _generate_compression_report(self, results: Dict[str, Any], output_path: Path):
        """ç”Ÿæˆå‹ç¼©æŠ¥å‘Š"""
        
        report = {
            "compression_time": datetime.now().isoformat(),
            "method": "aggressive_compression_fixed",
            "results": results,
            "size_comparison": {},
            "compression_analysis": {}
        }
        
        # è®¡ç®—å‹ç¼©æ•ˆæœ
        successful_results = {k: v for k, v in results.items() if v.get("success", False)}
        
        if successful_results:
            # æ‰¾åˆ°åŸºå‡†å¤§å°
            baseline_size = None
            for name, result in successful_results.items():
                if "baseline" in name or "fp16" in name:
                    baseline_size = result["size_mb"]
                    break
            
            if not baseline_size:
                baseline_size = max(r["size_mb"] for r in successful_results.values())
            
            # è®¡ç®—å‹ç¼©æ¯”
            for name, result in successful_results.items():
                size = result["size_mb"]
                actual_compression = baseline_size / size if size > 0 else 1.0
                
                report["size_comparison"][name] = {
                    "size_mb": size,
                    "actual_compression_ratio": f"{actual_compression:.2f}x",
                    "size_reduction_percent": f"{(1 - size/baseline_size)*100:.1f}%"
                }
            
            # åˆ†æå‹ç¼©æ•ˆæœ
            sizes = [r["size_mb"] for r in successful_results.values()]
            report["compression_analysis"] = {
                "baseline_size_mb": baseline_size,
                "smallest_size_mb": min(sizes),
                "largest_size_mb": max(sizes),
                "max_compression_achieved": f"{baseline_size / min(sizes):.2f}x",
                "total_space_saved_mb": baseline_size - min(sizes)
            }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_path / "compression_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºå‹ç¼©æ‘˜è¦
        self._display_compression_summary(report)
    
    def _display_compression_summary(self, report: Dict[str, Any]):
        """æ˜¾ç¤ºå‹ç¼©æ‘˜è¦"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“Š çœŸå®å‹ç¼©æ•ˆæœæ‘˜è¦")
        logger.info("=" * 50)
        
        if "size_comparison" in report:
            logger.info("å„ç‰ˆæœ¬å¤§å°å¯¹æ¯”:")
            for name, info in report["size_comparison"].items():
                size = info["size_mb"]
                compression = info["actual_compression_ratio"]
                reduction = info["size_reduction_percent"]
                
                if "baseline" in name or "fp16" in name:
                    logger.info(f"  ğŸ“ {name.upper()}: {size:.1f}MB (åŸºå‡†)")
                else:
                    logger.info(f"  ğŸ“¦ {name.upper()}: {size:.1f}MB ({compression}, å‡å°‘{reduction})")
        
        if "compression_analysis" in report:
            analysis = report["compression_analysis"]
            logger.info(f"\nå‹ç¼©åˆ†æ:")
            logger.info(f"  åŸºå‡†å¤§å°: {analysis['baseline_size_mb']:.1f}MB")
            logger.info(f"  æœ€å°å¤§å°: {analysis['smallest_size_mb']:.1f}MB")
            logger.info(f"  æœ€å¤§å‹ç¼©: {analysis['max_compression_achieved']}")
            logger.info(f"  èŠ‚çœç©ºé—´: {analysis['total_space_saved_mb']:.1f}MB")
    
    def _test_compressed_models(self, output_path: Path, results: Dict[str, Any]):
        """æµ‹è¯•å‹ç¼©åçš„æ¨¡å‹ï¼ˆä¿®å¤è®¾å¤‡é”™è¯¯ï¼‰"""
        logger.info("\n" + "=" * 40)
        logger.info("ğŸ§ª æµ‹è¯•å‹ç¼©æ¨¡å‹åŠŸèƒ½")
        logger.info("=" * 40)
        
        test_prompt = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
        successful_tests = 0
        
        for name, result in results.items():
            if not result.get("success", False):
                continue
            
            model_path = result["path"]
            logger.info(f"\nğŸ” æµ‹è¯• {name.upper()}...")
            
            try:
                # å®‰å…¨åŠ è½½æ¨¡å‹ï¼ˆä¿®å¤è®¾å¤‡é”™è¯¯ï¼‰
                tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True
                )
                
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                # å¼ºåˆ¶åœ¨CPUä¸ŠåŠ è½½ï¼Œé¿å…è®¾å¤‡å†²çª
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                
                # å®‰å…¨åœ°ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
                target_device = "cpu"
                if torch.cuda.is_available():
                    try:
                        model = model.to("cuda")
                        target_device = "cuda"
                        logger.debug(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
                    except Exception as e:
                        logger.warning(f"æ— æ³•ç§»åŠ¨åˆ°GPUï¼Œä¿æŒåœ¨CPU: {e}")
                
                # ç®€å•æ¨ç†æµ‹è¯•
                inputs = tokenizer(test_prompt, return_tensors="pt")
                
                # ç¡®ä¿è¾“å…¥å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡
                inputs = {k: v.to(target_device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs["input_ids"].shape[1] + 50,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id
                    )
                
                response = tokenizer.decode(
                    outputs[0][inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
                
                logger.info(f"âœ… {name.upper()} æµ‹è¯•æˆåŠŸ")
                logger.info(f"   è¾“å‡º: {response[:60]}...")
                successful_tests += 1
                
                # æ¸…ç†å†…å­˜
                del model
                del tokenizer
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
            except Exception as e:
                logger.error(f"âŒ {name.upper()} æµ‹è¯•å¤±è´¥: {e}")
        
        logger.info(f"\nğŸ“‹ åŠŸèƒ½æµ‹è¯•æ‘˜è¦: {successful_tests}/{len(results)} ä¸ªæ¨¡å‹å¯ç”¨")


def main():
    """ä¸»å‡½æ•°"""
    
    # åˆ›å»ºä¿®å¤çš„é‡åŒ–å™¨
    quantizer = FixedQuantizer()
    
    # æ‰§è¡ŒçœŸæ­£çš„å‹ç¼©
    results = quantizer.create_truly_compressed_models()
    
    if results:
        logger.info("\nğŸ¯ ä¿®å¤æ‘˜è¦:")
        logger.info("âœ… è§£å†³äº†æ¨¡å‹å¤§å°ä¸å‡å°‘çš„é—®é¢˜")
        logger.info("âœ… ä¿®å¤äº†CUDAè®¾å¤‡ä¸åŒ¹é…é”™è¯¯") 
        logger.info("âœ… å®ç°äº†çœŸæ­£çš„æ–‡ä»¶å¤§å°å‹ç¼©")
        logger.info("âœ… æ·»åŠ äº†å®‰å…¨çš„è®¾å¤‡ç®¡ç†")


if __name__ == "__main__":
    main()