# LLaMA Factory Finetuning for Qwen3-4B-Thinking

åŸºäºLLaMA Factoryæ¡†æ¶çš„Qwen3-4B-Thinkingæ¨¡å‹å¾®è°ƒç³»ç»Ÿï¼Œä¸“é—¨é’ˆå¯¹ä¸­æ–‡å¯†ç å­¦é¢†åŸŸä¼˜åŒ–ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **ä¸“ä¸šæ¨¡å‹æ”¯æŒ**: é’ˆå¯¹Qwen/Qwen3-4B-Thinking-2507æ¨¡å‹ä¼˜åŒ–
- ğŸ§  **æ·±åº¦æ€è€ƒæ•°æ®**: æ”¯æŒ`<thinking>`æ ‡ç­¾çš„æ¨ç†æ•°æ®æ ¼å¼
- ğŸ‡¨ğŸ‡³ **ä¸­æ–‡ä¼˜åŒ–**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡æ–‡æœ¬å’Œå¯†ç å­¦æœ¯è¯­å¤„ç†
- ğŸ’¾ **å†…å­˜é«˜æ•ˆ**: LoRAå¾®è°ƒã€æ··åˆç²¾åº¦è®­ç»ƒã€æ¢¯åº¦æ£€æŸ¥ç‚¹
- ğŸ”„ **å¤šGPUæ”¯æŒ**: æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œ
- ğŸ“Š **æ™ºèƒ½ç›‘æ§**: å®æ—¶è®­ç»ƒç›‘æ§å’Œä¸“å®¶è¯„ä¼°ç³»ç»Ÿ
- ğŸ“¦ **æ¨¡å‹å¯¼å‡º**: æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼å¯¼å‡º

## ç³»ç»Ÿè¦æ±‚

- Python 3.12+
- CUDA 12.9+ (æ¨è)
- GPUå†…å­˜: æœ€å°8GBï¼Œæ¨è16GB+
- ç³»ç»Ÿå†…å­˜: æœ€å°16GB
- uvåŒ…ç®¡ç†å™¨

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…uvåŒ…ç®¡ç†å™¨

é¦–å…ˆç¡®ä¿å·²å®‰è£…uvåŒ…ç®¡ç†å™¨ï¼š

```bash
# Windows (PowerShell)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# æˆ–ä½¿ç”¨pipå®‰è£…
pip install uv
```

### 2. ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd llama-factory-finetuning

# ä½¿ç”¨uvå®‰è£…ä¾èµ–
uv sync --extra dev

# è¿è¡Œç¯å¢ƒè®¾ç½®
uv run python src/environment_setup.py
```

### 3. æ£€æŸ¥ç¯å¢ƒ

```bash
# æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒå’ŒGPUçŠ¶æ€
uv run python scripts/check_environment.py
```

### 4. å‡†å¤‡æ•°æ®

å°†è®­ç»ƒæ•°æ®æ”¾ç½®åœ¨ä»¥ä¸‹ç›®å½•ï¼š
- `data/raw/` - åŸå§‹markdownæ–‡ä»¶
- `data/train/` - è®­ç»ƒæ•°æ®
- `data/eval/` - éªŒè¯æ•°æ®
- `data/test/` - æµ‹è¯•æ•°æ®

### 4. é…ç½®è°ƒæ•´

ç¼–è¾‘ `configs/config.yaml` æ–‡ä»¶ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´é…ç½®ï¼š

```yaml
model:
  model_name: "Qwen/Qwen3-4B-Thinking-2507"
  load_in_4bit: true  # 4bité‡åŒ–ä»¥èŠ‚çœå†…å­˜

training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  num_train_epochs: 3

lora:
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
```

### 6. å¼€å§‹è®­ç»ƒ

```bash
# å•GPUè®­ç»ƒ
uv run python scripts/train.py

# å¤šGPUè®­ç»ƒ
uv run torchrun --nproc_per_node=2 scripts/train.py
```

## é¡¹ç›®ç»“æ„

```
llama-factory-finetuning/
â”œâ”€â”€ src/                    # æºä»£ç 
â”‚   â”œâ”€â”€ gpu_utils.py       # GPUæ£€æµ‹å’Œç®¡ç†
â”‚   â”œâ”€â”€ model_config.py    # æ¨¡å‹é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ config_manager.py  # é…ç½®ç®¡ç†ç³»ç»Ÿ
â”‚   â””â”€â”€ environment_setup.py # ç¯å¢ƒè®¾ç½®
â”œâ”€â”€ scripts/               # æ‰§è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ train.py          # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ check_environment.py # ç¯å¢ƒæ£€æŸ¥
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ config.yaml       # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/              # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ train/            # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ eval/             # éªŒè¯æ•°æ®
â”‚   â””â”€â”€ test/             # æµ‹è¯•æ•°æ®
â”œâ”€â”€ output/                # è®­ç»ƒè¾“å‡º
â”œâ”€â”€ logs/                  # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ cache/                 # ç¼“å­˜ç›®å½•
â””â”€â”€ models/                # æ¨¡å‹æ–‡ä»¶
```

## é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

- `model_name`: Qwen3-4B-Thinkingæ¨¡å‹åç§°
- `load_in_4bit/8bit`: é‡åŒ–åŠ è½½ä»¥èŠ‚çœå†…å­˜
- `torch_dtype`: æ•°æ®ç±»å‹ï¼ˆauto/float16/bfloat16ï¼‰
- `device_map`: è®¾å¤‡æ˜ å°„ç­–ç•¥

### LoRAé…ç½®

- `r`: LoRA rankï¼Œæ§åˆ¶é€‚é…å™¨å¤§å°
- `lora_alpha`: LoRAç¼©æ”¾å‚æ•°
- `target_modules`: ç›®æ ‡æ¨¡å—åˆ—è¡¨
- `lora_dropout`: Dropoutç‡

### è®­ç»ƒé…ç½®

- `per_device_train_batch_size`: æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
- `gradient_accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- `learning_rate`: å­¦ä¹ ç‡
- `num_train_epochs`: è®­ç»ƒè½®æ•°

### å¤šGPUé…ç½®

- `enable_distributed`: å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
- `world_size`: æ€»è¿›ç¨‹æ•°
- `backend`: é€šä¿¡åç«¯ï¼ˆnccl/glooï¼‰

## æ•°æ®æ ¼å¼

### æ ‡å‡†æ ¼å¼

```json
{
  "instruction": "è¯·è§£é‡ŠAESåŠ å¯†ç®—æ³•çš„å·¥ä½œåŸç†",
  "input": "",
  "output": "AESï¼ˆé«˜çº§åŠ å¯†æ ‡å‡†ï¼‰æ˜¯ä¸€ç§å¯¹ç§°åŠ å¯†ç®—æ³•..."
}
```

### æ€è€ƒæ ¼å¼

```json
{
  "instruction": "åˆ†æRSAç®—æ³•çš„å®‰å…¨æ€§",
  "input": "",
  "output": "<thinking>é¦–å…ˆéœ€è¦è€ƒè™‘RSAç®—æ³•çš„æ•°å­¦åŸºç¡€...</thinking>RSAç®—æ³•çš„å®‰å…¨æ€§ä¸»è¦åŸºäº..."
}
```

## GPUå†…å­˜ä¼˜åŒ–

### 8GB GPU
- å¯ç”¨4bité‡åŒ–: `load_in_4bit: true`
- æ‰¹æ¬¡å¤§å°: `per_device_train_batch_size: 1`
- æ¢¯åº¦ç´¯ç§¯: `gradient_accumulation_steps: 8`

### 16GB GPU
- å¯ç”¨8bité‡åŒ–: `load_in_8bit: true`
- æ‰¹æ¬¡å¤§å°: `per_device_train_batch_size: 2`
- æ¢¯åº¦ç´¯ç§¯: `gradient_accumulation_steps: 4`

### 24GB+ GPU
- æ ‡å‡†ç²¾åº¦è®­ç»ƒ
- æ‰¹æ¬¡å¤§å°: `per_device_train_batch_size: 4`
- æ¢¯åº¦ç´¯ç§¯: `gradient_accumulation_steps: 2`

## ç›‘æ§å’Œè¯„ä¼°

è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç›‘æ§ï¼š

1. **TensorBoard**: `tensorboard --logdir logs/`
2. **æ—¥å¿—æ–‡ä»¶**: æŸ¥çœ‹ `logs/` ç›®å½•ä¸‹çš„æ—¥å¿—
3. **GPUç›‘æ§**: ä½¿ç”¨ `nvidia-smi` æˆ–é¡¹ç›®å†…ç½®ç›‘æ§

## uvåŒ…ç®¡ç†å™¨ä½¿ç”¨

æœ¬é¡¹ç›®ä½¿ç”¨uvä½œä¸ºåŒ…ç®¡ç†å™¨ï¼Œæä¾›æ›´å¿«çš„ä¾èµ–è§£æå’Œå®‰è£…ã€‚

### å¸¸ç”¨uvå‘½ä»¤

```bash
# åŒæ­¥ä¾èµ–
uv sync

# å®‰è£…æ–°åŒ…
uv add <package>

# ç§»é™¤åŒ…
uv remove <package>

# æŸ¥çœ‹ä¾èµ–æ ‘
uv tree

# è¿è¡Œè„šæœ¬
uv run python <script.py>

# è¿è¡Œæµ‹è¯•
uv run pytest

# æŸ¥çœ‹å·²å®‰è£…åŒ…
uv pip list

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows
```

### å¼€å‘å·¥ä½œæµ

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd llama-factory-finetuning

# 2. ä½¿ç”¨uvè®¾ç½®ç¯å¢ƒ
python setup_with_uv.py

# 3. æµ‹è¯•ç¯å¢ƒ
uv run python test_uv_setup.py

# 4. å¼€å‘å’Œæµ‹è¯•
uv run python scripts/check_environment.py
uv run python scripts/train.py
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **uvç›¸å…³é—®é¢˜**
   - é‡æ–°åŒæ­¥ä¾èµ–: `uv sync --extra dev`
   - æ¸…ç†ç¼“å­˜: `uv cache clean`
   - æ£€æŸ¥uvç‰ˆæœ¬: `uv --version`

2. **CUDAå†…å­˜ä¸è¶³**
   - å‡å°æ‰¹æ¬¡å¤§å°
   - å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
   - ä½¿ç”¨é‡åŒ–åŠ è½½

3. **æ¨¡å‹åŠ è½½å¤±è´¥**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - éªŒè¯æ¨¡å‹åç§°
   - æ£€æŸ¥ç¼“å­˜ç›®å½•æƒé™

4. **ä¸­æ–‡ç¼–ç é—®é¢˜**
   - ç¡®ä¿æ–‡ä»¶ä½¿ç”¨UTF-8ç¼–ç 
   - æ£€æŸ¥tokenizeré…ç½®

5. **ä¾èµ–å®‰è£…é—®é¢˜**
   - ä½¿ç”¨uvé‡æ–°å®‰è£…: `uv sync --reinstall`
   - æ£€æŸ¥Pythonç‰ˆæœ¬: `uv run python --version`
   - æŸ¥çœ‹è¯¦ç»†é”™è¯¯: `uv run python -c "import <module>"`

### è·å–å¸®åŠ©

- æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: `logs/setup.log`
- è¿è¡Œç¯å¢ƒæ£€æŸ¥: `uv run python scripts/check_environment.py`
- æµ‹è¯•uvç¯å¢ƒ: `uv run python test_uv_setup.py`
- æŸ¥çœ‹GPUçŠ¶æ€: `nvidia-smi`

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æºã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ã€‚

## æ›´æ–°æ—¥å¿—

### v0.1.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒQwen3-4B-Thinkingæ¨¡å‹
- å®ç°åŸºç¡€è®­ç»ƒåŠŸèƒ½
- æ·»åŠ GPUæ£€æµ‹å’Œä¼˜åŒ–