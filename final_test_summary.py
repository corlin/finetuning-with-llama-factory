#!/usr/bin/env python3
"""
æœ€ç»ˆæµ‹è¯•æ‘˜è¦ç¨‹åº

æ•´åˆæ‰€æœ‰å¯¼å‡ºæ¨¡å‹çš„æµ‹è¯•ç»“æœï¼Œç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
"""

import json
import torch
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False


def find_all_models() -> Dict[str, str]:
    """æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
    
    base_dirs = [
        "quantized_models_output",
        "fixed_quantized_output", 
        "size_demo_output",
        "safe_quantized_output",
        "integrated_export_test_output"
    ]
    
    models = {}
    
    for base_dir in base_dirs:
        base_path = Path(base_dir)
        if not base_path.exists():
            continue
            
        for format_dir in base_path.iterdir():
            if format_dir.is_dir():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                has_model = any([
                    list(format_dir.glob("*.safetensors")),
                    list(format_dir.glob("*.bin")),
                ])
                
                has_tokenizer = (format_dir / "tokenizer.json").exists()
                
                if has_model and has_tokenizer:
                    model_key = f"{base_dir}_{format_dir.name}"
                    models[model_key] = str(format_dir)
    
    return models


def quick_test_model(model_path: str, model_name: str) -> Dict[str, Any]:
    """å¿«é€Ÿæµ‹è¯•å•ä¸ªæ¨¡å‹"""
    
    result = {
        "model_name": model_name,
        "model_path": model_path,
        "test_time": datetime.now().isoformat(),
        "success": False,
        "load_time": 0,
        "model_size_mb": 0,
        "inference_test": {},
        "error": None
    }
    
    try:
        # è®¡ç®—æ¨¡å‹å¤§å°
        result["model_size_mb"] = get_directory_size(Path(model_path))
        
        # åŠ è½½æ¨¡å‹
        start_time = time.time()
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        result["load_time"] = time.time() - start_time
        
        # æ¨ç†æµ‹è¯•
        test_prompt = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
        inputs = tokenizer(test_prompt, return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=inputs["input_ids"].shape[1] + 80,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id
            )
        
        inference_time = time.time() - start_time
        
        response = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        # è¯„ä¼°å“åº”è´¨é‡
        chinese_chars = sum(1 for char in response if '\u4e00' <= char <= '\u9fff')
        chinese_ratio = chinese_chars / len(response) if response else 0
        
        result["inference_test"] = {
            "prompt": test_prompt,
            "response": response[:100] + "..." if len(response) > 100 else response,
            "response_length": len(response),
            "inference_time": inference_time,
            "chinese_ratio": chinese_ratio,
            "has_content": len(response.strip()) > 10
        }
        
        result["success"] = True
        
        # æ¸…ç†å†…å­˜
        del model
        del tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


def get_directory_size(directory: Path) -> float:
    """è®¡ç®—ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
    try:
        total_size = sum(
            f.stat().st_size for f in directory.rglob('*') if f.is_file()
        )
        return total_size / 1024 / 1024
    except:
        return 0.0


def generate_final_report(test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    
    successful_tests = [r for r in test_results if r["success"]]
    failed_tests = [r for r in test_results if not r["success"]]
    
    report = {
        "report_time": datetime.now().isoformat(),
        "summary": {
            "total_models": len(test_results),
            "successful_tests": len(successful_tests),
            "failed_tests": len(failed_tests),
            "success_rate": len(successful_tests) / len(test_results) if test_results else 0
        },
        "test_results": test_results,
        "analysis": {},
        "recommendations": []
    }
    
    if successful_tests:
        # æ€§èƒ½åˆ†æ
        sizes = [r["model_size_mb"] for r in successful_tests]
        load_times = [r["load_time"] for r in successful_tests]
        inference_times = [r["inference_test"]["inference_time"] for r in successful_tests 
                          if "inference_time" in r["inference_test"]]
        
        report["analysis"] = {
            "size_stats": {
                "min_mb": min(sizes),
                "max_mb": max(sizes),
                "avg_mb": sum(sizes) / len(sizes)
            },
            "performance_stats": {
                "avg_load_time": sum(load_times) / len(load_times),
                "avg_inference_time": sum(inference_times) / len(inference_times) if inference_times else 0
            }
        }
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_models = {
            "smallest": min(successful_tests, key=lambda x: x["model_size_mb"]),
            "fastest_load": min(successful_tests, key=lambda x: x["load_time"]),
            "fastest_inference": min(successful_tests, 
                                   key=lambda x: x["inference_test"].get("inference_time", float('inf')))
        }
        
        report["analysis"]["best_models"] = {
            "smallest": best_models["smallest"]["model_name"],
            "fastest_load": best_models["fastest_load"]["model_name"], 
            "fastest_inference": best_models["fastest_inference"]["model_name"]
        }
        
        # ç”Ÿæˆå»ºè®®
        if report["summary"]["success_rate"] > 0.8:
            report["recommendations"].append("å¤§éƒ¨åˆ†æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œå¯ä»¥è€ƒè™‘éƒ¨ç½²ä½¿ç”¨")
        
        if len([r for r in successful_tests if r["model_size_mb"] < 4000]) > 0:
            report["recommendations"].append("æœ‰å¯ç”¨çš„å‹ç¼©æ¨¡å‹ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒ")
        
        if len([r for r in successful_tests if r["inference_test"].get("inference_time", 0) < 2.0]) > 0:
            report["recommendations"].append("æœ‰å¿«é€Ÿæ¨ç†çš„æ¨¡å‹ï¼Œé€‚åˆå®æ—¶åº”ç”¨")
    
    if failed_tests:
        common_errors = {}
        for test in failed_tests:
            error = test.get("error", "æœªçŸ¥é”™è¯¯")
            error_type = error.split(":")[0] if ":" in error else error
            common_errors[error_type] = common_errors.get(error_type, 0) + 1
        
        report["analysis"]["common_errors"] = common_errors
        
        if "CUDA" in str(common_errors):
            report["recommendations"].append("å­˜åœ¨CUDAç›¸å…³é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥é‡åŒ–ç®—æ³•")
        
        if len(failed_tests) > len(successful_tests):
            report["recommendations"].append("å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®é‡æ–°æ£€æŸ¥æ¨¡å‹å¯¼å‡ºæµç¨‹")
    
    return report


def display_summary(report: Dict[str, Any]):
    """æ˜¾ç¤ºæµ‹è¯•æ‘˜è¦"""
    
    print("\n" + "=" * 80)
    print("ğŸ” å¯¼å‡ºæ¨¡å‹æœ€ç»ˆæµ‹è¯•æ‘˜è¦")
    print("=" * 80)
    
    summary = report["summary"]
    print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
    print(f"   æ€»æ¨¡å‹æ•°: {summary['total_models']}")
    print(f"   æˆåŠŸæµ‹è¯•: {summary['successful_tests']}")
    print(f"   å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
    print(f"   æˆåŠŸç‡: {summary['success_rate']:.1%}")
    
    if "analysis" in report:
        analysis = report["analysis"]
        
        if "size_stats" in analysis:
            size_stats = analysis["size_stats"]
            print(f"\nğŸ“ æ¨¡å‹å¤§å°ç»Ÿè®¡:")
            print(f"   æœ€å°: {size_stats['min_mb']:.1f} MB")
            print(f"   æœ€å¤§: {size_stats['max_mb']:.1f} MB") 
            print(f"   å¹³å‡: {size_stats['avg_mb']:.1f} MB")
        
        if "performance_stats" in analysis:
            perf_stats = analysis["performance_stats"]
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            print(f"   å¹³å‡åŠ è½½æ—¶é—´: {perf_stats['avg_load_time']:.1f}s")
            print(f"   å¹³å‡æ¨ç†æ—¶é—´: {perf_stats['avg_inference_time']:.2f}s")
        
        if "best_models" in analysis:
            best = analysis["best_models"]
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹:")
            print(f"   æœ€å°ä½“ç§¯: {best['smallest']}")
            print(f"   æœ€å¿«åŠ è½½: {best['fastest_load']}")
            print(f"   æœ€å¿«æ¨ç†: {best['fastest_inference']}")
        
        if "common_errors" in analysis:
            errors = analysis["common_errors"]
            print(f"\nâŒ å¸¸è§é”™è¯¯:")
            for error_type, count in errors.items():
                print(f"   {error_type}: {count} æ¬¡")
    
    print(f"\nâœ… æˆåŠŸçš„æ¨¡å‹:")
    for result in report["test_results"]:
        if result["success"]:
            size = result["model_size_mb"]
            load_time = result["load_time"]
            inf_time = result["inference_test"].get("inference_time", 0)
            print(f"   âœ“ {result['model_name']}")
            print(f"     å¤§å°: {size:.1f}MB, åŠ è½½: {load_time:.1f}s, æ¨ç†: {inf_time:.2f}s")
    
    if any(not r["success"] for r in report["test_results"]):
        print(f"\nâŒ å¤±è´¥çš„æ¨¡å‹:")
        for result in report["test_results"]:
            if not result["success"]:
                print(f"   âœ— {result['model_name']}: {result['error']}")
    
    if report.get("recommendations"):
        print(f"\nğŸ’¡ å»ºè®®:")
        for i, rec in enumerate(report["recommendations"], 1):
            print(f"   {i}. {rec}")
    
    print("\n" + "=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ” å¼€å§‹æœ€ç»ˆæµ‹è¯•æ‘˜è¦...")
    
    if not TRANSFORMERS_AVAILABLE:
        print("âŒ transformersåº“ä¸å¯ç”¨")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹
    models = find_all_models()
    
    if not models:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯æµ‹è¯•çš„æ¨¡å‹")
        return
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹:")
    for model_name, model_path in models.items():
        print(f"   - {model_name}: {model_path}")
    
    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•...")
    test_results = []
    
    for i, (model_name, model_path) in enumerate(models.items(), 1):
        print(f"\n[{i}/{len(models)}] æµ‹è¯• {model_name}...")
        
        result = quick_test_model(model_path, model_name)
        test_results.append(result)
        
        if result["success"]:
            print(f"   âœ… æˆåŠŸ ({result['model_size_mb']:.1f}MB, {result['load_time']:.1f}s)")
        else:
            print(f"   âŒ å¤±è´¥: {result['error']}")
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print(f"\nğŸ“Š ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    final_report = generate_final_report(test_results)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path("final_test_summary.json")
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    
    # æ˜¾ç¤ºæ‘˜è¦
    display_summary(final_report)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # ç”Ÿæˆç®€åŒ–çš„ä½¿ç”¨æŒ‡å—
    generate_usage_guide(final_report)


def generate_usage_guide(report: Dict[str, Any]):
    """ç”Ÿæˆä½¿ç”¨æŒ‡å—"""
    
    successful_models = [r for r in report["test_results"] if r["success"]]
    
    if not successful_models:
        return
    
    guide_content = f"""# å¯¼å‡ºæ¨¡å‹ä½¿ç”¨æŒ‡å—

## æµ‹è¯•æ‘˜è¦

- **æµ‹è¯•æ—¶é—´**: {report['report_time']}
- **æ€»æ¨¡å‹æ•°**: {report['summary']['total_models']}
- **å¯ç”¨æ¨¡å‹**: {report['summary']['successful_tests']}
- **æˆåŠŸç‡**: {report['summary']['success_rate']:.1%}

## æ¨èæ¨¡å‹

"""
    
    if "analysis" in report and "best_models" in report["analysis"]:
        best = report["analysis"]["best_models"]
        guide_content += f"""### ğŸ† æœ€ä½³é€‰æ‹©

- **æœ€å°ä½“ç§¯**: `{best['smallest']}` - é€‚åˆå­˜å‚¨å—é™ç¯å¢ƒ
- **æœ€å¿«åŠ è½½**: `{best['fastest_load']}` - é€‚åˆé¢‘ç¹é‡å¯åœºæ™¯  
- **æœ€å¿«æ¨ç†**: `{best['fastest_inference']}` - é€‚åˆå®æ—¶åº”ç”¨

"""
    
    guide_content += """## å¯ç”¨æ¨¡å‹åˆ—è¡¨

| æ¨¡å‹åç§° | å¤§å°(MB) | åŠ è½½æ—¶é—´(s) | æ¨ç†æ—¶é—´(s) | çŠ¶æ€ |
|----------|----------|-------------|-------------|------|
"""
    
    for result in successful_models:
        name = result["model_name"]
        size = result["model_size_mb"]
        load_time = result["load_time"]
        inf_time = result["inference_test"].get("inference_time", 0)
        
        guide_content += f"| {name} | {size:.1f} | {load_time:.1f} | {inf_time:.2f} | âœ… |\n"
    
    guide_content += f"""
## ä½¿ç”¨æ–¹æ³•

### åŠ è½½æ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹è·¯å¾„
model_path = "path/to/your/chosen/model"

# åŠ è½½
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
```

### æ¨ç†ç¤ºä¾‹

```python
# åŸºç¡€æ¨ç†
prompt = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

# æ€è€ƒæ¨ç†
thinking_prompt = "<thinking>åˆ†æè¿™ä¸ªé—®é¢˜</thinking>è¯·è§£é‡ŠRSAç®—æ³•ã€‚"
inputs = tokenizer(thinking_prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=300, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## æ€§èƒ½å»ºè®®

"""
    
    if report.get("recommendations"):
        for rec in report["recommendations"]:
            guide_content += f"- {rec}\n"
    
    guide_content += f"""
## æ•…éšœæ’é™¤

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. **CUDAå†…å­˜**: ç¡®ä¿GPUå†…å­˜è¶³å¤Ÿ
2. **ä¾èµ–ç‰ˆæœ¬**: ä½¿ç”¨å…¼å®¹çš„transformersç‰ˆæœ¬
3. **æ¨¡å‹å®Œæ•´æ€§**: ç¡®è®¤æ¨¡å‹æ–‡ä»¶å®Œæ•´ä¸‹è½½
4. **è®¾å¤‡å…¼å®¹æ€§**: æ£€æŸ¥CUDAç‰ˆæœ¬å…¼å®¹æ€§

---

*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*æµ‹è¯•ç‰ˆæœ¬: v1.0*
"""
    
    with open("MODEL_USAGE_GUIDE.md", "w", encoding="utf-8") as f:
        f.write(guide_content)
    
    print("ğŸ“– ä½¿ç”¨æŒ‡å—å·²ç”Ÿæˆ: MODEL_USAGE_GUIDE.md")


if __name__ == "__main__":
    main()