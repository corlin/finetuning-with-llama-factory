#!/usr/bin/env python3
"""
è®­ç»ƒæµæ°´çº¿ç¼–æ’å™¨æµ‹è¯•
"""

import sys
import os
import tempfile
import shutil
import time
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from training_pipeline import (
    TrainingPipelineOrchestrator, PipelineStage, PipelineStatus
)
from data_models import TrainingExample, DifficultyLevel
from config_manager import TrainingConfig, DataConfig
from lora_config_optimizer import LoRAMemoryProfile
from parallel_config import ParallelConfig, ParallelStrategy


def test_pipeline_basic_functionality():
    """æµ‹è¯•æµæ°´çº¿åŸºæœ¬åŠŸèƒ½"""
    print("æµ‹è¯•è®­ç»ƒæµæ°´çº¿ç¼–æ’å™¨åŸºæœ¬åŠŸèƒ½...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            TrainingExample(
                instruction="ä»€ä¹ˆæ˜¯AESåŠ å¯†ï¼Ÿ",
                input="",
                output="AESæ˜¯é«˜çº§åŠ å¯†æ ‡å‡†ã€‚",
                thinking="<thinking>éœ€è¦è§£é‡ŠAESçš„åŸºæœ¬æ¦‚å¿µã€‚</thinking>",
                crypto_terms=["AES", "åŠ å¯†"],
                difficulty_level=DifficultyLevel.BEGINNER
            ),
            TrainingExample(
                instruction="æ¯”è¾ƒRSAå’ŒECCç®—æ³•",
                input="åœ¨ç°ä»£å¯†ç å­¦ä¸­",
                output="RSAåŸºäºå¤§æ•´æ•°åˆ†è§£ï¼ŒECCåŸºäºæ¤­åœ†æ›²çº¿ã€‚",
                crypto_terms=["RSA", "ECC"],
                difficulty_level=DifficultyLevel.INTERMEDIATE
            )
        ]
        
        # åˆ›å»ºé…ç½®
        training_config = TrainingConfig(
            num_train_epochs=1,
            per_device_train_batch_size=1,
            output_dir=str(Path(temp_dir) / "model_output")
        )
        data_config = DataConfig()
        lora_config = LoRAMemoryProfile(
            rank=8, alpha=16, target_modules=["q_proj", "v_proj"]
        )
        parallel_config = ParallelConfig(strategy=ParallelStrategy.DATA_PARALLEL)
        
        # åˆ›å»ºæµæ°´çº¿ç¼–æ’å™¨
        orchestrator = TrainingPipelineOrchestrator(
            "test_pipeline",
            output_dir=temp_dir
        )
        print("âœ“ æµæ°´çº¿ç¼–æ’å™¨åˆ›å»ºæˆåŠŸ")
        
        # é…ç½®æµæ°´çº¿
        orchestrator.configure_pipeline(
            test_data, training_config, data_config, lora_config, parallel_config
        )
        print("âœ“ æµæ°´çº¿é…ç½®å®Œæˆ")
        
        # æ·»åŠ è¿›åº¦å›è°ƒ
        progress_updates = []
        def progress_callback(state):
            progress_updates.append((state.current_stage, state.progress))
            print(f"  è¿›åº¦æ›´æ–°: {state.current_stage.value} - {state.progress:.1f}%")
        
        orchestrator.add_progress_callback(progress_callback)
        
        # æ·»åŠ é˜¶æ®µå›è°ƒ
        stage_completions = []
        def stage_callback(state):
            stage_completions.append(state.current_stage)
            print(f"  é˜¶æ®µå®Œæˆ: {state.current_stage.value}")
        
        for stage in [PipelineStage.INITIALIZATION, PipelineStage.DATA_PREPARATION, 
                     PipelineStage.CONFIG_GENERATION]:
            orchestrator.add_stage_callback(stage, stage_callback)
        
        # æµ‹è¯•æµæ°´çº¿çŠ¶æ€
        assert orchestrator.get_state().status == PipelineStatus.PENDING
        assert not orchestrator.is_running()
        assert not orchestrator.is_completed()
        print("âœ“ åˆå§‹çŠ¶æ€éªŒè¯é€šè¿‡")
        
        # è¿è¡Œæµæ°´çº¿ï¼ˆåªè¿è¡Œå‰å‡ ä¸ªé˜¶æ®µï¼Œé¿å…å®é™…è®­ç»ƒï¼‰
        print("å¼€å§‹è¿è¡Œæµæ°´çº¿...")
        
        # æ‰‹åŠ¨æ‰§è¡Œå‰å‡ ä¸ªé˜¶æ®µè¿›è¡Œæµ‹è¯•
        success = True
        
        # æµ‹è¯•åˆå§‹åŒ–é˜¶æ®µ
        orchestrator.state.status = PipelineStatus.RUNNING
        orchestrator.state.start_time = orchestrator.state.start_time or time.time()
        
        if orchestrator._stage_initialization():
            print("âœ“ åˆå§‹åŒ–é˜¶æ®µå®Œæˆ")
        else:
            print("âœ— åˆå§‹åŒ–é˜¶æ®µå¤±è´¥")
            success = False
        
        # æµ‹è¯•æ•°æ®å‡†å¤‡é˜¶æ®µ
        if success and orchestrator._stage_data_preparation():
            print("âœ“ æ•°æ®å‡†å¤‡é˜¶æ®µå®Œæˆ")
            print(f"  æ•°æ®æ–‡ä»¶: {orchestrator.data_files}")
        else:
            print("âœ— æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥")
            success = False
        
        # æµ‹è¯•é…ç½®ç”Ÿæˆé˜¶æ®µ
        if success and orchestrator._stage_config_generation():
            print("âœ“ é…ç½®ç”Ÿæˆé˜¶æ®µå®Œæˆ")
            print(f"  é…ç½®æ–‡ä»¶: {orchestrator.config_files}")
        else:
            print("âœ— é…ç½®ç”Ÿæˆé˜¶æ®µå¤±è´¥")
            success = False
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        if success:
            output_dir = Path(temp_dir)
            expected_dirs = ["data", "configs", "checkpoints", "logs", "models"]
            for dir_name in expected_dirs:
                if (output_dir / dir_name).exists():
                    print(f"  âœ“ {dir_name} ç›®å½•å­˜åœ¨")
                else:
                    print(f"  âœ— {dir_name} ç›®å½•ä¸å­˜åœ¨")
            
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶
            if orchestrator.data_files:
                for file_type, file_path in orchestrator.data_files.items():
                    if Path(file_path).exists():
                        print(f"  âœ“ {file_type} æ–‡ä»¶å­˜åœ¨")
                    else:
                        print(f"  âœ— {file_type} æ–‡ä»¶ä¸å­˜åœ¨")
            
            # æ£€æŸ¥é…ç½®æ–‡ä»¶
            if orchestrator.config_files:
                for file_type, file_path in orchestrator.config_files.items():
                    if Path(file_path).exists():
                        print(f"  âœ“ {file_type} æ–‡ä»¶å­˜åœ¨")
                    else:
                        print(f"  âœ— {file_type} æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æµ‹è¯•çŠ¶æ€ç®¡ç†
        state = orchestrator.get_state()
        print(f"âœ“ æµæ°´çº¿çŠ¶æ€: {state.status.value}")
        print(f"âœ“ å½“å‰é˜¶æ®µ: {state.current_stage.value}")
        print(f"âœ“ è¿›åº¦: {state.progress:.1f}%")
        
        # æµ‹è¯•æ£€æŸ¥ç‚¹åŠŸèƒ½
        orchestrator._create_checkpoint(PipelineStage.DATA_PREPARATION)
        if state.checkpoints:
            print("âœ“ æ£€æŸ¥ç‚¹åˆ›å»ºæˆåŠŸ")
        else:
            print("âœ— æ£€æŸ¥ç‚¹åˆ›å»ºå¤±è´¥")
        
        return success
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir)
        print(f"æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")


def test_pipeline_control():
    """æµ‹è¯•æµæ°´çº¿æ§åˆ¶åŠŸèƒ½"""
    print("\næµ‹è¯•æµæ°´çº¿æ§åˆ¶åŠŸèƒ½...")
    
    temp_dir = tempfile.mkdtemp()
    
    try:
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        test_data = [
            TrainingExample(
                instruction="æµ‹è¯•é—®é¢˜",
                input="",
                output="æµ‹è¯•å›ç­”",
                crypto_terms=["æµ‹è¯•"]
            )
        ]
        
        # åˆ›å»ºé…ç½®
        training_config = TrainingConfig(num_train_epochs=1)
        data_config = DataConfig()
        lora_config = LoRAMemoryProfile(rank=4, alpha=8, target_modules=["q_proj"])
        parallel_config = ParallelConfig(strategy=ParallelStrategy.DATA_PARALLEL)
        
        # åˆ›å»ºæµæ°´çº¿ç¼–æ’å™¨
        orchestrator = TrainingPipelineOrchestrator("control_test", temp_dir)
        orchestrator.configure_pipeline(
            test_data, training_config, data_config, lora_config, parallel_config
        )
        
        # æµ‹è¯•æš‚åœå’Œæ¢å¤
        orchestrator.pause_pipeline()
        assert orchestrator.should_pause
        print("âœ“ æš‚åœåŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
        orchestrator.resume_pipeline()
        assert not orchestrator.should_pause
        print("âœ“ æ¢å¤åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•åœæ­¢
        orchestrator.stop_pipeline()
        assert orchestrator.should_stop
        print("âœ“ åœæ­¢åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ§åˆ¶åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    finally:
        shutil.rmtree(temp_dir)


def test_pipeline_state_management():
    """æµ‹è¯•æµæ°´çº¿çŠ¶æ€ç®¡ç†"""
    print("\næµ‹è¯•æµæ°´çº¿çŠ¶æ€ç®¡ç†...")
    
    try:
        from training_pipeline import PipelineState, PipelineCheckpoint
        
        # åˆ›å»ºæµæ°´çº¿çŠ¶æ€
        state = PipelineState("test_state")
        
        # æµ‹è¯•é˜¶æ®µæ›´æ–°
        state.update_stage(PipelineStage.INITIALIZATION, 50.0)
        assert state.current_stage == PipelineStage.INITIALIZATION
        assert state.stage_progress[PipelineStage.INITIALIZATION] == 50.0
        print("âœ“ é˜¶æ®µæ›´æ–°æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•è¿›åº¦æ›´æ–°
        state.update_stage_progress(PipelineStage.INITIALIZATION, 100.0)
        assert state.stage_progress[PipelineStage.INITIALIZATION] == 100.0
        print("âœ“ è¿›åº¦æ›´æ–°æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ£€æŸ¥ç‚¹
        from datetime import datetime
        checkpoint = PipelineCheckpoint(
            checkpoint_id="test_checkpoint",
            stage=PipelineStage.INITIALIZATION,
            timestamp=datetime.now(),
            state_data={"test": "data"}
        )
        
        state.add_checkpoint(checkpoint)
        assert len(state.checkpoints) == 1
        assert state.latest_checkpoint == checkpoint
        print("âœ“ æ£€æŸ¥ç‚¹ç®¡ç†æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•åºåˆ—åŒ–
        state_dict = state.to_dict()
        assert "pipeline_id" in state_dict
        assert "status" in state_dict
        assert "current_stage" in state_dict
        print("âœ“ çŠ¶æ€åºåˆ—åŒ–æµ‹è¯•é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âœ— çŠ¶æ€ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_integration_with_direct_training():
    """æµ‹è¯•ä¸ç›´æ¥è®­ç»ƒå¼•æ“çš„é›†æˆ"""
    print("\næµ‹è¯•ä¸ç›´æ¥è®­ç»ƒå¼•æ“çš„é›†æˆ...")
    
    temp_dir = tempfile.mkdtemp()
    
    try:
        # åˆ›å»ºæ›´å¤æ‚çš„æµ‹è¯•æ•°æ®
        test_data = []
        for i in range(5):
            example = TrainingExample(
                instruction=f"å¯†ç å­¦é—®é¢˜ {i+1}",
                input=f"ä¸Šä¸‹æ–‡ {i+1}",
                output=f"è¿™æ˜¯å…³äºå¯†ç å­¦çš„å›ç­” {i+1}",
                thinking=f"<thinking>è¿™æ˜¯æ€è€ƒè¿‡ç¨‹ {i+1}</thinking>",
                crypto_terms=["å¯†ç å­¦", "å®‰å…¨"],
                difficulty_level=DifficultyLevel.INTERMEDIATE
            )
            test_data.append(example)
        
        # åˆ›å»ºé…ç½®
        training_config = TrainingConfig(
            num_train_epochs=1,
            per_device_train_batch_size=1,
            learning_rate=1e-4
        )
        data_config = DataConfig()
        lora_config = LoRAMemoryProfile(
            rank=8, alpha=16, target_modules=["q_proj", "v_proj"]
        )
        parallel_config = ParallelConfig(strategy=ParallelStrategy.DATA_PARALLEL)
        
        # åˆ›å»ºæµæ°´çº¿ç¼–æ’å™¨
        orchestrator = TrainingPipelineOrchestrator("integration_test", temp_dir)
        orchestrator.configure_pipeline(
            test_data, training_config, data_config, lora_config, parallel_config
        )
        
        # æ‰§è¡Œæ•°æ®å‡†å¤‡å’Œé…ç½®ç”Ÿæˆé˜¶æ®µ
        success = (orchestrator._stage_initialization() and
                  orchestrator._stage_data_preparation() and
                  orchestrator._stage_config_generation())
        
        if success:
            print("âœ“ ç›´æ¥è®­ç»ƒå¼•æ“é›†æˆæµ‹è¯•é€šè¿‡")
            
            # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
            if orchestrator.data_files and orchestrator.config_files:
                print("  âœ“ æ•°æ®æ–‡ä»¶å’Œé…ç½®æ–‡ä»¶éƒ½å·²ç”Ÿæˆ")
                
                # æ£€æŸ¥ç›´æ¥è®­ç»ƒé…ç½®æ–‡ä»¶å†…å®¹
                config_file = orchestrator.config_files.get("direct_training_config")
                if config_file and Path(config_file).exists():
                    print("  âœ“ ç›´æ¥è®­ç»ƒé…ç½®æ–‡ä»¶å­˜åœ¨")
                    
                    # è¯»å–é…ç½®æ–‡ä»¶éªŒè¯å†…å®¹
                    import yaml
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = yaml.safe_load(f)
                    
                    required_keys = ["model_name", "data_path", "lora_r", "lora_alpha"]
                    if all(key in config for key in required_keys):
                        print("  âœ“ é…ç½®æ–‡ä»¶åŒ…å«å¿…éœ€å­—æ®µ")
                    else:
                        print("  âœ— é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€å­—æ®µ")
                        success = False
                
                # æ£€æŸ¥è®­ç»ƒè„šæœ¬
                script_file = orchestrator.config_files.get("training_script")
                if script_file and Path(script_file).exists():
                    print("  âœ“ è®­ç»ƒè„šæœ¬æ–‡ä»¶å­˜åœ¨")
                else:
                    print("  âœ— è®­ç»ƒè„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨")
                    success = False
            else:
                print("  âœ— æ–‡ä»¶ç”Ÿæˆä¸å®Œæ•´")
                success = False
        else:
            print("âœ— ç›´æ¥è®­ç»ƒå¼•æ“é›†æˆæµ‹è¯•å¤±è´¥")
        
        return success
        
    except Exception as e:
        print(f"âœ— é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        shutil.rmtree(temp_dir)


if __name__ == "__main__":
    print("è®­ç»ƒæµæ°´çº¿ç¼–æ’å™¨æµ‹è¯•")
    print("=" * 50)
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_results.append(test_pipeline_basic_functionality())
    test_results.append(test_pipeline_control())
    test_results.append(test_pipeline_state_management())
    test_results.append(test_integration_with_direct_training())
    
    # æ±‡æ€»ç»“æœ
    passed = sum(test_results)
    total = len(test_results)
    
    print(f"\næµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        exit(0)
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        exit(1)