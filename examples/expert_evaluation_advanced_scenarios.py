#!/usr/bin/env python3
"""
ä¸“å®¶è¯„ä¼°ç³»ç»Ÿé«˜çº§åœºæ™¯ç¤ºä¾‹

æœ¬è„šæœ¬å±•ç¤ºäº†ä¸“å®¶è¯„ä¼°ç³»ç»Ÿçš„é«˜çº§ä½¿ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬ï¼š
1. è‡ªå®šä¹‰è¯„ä¼°ç»´åº¦
2. å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°
3. å¤§è§„æ¨¡æ‰¹é‡å¤„ç†
4. å®æ—¶è¯„ä¼°ç›‘æ§
5. ç»“æœåˆ†æå’Œå¯è§†åŒ–

ä½¿ç”¨æ–¹æ³•:
    uv run python examples/expert_evaluation_advanced_scenarios.py

ä½œè€…: ä¸“å®¶è¯„ä¼°ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
"""

import json
import time
import sys
import asyncio
from pathlib import Path
from typing import List, Dict, Any
import matplotlib.pyplot as plt
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.expert_evaluation.engine import ExpertEvaluationEngine
from src.expert_evaluation.config import ExpertEvaluationConfig
from src.expert_evaluation.multi_dimensional import MultiDimensionalEvaluator
from src.expert_evaluation.performance import PerformanceMonitor

class AdvancedEvaluationScenarios:
    """é«˜çº§è¯„ä¼°åœºæ™¯æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.output_dir = Path("advanced_demo_output")
        self.output_dir.mkdir(exist_ok=True)
        
        print("ğŸš€ ä¸“å®¶è¯„ä¼°ç³»ç»Ÿé«˜çº§åœºæ™¯æ¼”ç¤º")
        print("=" * 60)
    
    def scenario_1_custom_dimensions(self):
        """åœºæ™¯1: è‡ªå®šä¹‰è¯„ä¼°ç»´åº¦"""
        print("\nğŸ¯ åœºæ™¯1: è‡ªå®šä¹‰è¯„ä¼°ç»´åº¦")
        print("-" * 40)
        
        # åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°é…ç½®
        custom_config = {
            "evaluation": {
                "dimensions": [
                    "semantic_similarity",
                    "domain_accuracy", 
                    "creativity_score",
                    "technical_depth",
                    "practical_applicability"
                ],
                "weights": {
                    "semantic_similarity": 0.20,
                    "domain_accuracy": 0.25,
                    "creativity_score": 0.20,
                    "technical_depth": 0.20,
                    "practical_applicability": 0.15
                },
                "algorithms": {
                    "creativity_score": {
                        "method": "novelty_detection",
                        "baseline_size": 50,
                        "diversity_weight": 0.4
                    },
                    "technical_depth": {
                        "method": "concept_complexity",
                        "min_concepts": 3,
                        "depth_threshold": 0.7
                    }
                }
            }
        }
        
        config = ExpertEvaluationConfig.from_dict(custom_config)
        engine = ExpertEvaluationEngine(config)
        
        # æµ‹è¯•æ•°æ®
        qa_item = {
            "question_id": "custom_001",
            "question": "è®¾è®¡ä¸€ä¸ªåŸºäºåŒºå—é“¾çš„å»ä¸­å¿ƒåŒ–èº«ä»½éªŒè¯ç³»ç»Ÿ",
            "reference_answer": "è¯¥ç³»ç»Ÿåº”åŒ…å«åˆ†å¸ƒå¼èº«ä»½æ ‡è¯†ç¬¦(DID)ã€å¯éªŒè¯å‡­è¯(VC)ã€æ™ºèƒ½åˆçº¦éªŒè¯æœºåˆ¶ç­‰æ ¸å¿ƒç»„ä»¶...",
            "model_answer": "å¯ä»¥ä½¿ç”¨ä»¥å¤ªåŠæ™ºèƒ½åˆçº¦åˆ›å»ºä¸€ä¸ªèº«ä»½æ³¨å†Œç³»ç»Ÿï¼Œç”¨æˆ·é€šè¿‡ç§é’¥ç­¾åè¯æ˜èº«ä»½ï¼Œç»“åˆIPFSå­˜å‚¨èº«ä»½ä¿¡æ¯...",
            "domain_tags": ["åŒºå—é“¾", "èº«ä»½éªŒè¯", "å»ä¸­å¿ƒåŒ–"],
            "difficulty_level": "expert",
            "expected_concepts": ["DID", "æ™ºèƒ½åˆçº¦", "å¯†ç å­¦", "åˆ†å¸ƒå¼ç³»ç»Ÿ"]
        }
        
        result = engine.evaluate_single_qa(qa_item)
        
        print("ğŸ“Š è‡ªå®šä¹‰ç»´åº¦è¯„ä¼°ç»“æœ:")
        for dimension, score in result.dimension_scores.items():
            print(f"   - {dimension}: {score:.3f}")
        
        # ä¿å­˜ç»“æœ
        with open(self.output_dir / "custom_dimensions_result.json", 'w', encoding='utf-8') as f:
            json.dump(result.to_dict(), f, indent=2, ensure_ascii=False)
        
        print("âœ… è‡ªå®šä¹‰ç»´åº¦è¯„ä¼°å®Œæˆ")
        return result
    
    def scenario_2_model_comparison(self):
        """åœºæ™¯2: å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°"""
        print("\nğŸ”„ åœºæ™¯2: å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°")
        print("-" * 40)
        
        # æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„é…ç½®
        model_configs = {
            "model_a": {
                "name": "æ¨¡å‹A (åŸºç¡€ç‰ˆ)",
                "config": {
                    "evaluation": {
                        "weights": {
                            "semantic_similarity": 0.4,
                            "domain_accuracy": 0.6
                        }
                    }
                }
            },
            "model_b": {
                "name": "æ¨¡å‹B (å¢å¼ºç‰ˆ)",
                "config": {
                    "evaluation": {
                        "weights": {
                            "semantic_similarity": 0.3,
                            "domain_accuracy": 0.4,
                            "innovation": 0.3
                        }
                    }
                }
            },
            "model_c": {
                "name": "æ¨¡å‹C (ä¸“ä¸šç‰ˆ)",
                "config": {
                    "evaluation": {
                        "weights": {
                            "semantic_similarity": 0.25,
                            "domain_accuracy": 0.25,
                            "innovation": 0.25,
                            "practical_value": 0.25
                        }
                    }
                }
            }
        }
        
        # æµ‹è¯•æ•°æ®é›†
        test_qa_items = [
            {
                "question_id": "comp_001",
                "question": "è§£é‡Šæ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶",
                "reference_answer": "æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶åŠ¨æ€åœ°å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†...",
                "model_answer": "æ³¨æ„åŠ›æœºåˆ¶æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„é‡è¦æŠ€æœ¯ï¼Œå®ƒå¸®åŠ©æ¨¡å‹è¯†åˆ«è¾“å…¥ä¸­çš„å…³é”®ä¿¡æ¯...",
                "domain_tags": ["æ·±åº¦å­¦ä¹ ", "æ³¨æ„åŠ›æœºåˆ¶"]
            },
            {
                "question_id": "comp_002", 
                "question": "ä»€ä¹ˆæ˜¯è”é‚¦å­¦ä¹ ï¼Ÿå®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ",
                "reference_answer": "è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå…è®¸å¤šä¸ªå‚ä¸æ–¹åœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹åä½œè®­ç»ƒæ¨¡å‹...",
                "model_answer": "è”é‚¦å­¦ä¹ è®©ä¸åŒçš„è®¾å¤‡å¯ä»¥ä¸€èµ·è®­ç»ƒAIæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŠ¤ç”¨æˆ·éšç§...",
                "domain_tags": ["è”é‚¦å­¦ä¹ ", "éšç§ä¿æŠ¤"]
            }
        ]
        
        comparison_results = {}
        
        # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
        for model_id, model_info in model_configs.items():
            print(f"ğŸ” è¯„ä¼° {model_info['name']}...")
            
            config = ExpertEvaluationConfig.from_dict(model_info['config'])
            engine = ExpertEvaluationEngine(config)
            
            model_results = []
            for qa_item in test_qa_items:
                result = engine.evaluate_single_qa(qa_item)
                model_results.append(result)
            
            # è®¡ç®—å¹³å‡å¾—åˆ†
            avg_score = sum(r.overall_score for r in model_results) / len(model_results)
            comparison_results[model_id] = {
                "name": model_info['name'],
                "average_score": avg_score,
                "individual_results": [r.to_dict() for r in model_results]
            }
            
            print(f"   å¹³å‡å¾—åˆ†: {avg_score:.3f}")
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        print("\nğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ:")
        sorted_models = sorted(comparison_results.items(), 
                             key=lambda x: x[1]['average_score'], 
                             reverse=True)
        
        for i, (model_id, result) in enumerate(sorted_models, 1):
            print(f"   {i}. {result['name']}: {result['average_score']:.3f}")
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        with open(self.output_dir / "model_comparison.json", 'w', encoding='utf-8') as f:
            json.dump(comparison_results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
        self._generate_comparison_chart(comparison_results)
        
        print("âœ… å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°å®Œæˆ")
        return comparison_results
    
    def scenario_3_large_scale_processing(self):
        """åœºæ™¯3: å¤§è§„æ¨¡æ‰¹é‡å¤„ç†"""
        print("\nğŸ“¦ åœºæ™¯3: å¤§è§„æ¨¡æ‰¹é‡å¤„ç†")
        print("-" * 40)
        
        # ç”Ÿæˆå¤§è§„æ¨¡æµ‹è¯•æ•°æ®
        large_dataset = []
        for i in range(100):  # ç”Ÿæˆ100ä¸ªQAé¡¹
            qa_item = {
                "question_id": f"large_scale_{i:03d}",
                "question": f"è¿™æ˜¯ç¬¬{i+1}ä¸ªæµ‹è¯•é—®é¢˜ï¼Œè¯·è¯¦ç»†å›ç­”ç›¸å…³æŠ€æœ¯åŸç†ã€‚",
                "reference_answer": f"è¿™æ˜¯ç¬¬{i+1}ä¸ªå‚è€ƒç­”æ¡ˆï¼ŒåŒ…å«è¯¦ç»†çš„æŠ€æœ¯è§£é‡Šå’Œå®ç°æ–¹æ¡ˆã€‚",
                "model_answer": f"è¿™æ˜¯ç¬¬{i+1}ä¸ªæ¨¡å‹ç­”æ¡ˆï¼Œæä¾›äº†ç›¸åº”çš„æŠ€æœ¯åˆ†æã€‚",
                "domain_tags": ["æŠ€æœ¯", "æµ‹è¯•"],
                "difficulty_level": "intermediate"
            }
            large_dataset.append(qa_item)
        
        print(f"ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®: {len(large_dataset)} ä¸ªQAé¡¹")
        
        # é…ç½®æ‰¹é‡å¤„ç†
        batch_config = {
            "performance": {
                "max_workers": 4,
                "batch_size": 10,
                "timeout": 300
            },
            "evaluation": {
                "dimensions": ["semantic_similarity", "domain_accuracy"]
            }
        }
        
        config = ExpertEvaluationConfig.from_dict(batch_config)
        engine = ExpertEvaluationEngine(config)
        
        # æ‰§è¡Œå¤§è§„æ¨¡æ‰¹é‡è¯„ä¼°
        print("â³ å¼€å§‹å¤§è§„æ¨¡æ‰¹é‡è¯„ä¼°...")
        start_time = time.time()
        
        batch_result = engine.evaluate_batch(large_dataset)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {len(large_dataset)/total_time:.1f} QAé¡¹/ç§’")
        print(f"ğŸ“Š å¹³å‡å¾—åˆ†: {batch_result.average_overall_score:.3f}")
        
        # åˆ†æå¤„ç†æ€§èƒ½
        performance_stats = {
            "total_items": len(large_dataset),
            "total_time": total_time,
            "throughput": len(large_dataset) / total_time,
            "average_score": batch_result.average_overall_score,
            "score_distribution": self._calculate_score_distribution(batch_result)
        }
        
        # ä¿å­˜æ€§èƒ½ç»Ÿè®¡
        with open(self.output_dir / "large_scale_performance.json", 'w', encoding='utf-8') as f:
            json.dump(performance_stats, f, indent=2, ensure_ascii=False)
        
        print("âœ… å¤§è§„æ¨¡æ‰¹é‡å¤„ç†å®Œæˆ")
        return performance_stats
    
    def scenario_4_realtime_monitoring(self):
        """åœºæ™¯4: å®æ—¶è¯„ä¼°ç›‘æ§"""
        print("\nğŸ“¡ åœºæ™¯4: å®æ—¶è¯„ä¼°ç›‘æ§")
        print("-" * 40)
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        monitor = PerformanceMonitor()
        
        # æ¨¡æ‹Ÿå®æ—¶è¯„ä¼°æµ
        config = ExpertEvaluationConfig()
        engine = ExpertEvaluationEngine(config)
        
        print("ğŸ”„ å¼€å§‹å®æ—¶è¯„ä¼°ç›‘æ§ (10ç§’æ¼”ç¤º)...")
        
        monitoring_data = []
        start_time = time.time()
        
        # æ¨¡æ‹Ÿ10ç§’çš„å®æ—¶è¯„ä¼°
        while time.time() - start_time < 10:
            # ç”ŸæˆéšæœºQAé¡¹
            qa_item = {
                "question_id": f"realtime_{int(time.time())}",
                "question": "å®æ—¶è¯„ä¼°æµ‹è¯•é—®é¢˜",
                "reference_answer": "å®æ—¶è¯„ä¼°å‚è€ƒç­”æ¡ˆ",
                "model_answer": "å®æ—¶è¯„ä¼°æ¨¡å‹ç­”æ¡ˆ",
                "domain_tags": ["å®æ—¶æµ‹è¯•"]
            }
            
            # æ‰§è¡Œè¯„ä¼°å¹¶ç›‘æ§
            eval_start = time.time()
            result = engine.evaluate_single_qa(qa_item)
            eval_time = time.time() - eval_start
            
            # æ”¶é›†ç›‘æ§æ•°æ®
            monitoring_data.append({
                "timestamp": time.time(),
                "evaluation_time": eval_time,
                "score": result.overall_score,
                "memory_usage": monitor.get_memory_usage(),
                "cpu_usage": monitor.get_cpu_usage()
            })
            
            print(f"âš¡ è¯„ä¼°å®Œæˆ - å¾—åˆ†: {result.overall_score:.3f}, è€—æ—¶: {eval_time:.3f}ç§’")
            
            time.sleep(1)  # æ¯ç§’ä¸€æ¬¡è¯„ä¼°
        
        print("âœ… å®æ—¶ç›‘æ§æ¼”ç¤ºå®Œæˆ")
        
        # åˆ†æç›‘æ§æ•°æ®
        avg_eval_time = sum(d['evaluation_time'] for d in monitoring_data) / len(monitoring_data)
        avg_score = sum(d['score'] for d in monitoring_data) / len(monitoring_data)
        avg_memory = sum(d['memory_usage'] for d in monitoring_data) / len(monitoring_data)
        avg_cpu = sum(d['cpu_usage'] for d in monitoring_data) / len(monitoring_data)
        
        print(f"\nğŸ“Š ç›‘æ§ç»Ÿè®¡:")
        print(f"   - å¹³å‡è¯„ä¼°æ—¶é—´: {avg_eval_time:.3f}ç§’")
        print(f"   - å¹³å‡å¾—åˆ†: {avg_score:.3f}")
        print(f"   - å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.1f}MB")
        print(f"   - å¹³å‡CPUä½¿ç”¨: {avg_cpu:.1f}%")
        
        # ä¿å­˜ç›‘æ§æ•°æ®
        with open(self.output_dir / "realtime_monitoring.json", 'w', encoding='utf-8') as f:
            json.dump(monitoring_data, f, indent=2, ensure_ascii=False)
        
        return monitoring_data
    
    def scenario_5_result_visualization(self):
        """åœºæ™¯5: ç»“æœåˆ†æå’Œå¯è§†åŒ–"""
        print("\nğŸ“ˆ åœºæ™¯5: ç»“æœåˆ†æå’Œå¯è§†åŒ–")
        print("-" * 40)
        
        # ç”Ÿæˆå¤šç»´åº¦è¯„ä¼°æ•°æ®
        evaluation_data = []
        dimensions = ["semantic_similarity", "domain_accuracy", "innovation", "practical_value"]
        
        for i in range(50):
            scores = {}
            for dim in dimensions:
                # ç”Ÿæˆæ¨¡æ‹Ÿå¾—åˆ† (æ·»åŠ ä¸€äº›éšæœºæ€§)
                base_score = 0.7 + (i % 10) * 0.03
                noise = (hash(f"{i}_{dim}") % 100) / 1000  # -0.05 åˆ° 0.05 çš„å™ªå£°
                scores[dim] = max(0, min(1, base_score + noise))
            
            overall_score = sum(scores.values()) / len(scores)
            
            evaluation_data.append({
                "item_id": f"viz_{i:03d}",
                "overall_score": overall_score,
                "dimension_scores": scores
            })
        
        # åˆ›å»ºæ•°æ®åˆ†æ
        df = pd.DataFrame(evaluation_data)
        
        # å±•å¼€ç»´åº¦å¾—åˆ†
        for dim in dimensions:
            df[dim] = df['dimension_scores'].apply(lambda x: x[dim])
        
        print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # 1. å¾—åˆ†åˆ†å¸ƒç›´æ–¹å›¾
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 2, 1)
        plt.hist(df['overall_score'], bins=20, alpha=0.7, color='skyblue')
        plt.title('æ€»ä½“å¾—åˆ†åˆ†å¸ƒ')
        plt.xlabel('å¾—åˆ†')
        plt.ylabel('é¢‘æ¬¡')
        
        # 2. ç»´åº¦å¾—åˆ†ç®±çº¿å›¾
        plt.subplot(2, 2, 2)
        dimension_data = [df[dim] for dim in dimensions]
        plt.boxplot(dimension_data, labels=dimensions)
        plt.title('å„ç»´åº¦å¾—åˆ†åˆ†å¸ƒ')
        plt.xticks(rotation=45)
        
        # 3. å¾—åˆ†è¶‹åŠ¿å›¾
        plt.subplot(2, 2, 3)
        plt.plot(df.index, df['overall_score'], marker='o', markersize=3)
        plt.title('å¾—åˆ†è¶‹åŠ¿')
        plt.xlabel('æ ·æœ¬åºå·')
        plt.ylabel('æ€»ä½“å¾—åˆ†')
        
        # 4. ç»´åº¦ç›¸å…³æ€§çƒ­åŠ›å›¾
        plt.subplot(2, 2, 4)
        correlation_matrix = df[dimensions].corr()
        plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
        plt.colorbar()
        plt.title('ç»´åº¦ç›¸å…³æ€§')
        plt.xticks(range(len(dimensions)), dimensions, rotation=45)
        plt.yticks(range(len(dimensions)), dimensions)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "evaluation_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats_report = {
            "summary": {
                "total_samples": len(evaluation_data),
                "mean_score": float(df['overall_score'].mean()),
                "std_score": float(df['overall_score'].std()),
                "min_score": float(df['overall_score'].min()),
                "max_score": float(df['overall_score'].max())
            },
            "dimension_stats": {}
        }
        
        for dim in dimensions:
            stats_report["dimension_stats"][dim] = {
                "mean": float(df[dim].mean()),
                "std": float(df[dim].std()),
                "min": float(df[dim].min()),
                "max": float(df[dim].max())
            }
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        with open(self.output_dir / "visualization_stats.json", 'w', encoding='utf-8') as f:
            json.dump(stats_report, f, indent=2, ensure_ascii=False)
        
        print("âœ… å¯è§†åŒ–åˆ†æå®Œæˆ")
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜: {self.output_dir / 'evaluation_analysis.png'}")
        
        return stats_report
    
    def _generate_comparison_chart(self, comparison_results):
        """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”å›¾è¡¨"""
        models = list(comparison_results.keys())
        scores = [comparison_results[model]['average_score'] for model in models]
        names = [comparison_results[model]['name'] for model in models]
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(names, scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')
        
        plt.title('æ¨¡å‹å¯¹æ¯”è¯„ä¼°ç»“æœ')
        plt.ylabel('å¹³å‡å¾—åˆ†')
        plt.ylim(0, 1)
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        plt.savefig(self.output_dir / "model_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _calculate_score_distribution(self, batch_result):
        """è®¡ç®—å¾—åˆ†åˆ†å¸ƒ"""
        scores = [result.overall_score for result in batch_result.individual_results]
        
        ranges = [(0.9, 1.0), (0.8, 0.9), (0.7, 0.8), (0.6, 0.7), (0.0, 0.6)]
        distribution = {}
        
        for min_score, max_score in ranges:
            count = sum(1 for score in scores if min_score <= score < max_score)
            distribution[f"{min_score}-{max_score}"] = count
        
        return distribution
    
    def run_all_scenarios(self):
        """è¿è¡Œæ‰€æœ‰é«˜çº§åœºæ™¯"""
        scenarios = [
            ("è‡ªå®šä¹‰è¯„ä¼°ç»´åº¦", self.scenario_1_custom_dimensions),
            ("å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°", self.scenario_2_model_comparison),
            ("å¤§è§„æ¨¡æ‰¹é‡å¤„ç†", self.scenario_3_large_scale_processing),
            ("å®æ—¶è¯„ä¼°ç›‘æ§", self.scenario_4_realtime_monitoring),
            ("ç»“æœåˆ†æå¯è§†åŒ–", self.scenario_5_result_visualization)
        ]
        
        results = {}
        
        for scenario_name, scenario_func in scenarios:
            try:
                print(f"\nğŸ¯ å¼€å§‹æ‰§è¡Œ: {scenario_name}")
                result = scenario_func()
                results[scenario_name] = result
                print(f"âœ… {scenario_name} å®Œæˆ")
            except Exception as e:
                print(f"âŒ {scenario_name} å¤±è´¥: {e}")
                results[scenario_name] = {"error": str(e)}
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "completed_scenarios": len([r for r in results.values() if "error" not in r]),
            "total_scenarios": len(scenarios),
            "output_directory": str(self.output_dir),
            "scenario_results": results
        }
        
        with open(self.output_dir / "advanced_scenarios_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ é«˜çº§åœºæ™¯æ¼”ç¤ºå®Œæˆ!")
        print(f"ğŸ“Š å®Œæˆåœºæ™¯: {summary['completed_scenarios']}/{summary['total_scenarios']}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        demo = AdvancedEvaluationScenarios()
        demo.run_all_scenarios()
    except Exception as e:
        print(f"âŒ é«˜çº§åœºæ™¯æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()