"""
æ¨¡å‹å¯¼å‡ºå’Œéƒ¨ç½²åŒ…ç”Ÿæˆæ¼”ç¤º

æœ¬è„šæœ¬æ¼”ç¤ºå®Œæ•´çš„æ¨¡å‹é‡åŒ–ã€å¯¼å‡ºå’Œéƒ¨ç½²åŒ…ç”Ÿæˆæµç¨‹ï¼Œ
åŒ…æ‹¬å…ƒæ•°æ®ç”Ÿæˆã€ä½¿ç”¨è¯´æ˜åˆ›å»ºå’Œéƒ¨ç½²éªŒè¯ã€‚
"""

import sys
import os
import logging
import torch
import torch.nn as nn
import tempfile
import shutil
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "src"))

from model_exporter import (
    ModelExporter,
    ModelQuantizer,
    ChineseCapabilityValidator,
    QuantizationConfig,
    QuantizationFormat,
    QuantizationBackend,
    ModelMetadata,
    DeploymentPackage
)


class DemoQwenModel(nn.Module):
    """æ¼”ç¤ºç”¨çš„Qwené£æ ¼æ¨¡å‹"""
    
    def __init__(self, vocab_size=50000, hidden_size=768, num_layers=12):
        super().__init__()
        self.config = type('Config', (), {
            'vocab_size': vocab_size,
            'hidden_size': hidden_size,
            'num_layers': num_layers,
            'max_position_embeddings': 2048
        })()
        
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                hidden_size, 
                nhead=8, 
                dim_feedforward=hidden_size * 4,
                batch_first=True
            ) for _ in range(num_layers)
        ])
        self.ln_f = nn.LayerNorm(hidden_size)
        self.lm_head = nn.Linear(hidden_size, vocab_size)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æƒé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(self, input_ids, attention_mask=None, **kwargs):
        """å‰å‘ä¼ æ’­"""
        x = self.embedding(input_ids)
        
        # é€šè¿‡transformerå±‚
        for layer in self.layers:
            # å¤„ç†attention mask - éœ€è¦è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
            if attention_mask is not None:
                # å°†attention_maskè½¬æ¢ä¸ºpadding mask (Trueè¡¨ç¤ºéœ€è¦maskçš„ä½ç½®)
                padding_mask = (attention_mask == 0)
                x = layer(x, src_key_padding_mask=padding_mask)
            else:
                x = layer(x)
        
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        # è¿”å›ç±»ä¼¼transformersçš„è¾“å‡ºæ ¼å¼
        class ModelOutput:
            def __init__(self, logits):
                self.logits = logits
        
        return ModelOutput(logits)
    
    def generate(self, input_ids, max_length=100, temperature=1.0, **kwargs):
        """ç®€å•çš„ç”Ÿæˆæ–¹æ³•"""
        self.eval()
        batch_size, seq_len = input_ids.shape
        
        with torch.no_grad():
            for _ in range(max_length - seq_len):
                # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
                outputs = self.forward(input_ids)
                next_token_logits = outputs.logits[:, -1, :] / temperature
                
                # ç®€å•çš„è´ªå©ªè§£ç 
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                input_ids = torch.cat([input_ids, next_token], dim=-1)
                
                # å¦‚æœç”Ÿæˆäº†ç»“æŸtokenï¼Œåœæ­¢ç”Ÿæˆ
                if next_token.item() == 2:  # å‡è®¾2æ˜¯ç»“æŸtoken
                    break
        
        return input_ids
    
    def save_pretrained(self, save_directory):
        """ä¿å­˜æ¨¡å‹"""
        save_path = Path(save_directory)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        torch.save(self.state_dict(), save_path / "pytorch_model.bin")
        
        # ä¿å­˜é…ç½®
        config_dict = {
            "vocab_size": self.config.vocab_size,
            "hidden_size": self.config.hidden_size,
            "num_layers": self.config.num_layers,
            "max_position_embeddings": self.config.max_position_embeddings,
            "model_type": "qwen-demo",
            "architectures": ["DemoQwenModel"]
        }
        
        with open(save_path / "config.json", 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)


class DemoQwenTokenizer:
    """æ¼”ç¤ºç”¨çš„Qwené£æ ¼åˆ†è¯å™¨"""
    
    def __init__(self):
        self.vocab_size = 50000
        self.pad_token_id = 0
        self.eos_token_id = 2
        self.bos_token_id = 1
        self.unk_token_id = 3
        
        # ç‰¹æ®Štoken
        self.special_tokens = {
            "<pad>": 0,
            "<s>": 1,
            "</s>": 2,
            "<unk>": 3,
            "<thinking>": 4,
            "</thinking>": 5
        }
        
        # ä¸­æ–‡å¯†ç å­¦è¯æ±‡
        self.crypto_vocab = {
            "AES": 100, "RSA": 101, "SHA": 102, "æ¤­åœ†æ›²çº¿": 103,
            "æ•°å­—ç­¾å": 104, "å¯¹ç§°åŠ å¯†": 105, "éå¯¹ç§°åŠ å¯†": 106,
            "å“ˆå¸Œå‡½æ•°": 107, "å¯†é’¥ç®¡ç†": 108, "åŒºå—é“¾": 109,
            "å¯†ç å­¦": 110, "åŠ å¯†ç®—æ³•": 111, "å®‰å…¨æ€§": 112
        }
        
        # å¸¸ç”¨ä¸­æ–‡è¯æ±‡
        self.chinese_vocab = {
            "ä»€ä¹ˆ": 200, "æ˜¯": 201, "çš„": 202, "å¦‚ä½•": 203,
            "ä¸ºä»€ä¹ˆ": 204, "æ€ä¹ˆ": 205, "è§£é‡Š": 206, "åˆ†æ": 207,
            "åŸç†": 208, "å·¥ä½œ": 209, "ç®—æ³•": 210, "æŠ€æœ¯": 211
        }
    
    def __call__(self, text, return_tensors=None, padding=False, truncation=False, max_length=512, **kwargs):
        """ç¼–ç æ–‡æœ¬"""
        tokens = self.encode(text)
        
        if truncation and len(tokens) > max_length:
            tokens = tokens[:max_length]
        
        if padding and len(tokens) < max_length:
            tokens.extend([self.pad_token_id] * (max_length - len(tokens)))
        
        result = {
            "input_ids": tokens,
            "attention_mask": [1 if t != self.pad_token_id else 0 for t in tokens]
        }
        
        if return_tensors == "pt":
            result = {k: torch.tensor([v]) for k, v in result.items()}
        
        return result
    
    def encode(self, text):
        """ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—"""
        tokens = [self.bos_token_id]  # å¼€å§‹token
        
        # æ£€æŸ¥ç‰¹æ®Štoken
        for token, token_id in self.special_tokens.items():
            if token in text:
                tokens.append(token_id)
        
        # æ£€æŸ¥å¯†ç å­¦è¯æ±‡
        for term, token_id in self.crypto_vocab.items():
            if term in text:
                tokens.append(token_id)
        
        # æ£€æŸ¥ä¸­æ–‡è¯æ±‡
        for word, token_id in self.chinese_vocab.items():
            if word in text:
                tokens.append(token_id)
        
        # æ·»åŠ ä¸€äº›éšæœºtokenæ¨¡æ‹Ÿå…¶ä»–è¯æ±‡
        import random
        tokens.extend(random.randint(300, 1000) for _ in range(min(10, len(text) // 5)))
        
        tokens.append(self.eos_token_id)  # ç»“æŸtoken
        return tokens
    
    def decode(self, tokens, skip_special_tokens=False):
        """è§£ç tokenåºåˆ—ä¸ºæ–‡æœ¬"""
        if isinstance(tokens, torch.Tensor):
            tokens = tokens.tolist()
        
        # å¦‚æœæ˜¯äºŒç»´tensorï¼Œå–ç¬¬ä¸€è¡Œ
        if isinstance(tokens[0], list):
            tokens = tokens[0]
        
        # åå‘æ˜ å°„
        reverse_special = {v: k for k, v in self.special_tokens.items()}
        reverse_crypto = {v: k for k, v in self.crypto_vocab.items()}
        reverse_chinese = {v: k for k, v in self.chinese_vocab.items()}
        
        text_parts = []
        
        for token in tokens:
            if skip_special_tokens and token in reverse_special:
                continue
            elif token in reverse_special:
                text_parts.append(reverse_special[token])
            elif token in reverse_crypto:
                term = reverse_crypto[token]
                text_parts.append(f"{term}æ˜¯ä¸€ç§é‡è¦çš„å¯†ç å­¦æŠ€æœ¯")
            elif token in reverse_chinese:
                text_parts.append(reverse_chinese[token])
            elif token > 300:  # å…¶ä»–è¯æ±‡
                text_parts.append("ç›¸å…³æŠ€æœ¯æ¦‚å¿µ")
        
        return " ".join(text_parts) if text_parts else "è¿™æ˜¯ä¸€ä¸ªå…³äºå¯†ç å­¦çš„ä¸­æ–‡å›ç­”ã€‚"
    
    def save_pretrained(self, save_directory):
        """ä¿å­˜åˆ†è¯å™¨"""
        save_path = Path(save_directory)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯æ±‡è¡¨
        vocab = {}
        vocab.update(self.special_tokens)
        vocab.update(self.crypto_vocab)
        vocab.update(self.chinese_vocab)
        
        # å¡«å……å‰©ä½™è¯æ±‡è¡¨
        for i in range(len(vocab), self.vocab_size):
            vocab[f"token_{i}"] = i
        
        with open(save_path / "vocab.json", 'w', encoding='utf-8') as f:
            json.dump(vocab, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜åˆ†è¯å™¨é…ç½®
        tokenizer_config = {
            "vocab_size": self.vocab_size,
            "pad_token_id": self.pad_token_id,
            "eos_token_id": self.eos_token_id,
            "bos_token_id": self.bos_token_id,
            "unk_token_id": self.unk_token_id,
            "model_max_length": 2048,
            "tokenizer_class": "DemoQwenTokenizer"
        }
        
        with open(save_path / "tokenizer_config.json", 'w', encoding='utf-8') as f:
            json.dump(tokenizer_config, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºtokenizer.jsonï¼ˆç®€åŒ–ç‰ˆï¼‰
        tokenizer_json = {
            "version": "1.0",
            "truncation": None,
            "padding": None,
            "added_tokens": [
                {"id": token_id, "content": token, "single_word": False, "lstrip": False, "rstrip": False, "normalized": False, "special": True}
                for token, token_id in self.special_tokens.items()
            ],
            "normalizer": None,
            "pre_tokenizer": None,
            "post_processor": None,
            "decoder": None,
            "model": {
                "type": "WordLevel",
                "vocab": vocab,
                "unk_token": "<unk>"
            }
        }
        
        with open(save_path / "tokenizer.json", 'w', encoding='utf-8') as f:
            json.dump(tokenizer_json, f, indent=2, ensure_ascii=False)


def demonstrate_model_export_workflow():
    """æ¼”ç¤ºå®Œæ•´çš„æ¨¡å‹å¯¼å‡ºå·¥ä½œæµ"""
    print("=" * 70)
    print("æ¨¡å‹å¯¼å‡ºå’Œéƒ¨ç½²åŒ…ç”Ÿæˆæ¼”ç¤º")
    print("=" * 70)
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    print(f"\nä½¿ç”¨ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    try:
        # 1. åˆ›å»ºæ¼”ç¤ºæ¨¡å‹å’Œåˆ†è¯å™¨
        print("\n1. åˆ›å»ºæ¼”ç¤ºæ¨¡å‹å’Œåˆ†è¯å™¨...")
        model = DemoQwenModel(vocab_size=50000, hidden_size=768, num_layers=6)
        tokenizer = DemoQwenTokenizer()
        
        print(f"   æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   æ¨¡å‹å¤§å°: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024:.2f} MB")
        print(f"   åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
        
        # 2. æµ‹è¯•æ¨¡å‹åŸºæœ¬åŠŸèƒ½
        print("\n2. æµ‹è¯•æ¨¡å‹åŸºæœ¬åŠŸèƒ½...")
        test_input = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model(**inputs)
            print(f"   è¾“å…¥: {test_input}")
            print(f"   è¾“å…¥tokenæ•°é‡: {inputs['input_ids'].shape[1]}")
            print(f"   è¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
        
        # 3. é…ç½®é‡åŒ–å‚æ•°
        print("\n3. é…ç½®é‡åŒ–å‚æ•°...")
        quantization_configs = [
            QuantizationConfig(
                format=QuantizationFormat.DYNAMIC,
                backend=QuantizationBackend.PYTORCH
            ),
            # å¯ä»¥æ·»åŠ æ›´å¤šé‡åŒ–é…ç½®
        ]
        
        for i, config in enumerate(quantization_configs, 1):
            print(f"   é…ç½® {i}: {config.format.value} ({config.backend.value})")
        
        # 4. æ‰§è¡Œæ¨¡å‹å¯¼å‡º
        print("\n4. æ‰§è¡Œæ¨¡å‹å¯¼å‡º...")
        exporter = ModelExporter()
        
        for i, config in enumerate(quantization_configs, 1):
            print(f"\n   å¯¼å‡ºé…ç½® {i}: {config.format.value}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path(temp_dir) / f"export_{config.format.value}"
            
            # æ‰§è¡Œå¯¼å‡º
            deployment_package = exporter.export_quantized_model(
                model=model,
                tokenizer=tokenizer,
                output_dir=str(output_dir),
                quantization_config=config,
                model_name=f"qwen3-4b-thinking-{config.format.value}"
            )
            
            print(f"   âœ… å¯¼å‡ºæˆåŠŸ!")
            print(f"   ğŸ“¦ éƒ¨ç½²åŒ…è·¯å¾„: {deployment_package.package_path}")
            print(f"   ğŸ“Š åŒ…å¤§å°: {deployment_package.package_size_mb:.2f} MB")
            print(f"   ğŸ” æ ¡éªŒå’Œ: {deployment_package.checksum[:16]}...")
            
            # éªŒè¯å¯¼å‡ºçš„æ–‡ä»¶
            print(f"   ğŸ“ åŒ…å«æ–‡ä»¶:")
            for file_path in deployment_package.model_files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"      - {Path(file_path).name}")
            if len(deployment_package.model_files) > 3:
                print(f"      - ... è¿˜æœ‰ {len(deployment_package.model_files) - 3} ä¸ªæ–‡ä»¶")
            
            # æ˜¾ç¤ºé…ç½®æ–‡ä»¶
            print(f"   âš™ï¸  é…ç½®æ–‡ä»¶:")
            for config_file in deployment_package.config_files:
                print(f"      - {Path(config_file).name}")
        
        # 5. éªŒè¯éƒ¨ç½²åŒ…
        print("\n5. éªŒè¯éƒ¨ç½²åŒ…...")
        for i, config in enumerate(quantization_configs, 1):
            output_dir = Path(temp_dir) / f"export_{config.format.value}"
            
            print(f"\n   éªŒè¯é…ç½® {i}: {config.format.value}")
            
            # æ£€æŸ¥å¿…è¦æ–‡ä»¶
            required_files = [
                "model/pytorch_model.bin",
                "model/config.json",
                "model/tokenizer.json",
                "model/tokenizer_config.json",
                "metadata.json",
                "README.md",
                "requirements.txt"
            ]
            
            missing_files = []
            for file_path in required_files:
                if not (output_dir / file_path).exists():
                    missing_files.append(file_path)
            
            if missing_files:
                print(f"   âŒ ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            else:
                print(f"   âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å­˜åœ¨")
            
            # éªŒè¯å…ƒæ•°æ®
            metadata_path = output_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                print(f"   ğŸ“‹ å…ƒæ•°æ®éªŒè¯:")
                print(f"      - æ¨¡å‹åç§°: {metadata['model_name']}")
                print(f"      - é‡åŒ–æ ¼å¼: {metadata['quantization_format']}")
                print(f"      - å‹ç¼©æ¯”: {metadata['compression_ratio']:.2f}x")
                print(f"      - æ”¯æŒè¯­è¨€: {', '.join(metadata['supported_languages'])}")
                print(f"      - ä¸“ä¸šé¢†åŸŸ: {', '.join(metadata['specialized_domains'])}")
            
            # éªŒè¯README
            readme_path = output_dir / "README.md"
            if readme_path.exists():
                readme_content = readme_path.read_text(encoding='utf-8')
                print(f"   ğŸ“– READMEéªŒè¯:")
                print(f"      - æ–‡ä»¶å¤§å°: {len(readme_content)} å­—ç¬¦")
                print(f"      - åŒ…å«å®‰è£…è¯´æ˜: {'âœ…' if 'å®‰è£…ä¾èµ–' in readme_content else 'âŒ'}")
                print(f"      - åŒ…å«ä½¿ç”¨ç¤ºä¾‹: {'âœ…' if 'ä½¿ç”¨ç¤ºä¾‹' in readme_content else 'âŒ'}")
                print(f"      - åŒ…å«æ€§èƒ½æŒ‡æ ‡: {'âœ…' if 'æ€§èƒ½æŒ‡æ ‡' in readme_content else 'âŒ'}")
        
        # 6. ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š
        print("\n6. ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š...")
        report = generate_deployment_report(temp_dir, quantization_configs)
        
        report_path = Path(temp_dir) / "deployment_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"   ğŸ“Š éƒ¨ç½²æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        print(f"   ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"      - å¯¼å‡ºé…ç½®æ•°é‡: {report['summary']['total_configs']}")
        print(f"      - æˆåŠŸå¯¼å‡ºæ•°é‡: {report['summary']['successful_exports']}")
        print(f"      - æ€»åŒ…å¤§å°: {report['summary']['total_package_size_mb']:.2f} MB")
        print(f"      - å¹³å‡å‹ç¼©æ¯”: {report['summary']['average_compression_ratio']:.2f}x")
        
        # 7. æ¼”ç¤ºéƒ¨ç½²åŒ…ä½¿ç”¨
        print("\n7. æ¼”ç¤ºéƒ¨ç½²åŒ…ä½¿ç”¨...")
        demonstrate_deployment_usage(temp_dir, quantization_configs[0])
        
        print("\n" + "=" * 70)
        print("âœ… æ¨¡å‹å¯¼å‡ºå’Œéƒ¨ç½²åŒ…ç”Ÿæˆæ¼”ç¤ºå®Œæˆ!")
        print("=" * 70)
        
        print(f"\nğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {temp_dir}")
        print("ğŸ’¡ æç¤º: åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¯·ä½¿ç”¨çœŸå®çš„Qwen3-4B-Thinkingæ¨¡å‹")
        
        return temp_dir, report
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None
    
    finally:
        # æ³¨æ„ï¼šåœ¨å®é™…ä½¿ç”¨ä¸­å¯èƒ½ä¸æƒ³è‡ªåŠ¨åˆ é™¤ä¸´æ—¶ç›®å½•
        print(f"\nğŸ—‘ï¸  ä¸´æ—¶ç›®å½•å°†ä¿ç•™ä»¥ä¾›æ£€æŸ¥: {temp_dir}")


def generate_deployment_report(base_dir, configs):
    """ç”Ÿæˆéƒ¨ç½²æŠ¥å‘Š"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_configs": len(configs),
            "successful_exports": 0,
            "total_package_size_mb": 0.0,
            "average_compression_ratio": 0.0
        },
        "exports": []
    }
    
    total_compression_ratio = 0.0
    
    for config in configs:
        export_dir = Path(base_dir) / f"export_{config.format.value}"
        
        if export_dir.exists():
            # è¯»å–å…ƒæ•°æ®
            metadata_path = export_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # è®¡ç®—ç›®å½•å¤§å°
                package_size = sum(
                    f.stat().st_size for f in export_dir.rglob('*') if f.is_file()
                ) / 1024 / 1024
                
                export_info = {
                    "config": config.format.value,
                    "success": True,
                    "package_size_mb": package_size,
                    "compression_ratio": metadata.get("compression_ratio", 1.0),
                    "model_name": metadata.get("model_name", "unknown"),
                    "quantization_format": metadata.get("quantization_format", "unknown"),
                    "chinese_capability_score": metadata.get("performance_metrics", {}).get("chinese_capability_score", 0.0),
                    "crypto_term_accuracy": metadata.get("performance_metrics", {}).get("crypto_term_accuracy", 0.0)
                }
                
                report["exports"].append(export_info)
                report["summary"]["successful_exports"] += 1
                report["summary"]["total_package_size_mb"] += package_size
                total_compression_ratio += metadata.get("compression_ratio", 1.0)
    
    if report["summary"]["successful_exports"] > 0:
        report["summary"]["average_compression_ratio"] = total_compression_ratio / report["summary"]["successful_exports"]
    
    return report


def demonstrate_deployment_usage(base_dir, config):
    """æ¼”ç¤ºéƒ¨ç½²åŒ…çš„ä½¿ç”¨æ–¹æ³•"""
    print(f"\n   æ¼”ç¤º {config.format.value} éƒ¨ç½²åŒ…ä½¿ç”¨:")
    
    export_dir = Path(base_dir) / f"export_{config.format.value}"
    
    if not export_dir.exists():
        print("   âŒ éƒ¨ç½²åŒ…ä¸å­˜åœ¨")
        return
    
    # 1. æ£€æŸ¥README
    readme_path = export_dir / "README.md"
    if readme_path.exists():
        print("   ğŸ“– README.md å¯ç”¨äºç”¨æˆ·æŒ‡å¯¼")
    
    # 2. æ£€æŸ¥requirements
    requirements_path = export_dir / "requirements.txt"
    if requirements_path.exists():
        print("   ğŸ“¦ requirements.txt åŒ…å«ä¾èµ–ä¿¡æ¯")
        requirements = requirements_path.read_text(encoding='utf-8')
        print(f"      ä¸»è¦ä¾èµ–: {', '.join(req.split('>=')[0] for req in requirements.split('\\n')[:3] if req.strip())}")
    
    # 3. æ¨¡æ‹ŸåŠ è½½æ¨¡å‹
    print("   ğŸ”„ æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½è¿‡ç¨‹:")
    model_dir = export_dir / "model"
    
    if (model_dir / "config.json").exists():
        with open(model_dir / "config.json", 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        print(f"      âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ (vocab_size: {config_data.get('vocab_size', 'unknown')})")
    
    if (model_dir / "pytorch_model.bin").exists():
        model_size = (model_dir / "pytorch_model.bin").stat().st_size / 1024 / 1024
        print(f"      âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ (å¤§å°: {model_size:.2f} MB)")
    
    if (model_dir / "tokenizer_config.json").exists():
        print("      âœ… åˆ†è¯å™¨é…ç½®åŠ è½½æˆåŠŸ")
    
    # 4. éªŒè¯ç»“æœæ£€æŸ¥
    validation_path = export_dir / "validation_results.json"
    if validation_path.exists():
        with open(validation_path, 'r', encoding='utf-8') as f:
            validation_data = json.load(f)
        
        print("   ğŸ§ª ä¸­æ–‡èƒ½åŠ›éªŒè¯ç»“æœ:")
        print(f"      - æ€»ä½“å¾—åˆ†: {validation_data.get('overall_score', 0):.2%}")
        print(f"      - å¯†ç å­¦æœ¯è¯­å‡†ç¡®æ€§: {validation_data.get('crypto_term_accuracy', 0):.2%}")
        print(f"      - æ€è€ƒç»“æ„ä¿æŒ: {validation_data.get('thinking_structure_preservation', 0):.2%}")
    
    print("   âœ… éƒ¨ç½²åŒ…éªŒè¯å®Œæˆï¼Œå¯ç”¨äºç”Ÿäº§éƒ¨ç½²")


def create_deployment_guide():
    """åˆ›å»ºéƒ¨ç½²æŒ‡å—"""
    guide_content = """# æ¨¡å‹éƒ¨ç½²æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\\Scripts\\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ¨¡å‹åŠ è½½

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("./model")

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "./model",
    device_map="auto",
    trust_remote_code=True
)
```

### 3. åŸºæœ¬ä½¿ç”¨

```python
# åŸºç¡€é—®ç­”
def ask_question(question):
    inputs = tokenizer(question, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=200,
        temperature=0.7,
        do_sample=True
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# ç¤ºä¾‹
question = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
answer = ask_question(question)
print(answer)
```

### 4. æ·±åº¦æ€è€ƒæ¨¡å¼

```python
# ä½¿ç”¨thinkingæ ‡ç­¾è¿›è¡Œæ·±åº¦æ¨ç†
def deep_thinking(question):
    thinking_prompt = f"<thinking>è®©æˆ‘ä»”ç»†åˆ†æè¿™ä¸ªé—®é¢˜</thinking>{question}"
    inputs = tokenizer(thinking_prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=500,
        temperature=0.7,
        do_sample=True
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# ç¤ºä¾‹
question = "åˆ†æRSAç®—æ³•çš„å®‰å…¨æ€§"
answer = deep_thinking(question)
print(answer)
```

## æ€§èƒ½ä¼˜åŒ–

### GPUåŠ é€Ÿ
- ç¡®ä¿CUDAå¯ç”¨
- ä½¿ç”¨é€‚å½“çš„device_map
- è€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦

### å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨é‡åŒ–æ¨¡å‹å‡å°‘å†…å­˜å ç”¨
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- è°ƒæ•´æ‰¹æ¬¡å¤§å°

### æ¨ç†ä¼˜åŒ–
- ä½¿ç”¨ç¼“å­˜æœºåˆ¶
- æ‰¹é‡å¤„ç†è¯·æ±‚
- è€ƒè™‘ä½¿ç”¨TensorRTç­‰æ¨ç†å¼•æ“

## éƒ¨ç½²é€‰é¡¹

### æœ¬åœ°éƒ¨ç½²
- ç›´æ¥ä½¿ç”¨Pythonè„šæœ¬
- åˆ›å»ºFlask/FastAPIæœåŠ¡
- ä½¿ç”¨Gradioåˆ›å»ºWebç•Œé¢

### äº‘ç«¯éƒ¨ç½²
- ä½¿ç”¨Dockerå®¹å™¨åŒ–
- éƒ¨ç½²åˆ°äº‘æœåŠ¡å™¨
- ä½¿ç”¨Kubernetesç¼–æ’

### è¾¹ç¼˜éƒ¨ç½²
- ä½¿ç”¨ONNXæ ¼å¼
- ç§»åŠ¨ç«¯éƒ¨ç½²
- åµŒå…¥å¼è®¾å¤‡éƒ¨ç½²

## ç›‘æ§å’Œç»´æŠ¤

### æ€§èƒ½ç›‘æ§
- å“åº”æ—¶é—´ç›‘æ§
- å†…å­˜ä½¿ç”¨ç›‘æ§
- GPUåˆ©ç”¨ç‡ç›‘æ§

### è´¨é‡ç›‘æ§
- è¾“å‡ºè´¨é‡è¯„ä¼°
- ç”¨æˆ·åé¦ˆæ”¶é›†
- A/Bæµ‹è¯•

### æ›´æ–°ç»´æŠ¤
- æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- å¢é‡æ›´æ–°
- å›æ»šæœºåˆ¶

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
1. å†…å­˜ä¸è¶³ - å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨é‡åŒ–
2. æ¨ç†é€Ÿåº¦æ…¢ - æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
3. è¾“å‡ºè´¨é‡å·® - è°ƒæ•´ç”Ÿæˆå‚æ•°

### æ—¥å¿—åˆ†æ
- å¯ç”¨è¯¦ç»†æ—¥å¿—
- ç›‘æ§é”™è¯¯ä¿¡æ¯
- æ€§èƒ½åˆ†æ

## å®‰å…¨è€ƒè™‘

### è¾“å…¥éªŒè¯
- è¿‡æ»¤æ¶æ„è¾“å…¥
- é™åˆ¶è¾“å…¥é•¿åº¦
- å†…å®¹å®‰å…¨æ£€æŸ¥

### è¾“å‡ºè¿‡æ»¤
- æ•æ„Ÿä¿¡æ¯è¿‡æ»¤
- å†…å®¹åˆè§„æ£€æŸ¥
- è´¨é‡æ§åˆ¶

### è®¿é—®æ§åˆ¶
- APIè®¤è¯
- é€Ÿç‡é™åˆ¶
- ç”¨æˆ·æƒé™ç®¡ç†
"""
    
    return guide_content


def main():
    """ä¸»å‡½æ•°"""
    print("æ¨¡å‹å¯¼å‡ºå’Œéƒ¨ç½²åŒ…ç”Ÿæˆç³»ç»Ÿæ¼”ç¤º")
    print("=" * 70)
    
    try:
        # æ‰§è¡Œå®Œæ•´çš„å¯¼å‡ºæ¼”ç¤º
        temp_dir, report = demonstrate_model_export_workflow()
        
        if temp_dir and report:
            # åˆ›å»ºéƒ¨ç½²æŒ‡å—
            guide_content = create_deployment_guide()
            guide_path = Path(temp_dir) / "deployment_guide.md"
            guide_path.write_text(guide_content, encoding='utf-8')
            
            print(f"\nğŸ“š éƒ¨ç½²æŒ‡å—å·²åˆ›å»º: {guide_path}")
            
            # æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“
            print("\n" + "=" * 70)
            print("ğŸ‰ æ¼”ç¤ºæ€»ç»“")
            print("=" * 70)
            
            print(f"âœ… æˆåŠŸå¯¼å‡º {report['summary']['successful_exports']} ä¸ªæ¨¡å‹é…ç½®")
            print(f"ğŸ“¦ æ€»åŒ…å¤§å°: {report['summary']['total_package_size_mb']:.2f} MB")
            print(f"ğŸ—œï¸  å¹³å‡å‹ç¼©æ¯”: {report['summary']['average_compression_ratio']:.2f}x")
            print(f"ğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: {temp_dir}")
            
            print("\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
            print("   - é‡åŒ–æ¨¡å‹æ–‡ä»¶")
            print("   - é…ç½®å’Œå…ƒæ•°æ®")
            print("   - READMEå’Œä½¿ç”¨è¯´æ˜")
            print("   - éƒ¨ç½²æŒ‡å—")
            print("   - éªŒè¯æŠ¥å‘Š")
            
            print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
            print("   1. æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶")
            print("   2. æ ¹æ®READMEè¿›è¡Œéƒ¨ç½²æµ‹è¯•")
            print("   3. ä½¿ç”¨éƒ¨ç½²æŒ‡å—è¿›è¡Œç”Ÿäº§éƒ¨ç½²")
            print("   4. ç›‘æ§æ¨¡å‹æ€§èƒ½å’Œè´¨é‡")
            
        else:
            print("âŒ æ¼”ç¤ºæœªèƒ½æˆåŠŸå®Œæˆ")
            
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()