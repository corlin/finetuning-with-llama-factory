#!/usr/bin/env python3
"""
ä¸“å®¶è¯„ä¼°ç³»ç»Ÿå®Œæ•´æ¼”ç¤ºè„šæœ¬

æœ¬è„šæœ¬å±•ç¤ºäº†ä¸“å®¶è¯„ä¼°ç³»ç»Ÿçš„å®Œæ•´ä½¿ç”¨æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. ç³»ç»Ÿåˆå§‹åŒ–å’Œé…ç½®
2. æ¨¡å‹åŠ è½½å’ŒéªŒè¯
3. æ•°æ®å‡†å¤‡å’ŒéªŒè¯
4. å•é¡¹è¯„ä¼°æ¼”ç¤º
5. æ‰¹é‡è¯„ä¼°æ¼”ç¤º
6. ç»“æœåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
7. æ€§èƒ½åŸºå‡†æµ‹è¯•
8. APIæœåŠ¡æ¼”ç¤º

ä½¿ç”¨æ–¹æ³•:
    uv run python examples/expert_evaluation_demo.py

ä½œè€…: ä¸“å®¶è¯„ä¼°ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0
"""

import json
import time
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.expert_evaluation.engine import ExpertEvaluationEngine
    from src.expert_evaluation.config import ExpertEvaluationConfig
    from src.expert_evaluation.data_manager import EvaluationDataManager
    from src.expert_evaluation.report_generator import EvaluationReportGenerator
    from src.expert_evaluation.performance import PerformanceBenchmark
    from src.expert_evaluation.data_models import QAEvaluationItem
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬ï¼Œå¹¶å·²æ­£ç¡®å®‰è£…ä¾èµ–")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ExpertEvaluationDemo:
    """ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¼”ç¤ºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤ºç¯å¢ƒ"""
        self.demo_data_dir = Path("demo_data")
        self.demo_output_dir = Path("demo_output")
        self.config = None
        self.engine = None
        self.data_manager = None
        
        # åˆ›å»ºæ¼”ç¤ºç›®å½•
        self.demo_data_dir.mkdir(exist_ok=True)
        self.demo_output_dir.mkdir(exist_ok=True)
        
        print("ğŸš€ ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¼”ç¤ºå¼€å§‹")
        print("=" * 60)
    
    def step_1_system_initialization(self):
        """æ­¥éª¤1: ç³»ç»Ÿåˆå§‹åŒ–å’Œé…ç½®"""
        print("\nğŸ“‹ æ­¥éª¤1: ç³»ç»Ÿåˆå§‹åŒ–å’Œé…ç½®")
        print("-" * 40)
        
        try:
            # åˆ›å»ºæ¼”ç¤ºé…ç½®
            demo_config = {
                "model": {
                    "model_path": "",  # å°†åœ¨è¿è¡Œæ—¶è®¾ç½®
                    "device": "auto",
                    "quantization": None,
                    "max_length": 1024,
                    "batch_size": 1
                },
                "evaluation": {
                    "dimensions": [
                        "semantic_similarity",
                        "domain_accuracy",
                        "response_relevance",
                        "factual_correctness",
                        "completeness"
                    ],
                    "weights": {
                        "semantic_similarity": 0.25,
                        "domain_accuracy": 0.25,
                        "response_relevance": 0.20,
                        "factual_correctness": 0.15,
                        "completeness": 0.15
                    },
                    "thresholds": {
                        "min_score": 0.6,
                        "confidence_level": 0.95
                    }
                },
                "performance": {
                    "max_workers": 2,
                    "timeout": 120,
                    "memory_limit": "4GB",
                    "cache_size": "512MB"
                },
                "output": {
                    "format": "json",
                    "detailed": True,
                    "save_intermediate": False
                },
                "logging": {
                    "level": "INFO"
                }
            }
            
            # ä¿å­˜æ¼”ç¤ºé…ç½®
            config_path = self.demo_data_dir / "demo_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(demo_config, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… æ¼”ç¤ºé…ç½®å·²åˆ›å»º: {config_path}")
            
            # åŠ è½½é…ç½®
            self.config = ExpertEvaluationConfig.from_dict(demo_config)
            print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
            
            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            print(f"ğŸ“Š è¯„ä¼°ç»´åº¦: {len(self.config.evaluation_dimensions)}ä¸ª")
            print(f"âš™ï¸  è®¾å¤‡é…ç½®: {self.config.device}")
            print(f"ğŸ”§ æ€§èƒ½é…ç½®: {self.config.max_workers}ä¸ªå·¥ä½œçº¿ç¨‹")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def step_2_model_loading(self):
        """æ­¥éª¤2: æ¨¡å‹åŠ è½½å’ŒéªŒè¯"""
        print("\nğŸ¤– æ­¥éª¤2: æ¨¡å‹åŠ è½½å’ŒéªŒè¯")
        print("-" * 40)
        
        try:
            # åˆå§‹åŒ–è¯„ä¼°å¼•æ“
            self.engine = ExpertEvaluationEngine(self.config)
            print("âœ… è¯„ä¼°å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            
            # æ³¨æ„: åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½çœŸå®çš„æ¨¡å‹
            # ä¸ºäº†æ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
            print("â„¹ï¸  æ¼”ç¤ºæ¨¡å¼: ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹è¿›è¡Œæ¼”ç¤º")
            print("   åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¯·é…ç½®çœŸå®çš„æ¨¡å‹è·¯å¾„")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            print("ğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯:")
            print(f"   - è®¾å¤‡: {self.config.device}")
            print(f"   - é‡åŒ–: {self.config.quantization or 'æœªå¯ç”¨'}")
            print(f"   - æœ€å¤§é•¿åº¦: {self.config.max_length}")
            print(f"   - æ‰¹å¤„ç†å¤§å°: {self.config.batch_size}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def step_3_data_preparation(self):
        """æ­¥éª¤3: æ•°æ®å‡†å¤‡å’ŒéªŒè¯"""
        print("\nğŸ“Š æ­¥éª¤3: æ•°æ®å‡†å¤‡å’ŒéªŒè¯")
        print("-" * 40)
        
        try:
            # åˆ›å»ºæ¼”ç¤ºQAæ•°æ®
            demo_qa_data = [
                {
                    "question_id": "crypto_001",
                    "question": "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿè¯·è¯¦ç»†è¯´æ˜å…¶å·¥ä½œåŸç†ã€‚",
                    "context": "å¯†ç å­¦åŸºç¡€çŸ¥è¯†",
                    "reference_answer": "AESï¼ˆAdvanced Encryption Standardï¼‰æ˜¯ä¸€ç§å¯¹ç§°åŠ å¯†ç®—æ³•ï¼Œé‡‡ç”¨åˆ†ç»„å¯†ç ä½“åˆ¶ã€‚å®ƒä½¿ç”¨128ä½åˆ†ç»„é•¿åº¦ï¼Œæ”¯æŒ128ã€192ã€256ä½å¯†é’¥é•¿åº¦ã€‚AESç®—æ³•åŸºäºä»£æ›¿-ç½®æ¢ç½‘ç»œï¼ˆSPNï¼‰ç»“æ„ï¼ŒåŒ…å«å­—èŠ‚ä»£æ›¿ã€è¡Œç§»ä½ã€åˆ—æ··åˆå’Œè½®å¯†é’¥åŠ ç­‰æ“ä½œã€‚",
                    "model_answer": "AESæ˜¯é«˜çº§åŠ å¯†æ ‡å‡†ï¼Œæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„å¯¹ç§°åŠ å¯†ç®—æ³•ã€‚å®ƒå°†æ•°æ®åˆ†æˆ128ä½çš„å—è¿›è¡ŒåŠ å¯†ï¼Œæ”¯æŒä¸åŒé•¿åº¦çš„å¯†é’¥ã€‚AESç®—æ³•å®‰å…¨æ€§é«˜ï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œè¢«å¹¿æ³›åº”ç”¨äºå„ç§å®‰å…¨ç³»ç»Ÿä¸­ã€‚",
                    "domain_tags": ["å¯†ç å­¦", "å¯¹ç§°åŠ å¯†", "åˆ†ç»„å¯†ç "],
                    "difficulty_level": "intermediate",
                    "expected_concepts": ["å¯¹ç§°åŠ å¯†", "åˆ†ç»„å¯†ç ", "å¯†é’¥é•¿åº¦", "åŠ å¯†è½®æ•°"]
                },
                {
                    "question_id": "crypto_002", 
                    "question": "RSAç®—æ³•çš„å®‰å…¨æ€§åŸºäºä»€ä¹ˆæ•°å­¦éš¾é¢˜ï¼Ÿ",
                    "context": "å…¬é’¥å¯†ç å­¦",
                    "reference_answer": "RSAç®—æ³•çš„å®‰å…¨æ€§åŸºäºå¤§æ•´æ•°åˆ†è§£çš„æ•°å­¦éš¾é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæ˜¯åŸºäºåœ¨è®¡ç®—ä¸Šéš¾ä»¥å°†ä¸¤ä¸ªå¤§ç´ æ•°çš„ä¹˜ç§¯è¿›è¡Œå› å¼åˆ†è§£ã€‚RSAçš„å…¬é’¥åŒ…å«ä¸€ä¸ªå¤§åˆæ•°n=pÃ—qï¼Œå…¶ä¸­på’Œqæ˜¯ä¸¤ä¸ªå¤§ç´ æ•°ã€‚æ”»å‡»è€…éœ€è¦åˆ†è§£næ‰èƒ½è·å¾—ç§é’¥ï¼Œä½†ç›®å‰æ²¡æœ‰æœ‰æ•ˆçš„ç®—æ³•èƒ½åœ¨å¤šé¡¹å¼æ—¶é—´å†…åˆ†è§£å¤§æ•´æ•°ã€‚",
                    "model_answer": "RSAç®—æ³•çš„å®‰å…¨æ€§ä¾èµ–äºå¤§æ•°åˆ†è§£é—®é¢˜çš„å›°éš¾æ€§ã€‚å½“ä¸¤ä¸ªå¤§ç´ æ•°ç›¸ä¹˜å¾—åˆ°ä¸€ä¸ªåˆæ•°æ—¶ï¼Œè¦ä»è¿™ä¸ªåˆæ•°åæ¨å‡ºåŸæ¥çš„ä¸¤ä¸ªç´ æ•°æ˜¯éå¸¸å›°éš¾çš„ã€‚è¿™ä¸ªæ•°å­¦éš¾é¢˜ä¿è¯äº†RSAåŠ å¯†çš„å®‰å…¨æ€§ã€‚",
                    "domain_tags": ["å¯†ç å­¦", "å…¬é’¥åŠ å¯†", "æ•°è®º"],
                    "difficulty_level": "advanced",
                    "expected_concepts": ["å¤§æ•´æ•°åˆ†è§£", "ç´ æ•°", "å…¬é’¥", "ç§é’¥", "æ•°å­¦éš¾é¢˜"]
                },
                {
                    "question_id": "crypto_003",
                    "question": "ä»€ä¹ˆæ˜¯æ•°å­—ç­¾åï¼Ÿå®ƒå¦‚ä½•ä¿è¯æ•°æ®çš„å®Œæ•´æ€§å’Œè®¤è¯æ€§ï¼Ÿ",
                    "context": "æ•°å­—ç­¾åæŠ€æœ¯",
                    "reference_answer": "æ•°å­—ç­¾åæ˜¯ä¸€ç§æ•°å­¦æœºåˆ¶ï¼Œç”¨äºéªŒè¯æ•°å­—æ¶ˆæ¯æˆ–æ–‡æ¡£çš„çœŸå®æ€§ã€‚å®ƒåŸºäºå…¬é’¥å¯†ç å­¦ï¼Œä½¿ç”¨ç§é’¥å¯¹æ¶ˆæ¯çš„å“ˆå¸Œå€¼è¿›è¡ŒåŠ å¯†ç”Ÿæˆç­¾åï¼Œæ¥æ”¶æ–¹ç”¨å¯¹åº”çš„å…¬é’¥éªŒè¯ç­¾åã€‚æ•°å­—ç­¾åæä¾›ä¸‰ä¸ªå®‰å…¨å±æ€§ï¼š1ï¼‰è®¤è¯æ€§-ç¡®è®¤æ¶ˆæ¯æ¥æºï¼›2ï¼‰å®Œæ•´æ€§-æ£€æµ‹æ¶ˆæ¯æ˜¯å¦è¢«ç¯¡æ”¹ï¼›3ï¼‰ä¸å¯å¦è®¤æ€§-å‘é€æ–¹æ— æ³•å¦è®¤å‘é€è¿‡è¯¥æ¶ˆæ¯ã€‚",
                    "model_answer": "æ•°å­—ç­¾åæ˜¯ç”¨æ¥éªŒè¯ç”µå­æ–‡æ¡£çœŸå®æ€§çš„æŠ€æœ¯ã€‚å‘é€æ–¹ç”¨è‡ªå·±çš„ç§é’¥å¯¹æ–‡æ¡£è¿›è¡Œç­¾åï¼Œæ¥æ”¶æ–¹ç”¨å‘é€æ–¹çš„å…¬é’¥æ¥éªŒè¯ç­¾åã€‚å¦‚æœéªŒè¯æˆåŠŸï¼Œè¯´æ˜æ–‡æ¡£ç¡®å®æ¥è‡ªå‘é€æ–¹ä¸”æœªè¢«ä¿®æ”¹ã€‚è¿™æ ·å°±ä¿è¯äº†æ•°æ®çš„å®Œæ•´æ€§å’Œå‘é€æ–¹çš„èº«ä»½è®¤è¯ã€‚",
                    "domain_tags": ["å¯†ç å­¦", "æ•°å­—ç­¾å", "èº«ä»½è®¤è¯"],
                    "difficulty_level": "intermediate",
                    "expected_concepts": ["æ•°å­—ç­¾å", "å“ˆå¸Œå‡½æ•°", "å…¬é’¥", "ç§é’¥", "å®Œæ•´æ€§", "è®¤è¯æ€§", "ä¸å¯å¦è®¤æ€§"]
                }
            ]
            
            # ä¿å­˜æ¼”ç¤ºæ•°æ®
            qa_data_path = self.demo_data_dir / "demo_qa_data.json"
            with open(qa_data_path, 'w', encoding='utf-8') as f:
                json.dump(demo_qa_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… æ¼”ç¤ºQAæ•°æ®å·²åˆ›å»º: {qa_data_path}")
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(demo_qa_data)}ä¸ªQAé¡¹")
            
            # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
            self.data_manager = EvaluationDataManager()
            
            # éªŒè¯æ•°æ®æ ¼å¼
            validation_result = self.data_manager.validate_qa_data(demo_qa_data)
            if validation_result.is_valid:
                print("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
                print(f"ğŸ“‹ éªŒè¯ç»Ÿè®¡:")
                print(f"   - æœ‰æ•ˆé¡¹ç›®: {validation_result.valid_count}")
                print(f"   - æ— æ•ˆé¡¹ç›®: {validation_result.invalid_count}")
                print(f"   - å¹³å‡é—®é¢˜é•¿åº¦: {validation_result.avg_question_length:.0f}å­—ç¬¦")
                print(f"   - å¹³å‡ç­”æ¡ˆé•¿åº¦: {validation_result.avg_answer_length:.0f}å­—ç¬¦")
            else:
                print("âŒ æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥")
                for error in validation_result.errors:
                    print(f"   - {error}")
                return False
            
            # ä¿å­˜éªŒè¯åçš„æ•°æ®
            self.demo_qa_data = demo_qa_data
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return False
    
    def step_4_single_evaluation(self):
        """æ­¥éª¤4: å•é¡¹è¯„ä¼°æ¼”ç¤º"""
        print("\nğŸ” æ­¥éª¤4: å•é¡¹è¯„ä¼°æ¼”ç¤º")
        print("-" * 40)
        
        try:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªQAé¡¹è¿›è¡Œæ¼”ç¤º
            qa_item = self.demo_qa_data[0]
            print(f"ğŸ“ è¯„ä¼°é¡¹ç›®: {qa_item['question_id']}")
            print(f"â“ é—®é¢˜: {qa_item['question'][:50]}...")
            
            # æ‰§è¡Œè¯„ä¼°
            print("â³ æ­£åœ¨æ‰§è¡Œè¯„ä¼°...")
            start_time = time.time()
            
            result = self.engine.evaluate_single_qa(qa_item)
            
            end_time = time.time()
            evaluation_time = end_time - start_time
            
            print(f"âœ… è¯„ä¼°å®Œæˆ (è€—æ—¶: {evaluation_time:.2f}ç§’)")
            
            # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
            print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
            print(f"ğŸ¯ æ€»ä½“å¾—åˆ†: {result.overall_score:.3f}")
            
            print("\nğŸ“ˆ ç»´åº¦å¾—åˆ†:")
            for dimension, score in result.dimension_scores.items():
                print(f"   - {dimension}: {score:.3f}")
            
            print("\nğŸ­ è¡Œä¸šæŒ‡æ ‡:")
            for metric, value in result.industry_metrics.items():
                print(f"   - {metric}: {value:.3f}")
            
            if result.improvement_suggestions:
                print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
                for i, suggestion in enumerate(result.improvement_suggestions, 1):
                    print(f"   {i}. {suggestion}")
            
            # ä¿å­˜å•é¡¹è¯„ä¼°ç»“æœ
            single_result_path = self.demo_output_dir / "single_evaluation_result.json"
            with open(single_result_path, 'w', encoding='utf-8') as f:
                json.dump(result.to_dict(), f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {single_result_path}")
            return True
            
        except Exception as e:
            print(f"âŒ å•é¡¹è¯„ä¼°å¤±è´¥: {e}")
            return False
    
    def step_5_batch_evaluation(self):
        """æ­¥éª¤5: æ‰¹é‡è¯„ä¼°æ¼”ç¤º"""
        print("\nğŸ“¦ æ­¥éª¤5: æ‰¹é‡è¯„ä¼°æ¼”ç¤º")
        print("-" * 40)
        
        try:
            print(f"ğŸ“Š æ‰¹é‡è¯„ä¼° {len(self.demo_qa_data)} ä¸ªQAé¡¹")
            
            # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
            print("â³ æ­£åœ¨æ‰§è¡Œæ‰¹é‡è¯„ä¼°...")
            start_time = time.time()
            
            batch_result = self.engine.evaluate_batch(self.demo_qa_data)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ (æ€»è€—æ—¶: {total_time:.2f}ç§’)")
            print(f"âš¡ å¹³å‡æ¯é¡¹è€—æ—¶: {total_time/len(self.demo_qa_data):.2f}ç§’")
            
            # æ˜¾ç¤ºæ‰¹é‡è¯„ä¼°ç»Ÿè®¡
            print("\nğŸ“Š æ‰¹é‡è¯„ä¼°ç»Ÿè®¡:")
            print(f"ğŸ¯ å¹³å‡æ€»ä½“å¾—åˆ†: {batch_result.average_overall_score:.3f}")
            print(f"ğŸ“ˆ æœ€é«˜å¾—åˆ†: {batch_result.max_score:.3f}")
            print(f"ğŸ“‰ æœ€ä½å¾—åˆ†: {batch_result.min_score:.3f}")
            print(f"ğŸ“Š æ ‡å‡†å·®: {batch_result.score_std:.3f}")
            
            print("\nğŸ“ˆ å¹³å‡ç»´åº¦å¾—åˆ†:")
            for dimension, avg_score in batch_result.average_dimension_scores.items():
                print(f"   - {dimension}: {avg_score:.3f}")
            
            # æ˜¾ç¤ºè¯„ä¼°åˆ†å¸ƒ
            print("\nğŸ“Š å¾—åˆ†åˆ†å¸ƒ:")
            score_ranges = [
                (0.9, 1.0, "ä¼˜ç§€"),
                (0.8, 0.9, "è‰¯å¥½"), 
                (0.7, 0.8, "ä¸­ç­‰"),
                (0.6, 0.7, "åŠæ ¼"),
                (0.0, 0.6, "ä¸åŠæ ¼")
            ]
            
            for min_score, max_score, label in score_ranges:
                count = sum(1 for result in batch_result.individual_results 
                           if min_score <= result.overall_score < max_score)
                percentage = count / len(batch_result.individual_results) * 100
                print(f"   - {label} ({min_score:.1f}-{max_score:.1f}): {count}é¡¹ ({percentage:.1f}%)")
            
            # ä¿å­˜æ‰¹é‡è¯„ä¼°ç»“æœ
            batch_result_path = self.demo_output_dir / "batch_evaluation_result.json"
            with open(batch_result_path, 'w', encoding='utf-8') as f:
                json.dump(batch_result.to_dict(), f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ æ‰¹é‡ç»“æœå·²ä¿å­˜: {batch_result_path}")
            
            self.batch_result = batch_result
            return True
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
            return False
    
    def step_6_report_generation(self):
        """æ­¥éª¤6: ç»“æœåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ"""
        print("\nğŸ“‹ æ­¥éª¤6: ç»“æœåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ")
        print("-" * 40)
        
        try:
            # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
            report_generator = EvaluationReportGenerator()
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            print("ğŸ“ æ­£åœ¨ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
            
            detailed_report = report_generator.generate_detailed_report(
                self.batch_result,
                include_charts=True,
                include_recommendations=True
            )
            
            # ä¿å­˜HTMLæŠ¥å‘Š
            html_report_path = self.demo_output_dir / "evaluation_report.html"
            report_generator.save_html_report(detailed_report, html_report_path)
            print(f"âœ… HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_report_path}")
            
            # ç”ŸæˆJSONæŠ¥å‘Š
            json_report_path = self.demo_output_dir / "evaluation_report.json"
            with open(json_report_path, 'w', encoding='utf-8') as f:
                json.dump(detailed_report.to_dict(), f, indent=2, ensure_ascii=False)
            print(f"âœ… JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {json_report_path}")
            
            # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
            print("\nğŸ“Š æŠ¥å‘Šæ‘˜è¦:")
            print(f"ğŸ“ è¯„ä¼°é¡¹ç›®æ€»æ•°: {detailed_report.total_evaluations}")
            print(f"ğŸ¯ å¹³å‡å¾—åˆ†: {detailed_report.overall_statistics['mean']:.3f}")
            print(f"ğŸ“Š å¾—åˆ†ä¸­ä½æ•°: {detailed_report.overall_statistics['median']:.3f}")
            print(f"ğŸ“ˆ æœ€ä½³è¡¨ç°ç»´åº¦: {detailed_report.best_performing_dimension}")
            print(f"ğŸ“‰ å¾…æ”¹è¿›ç»´åº¦: {detailed_report.worst_performing_dimension}")
            
            if detailed_report.key_insights:
                print("\nğŸ” å…³é”®æ´å¯Ÿ:")
                for i, insight in enumerate(detailed_report.key_insights, 1):
                    print(f"   {i}. {insight}")
            
            if detailed_report.recommendations:
                print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
                for i, recommendation in enumerate(detailed_report.recommendations, 1):
                    print(f"   {i}. {recommendation}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    def step_7_performance_benchmark(self):
        """æ­¥éª¤7: æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\nâš¡ æ­¥éª¤7: æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("-" * 40)
        
        try:
            # åˆå§‹åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•
            benchmark = PerformanceBenchmark()
            
            print("ğŸ”§ æ­£åœ¨è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
            
            # è¿è¡ŒåŸºå‡†æµ‹è¯•
            benchmark_result = benchmark.run_comprehensive_benchmark(
                qa_data=self.demo_qa_data[:2],  # ä½¿ç”¨å‰2ä¸ªé¡¹ç›®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
                config=self.config
            )
            
            print("âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
            
            # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
            print("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            print(f"â±ï¸  å¹³å‡è¯„ä¼°æ—¶é—´: {benchmark_result.avg_evaluation_time:.2f}ç§’")
            print(f"ğŸš€ ååé‡: {benchmark_result.throughput:.1f} QAé¡¹/ç§’")
            print(f"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {benchmark_result.peak_memory_mb:.1f}MB")
            print(f"ğŸ”¥ å¹³å‡CPUä½¿ç”¨ç‡: {benchmark_result.avg_cpu_percent:.1f}%")
            
            if benchmark_result.gpu_metrics:
                print(f"ğŸ® GPUä½¿ç”¨ç‡: {benchmark_result.gpu_metrics.get('utilization', 0):.1f}%")
                print(f"ğŸ® GPUå†…å­˜ä½¿ç”¨: {benchmark_result.gpu_metrics.get('memory_used_mb', 0):.1f}MB")
            
            # æ€§èƒ½è¯„çº§
            performance_grade = self._calculate_performance_grade(benchmark_result)
            print(f"\nğŸ† æ€§èƒ½è¯„çº§: {performance_grade}")
            
            # ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ
            benchmark_path = self.demo_output_dir / "performance_benchmark.json"
            with open(benchmark_path, 'w', encoding='utf-8') as f:
                json.dump(benchmark_result.to_dict(), f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜: {benchmark_path}")
            return True
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def step_8_api_demo(self):
        """æ­¥éª¤8: APIæœåŠ¡æ¼”ç¤º"""
        print("\nğŸŒ æ­¥éª¤8: APIæœåŠ¡æ¼”ç¤º")
        print("-" * 40)
        
        try:
            print("â„¹ï¸  APIæœåŠ¡æ¼”ç¤º (æ¨¡æ‹Ÿ)")
            print("   åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œå¯ä»¥å¯åŠ¨APIæœåŠ¡å™¨:")
            print("   uv run python -m src.expert_evaluation.api")
            
            # æ¨¡æ‹ŸAPIè¯·æ±‚ç¤ºä¾‹
            api_request_example = {
                "qa_items": self.demo_qa_data[:1],
                "config": {
                    "evaluation_dimensions": ["semantic_similarity", "domain_accuracy"],
                    "async_mode": False
                }
            }
            
            # ä¿å­˜APIè¯·æ±‚ç¤ºä¾‹
            api_example_path = self.demo_output_dir / "api_request_example.json"
            with open(api_example_path, 'w', encoding='utf-8') as f:
                json.dump(api_request_example, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“ APIè¯·æ±‚ç¤ºä¾‹å·²ä¿å­˜: {api_example_path}")
            
            # æ˜¾ç¤ºAPIä½¿ç”¨ç¤ºä¾‹
            print("\nğŸ“‹ APIä½¿ç”¨ç¤ºä¾‹:")
            print("```bash")
            print("# å¯åŠ¨APIæœåŠ¡")
            print("uv run uvicorn src.expert_evaluation.api:app --host 0.0.0.0 --port 8000")
            print("")
            print("# å¥åº·æ£€æŸ¥")
            print("curl -X GET 'http://localhost:8000/health'")
            print("")
            print("# æäº¤è¯„ä¼°ä»»åŠ¡")
            print("curl -X POST 'http://localhost:8000/evaluate' \\")
            print("  -H 'Content-Type: application/json' \\")
            print(f"  -d @{api_example_path}")
            print("```")
            
            return True
            
        except Exception as e:
            print(f"âŒ APIæ¼”ç¤ºå¤±è´¥: {e}")
            return False
    
    def _calculate_performance_grade(self, benchmark_result) -> str:
        """è®¡ç®—æ€§èƒ½è¯„çº§"""
        score = 0
        
        # è¯„ä¼°æ—¶é—´è¯„åˆ† (è¶Šå¿«è¶Šå¥½)
        if benchmark_result.avg_evaluation_time < 1.0:
            score += 30
        elif benchmark_result.avg_evaluation_time < 2.0:
            score += 25
        elif benchmark_result.avg_evaluation_time < 5.0:
            score += 20
        else:
            score += 10
        
        # ååé‡è¯„åˆ† (è¶Šé«˜è¶Šå¥½)
        if benchmark_result.throughput > 2.0:
            score += 30
        elif benchmark_result.throughput > 1.0:
            score += 25
        elif benchmark_result.throughput > 0.5:
            score += 20
        else:
            score += 10
        
        # å†…å­˜ä½¿ç”¨è¯„åˆ† (è¶Šå°‘è¶Šå¥½)
        if benchmark_result.peak_memory_mb < 1000:
            score += 25
        elif benchmark_result.peak_memory_mb < 2000:
            score += 20
        elif benchmark_result.peak_memory_mb < 4000:
            score += 15
        else:
            score += 10
        
        # CPUä½¿ç”¨è¯„åˆ† (é€‚ä¸­æœ€å¥½)
        if 30 <= benchmark_result.avg_cpu_percent <= 70:
            score += 15
        elif 20 <= benchmark_result.avg_cpu_percent <= 80:
            score += 10
        else:
            score += 5
        
        # æ ¹æ®æ€»åˆ†ç¡®å®šç­‰çº§
        if score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif score >= 80:
            return "A (è‰¯å¥½)"
        elif score >= 70:
            return "B (ä¸­ç­‰)"
        elif score >= 60:
            return "C (åŠæ ¼)"
        else:
            return "D (éœ€è¦ä¼˜åŒ–)"
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ¼”ç¤ºæ€»ç»“æŠ¥å‘Š"""
        print("\nğŸ“‹ æ¼”ç¤ºæ€»ç»“æŠ¥å‘Š")
        print("=" * 60)
        
        summary = {
            "demo_info": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "version": "1.0.0",
                "total_qa_items": len(self.demo_qa_data) if hasattr(self, 'demo_qa_data') else 0
            },
            "system_status": {
                "initialization": "âœ… æˆåŠŸ",
                "model_loading": "âœ… æˆåŠŸ (æ¼”ç¤ºæ¨¡å¼)",
                "data_preparation": "âœ… æˆåŠŸ",
                "evaluation_engine": "âœ… æ­£å¸¸è¿è¡Œ"
            },
            "evaluation_results": {
                "single_evaluation": "âœ… å®Œæˆ",
                "batch_evaluation": "âœ… å®Œæˆ",
                "report_generation": "âœ… å®Œæˆ",
                "performance_benchmark": "âœ… å®Œæˆ"
            },
            "output_files": [
                str(self.demo_output_dir / "single_evaluation_result.json"),
                str(self.demo_output_dir / "batch_evaluation_result.json"),
                str(self.demo_output_dir / "evaluation_report.html"),
                str(self.demo_output_dir / "evaluation_report.json"),
                str(self.demo_output_dir / "performance_benchmark.json"),
                str(self.demo_output_dir / "api_request_example.json")
            ],
            "next_steps": [
                "é…ç½®çœŸå®çš„æ¨¡å‹è·¯å¾„",
                "å‡†å¤‡å®é™…çš„QAè¯„ä¼°æ•°æ®",
                "æ ¹æ®éœ€æ±‚è°ƒæ•´è¯„ä¼°ç»´åº¦å’Œæƒé‡",
                "éƒ¨ç½²APIæœåŠ¡åˆ°ç”Ÿäº§ç¯å¢ƒ",
                "é›†æˆåˆ°ç°æœ‰çš„è¯„ä¼°æµç¨‹ä¸­"
            ]
        }
        
        # ä¿å­˜æ€»ç»“æŠ¥å‘Š
        summary_path = self.demo_output_dir / "demo_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š æ¼”ç¤ºæ•°æ®: {summary['demo_info']['total_qa_items']} ä¸ªQAé¡¹")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {len(summary['output_files'])} ä¸ª")
        print(f"ğŸ’¾ æ€»ç»“æŠ¥å‘Š: {summary_path}")
        
        print("\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
        for i, step in enumerate(summary['next_steps'], 1):
            print(f"   {i}. {step}")
        
        print(f"\nğŸ“‚ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ä½äº: {self.demo_output_dir}")
        print("ğŸ‰ ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ!")
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        steps = [
            ("ç³»ç»Ÿåˆå§‹åŒ–", self.step_1_system_initialization),
            ("æ¨¡å‹åŠ è½½", self.step_2_model_loading),
            ("æ•°æ®å‡†å¤‡", self.step_3_data_preparation),
            ("å•é¡¹è¯„ä¼°", self.step_4_single_evaluation),
            ("æ‰¹é‡è¯„ä¼°", self.step_5_batch_evaluation),
            ("æŠ¥å‘Šç”Ÿæˆ", self.step_6_report_generation),
            ("æ€§èƒ½æµ‹è¯•", self.step_7_performance_benchmark),
            ("APIæ¼”ç¤º", self.step_8_api_demo)
        ]
        
        success_count = 0
        
        for step_name, step_func in steps:
            try:
                if step_func():
                    success_count += 1
                else:
                    print(f"âš ï¸  æ­¥éª¤ '{step_name}' æœªå®Œå…¨æˆåŠŸï¼Œç»§ç»­ä¸‹ä¸€æ­¥...")
            except Exception as e:
                print(f"âŒ æ­¥éª¤ '{step_name}' æ‰§è¡Œå¤±è´¥: {e}")
                print("ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥...")
        
        print(f"\nğŸ“Š æ¼”ç¤ºå®Œæˆç»Ÿè®¡: {success_count}/{len(steps)} ä¸ªæ­¥éª¤æˆåŠŸ")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
        demo = ExpertEvaluationDemo()
        demo.run_complete_demo()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¼”ç¤º!")

if __name__ == "__main__":
    main()