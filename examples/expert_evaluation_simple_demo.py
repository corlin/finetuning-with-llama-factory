#!/usr/bin/env python3
"""
ä¸“å®¶è¯„ä¼°ç³»ç»Ÿç®€å•æ¼”ç¤º

è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ¼”ç¤ºè„šæœ¬ï¼Œå±•ç¤ºä¸“å®¶è¯„ä¼°ç³»ç»Ÿçš„æ ¸å¿ƒæ¦‚å¿µå’Œä½¿ç”¨æ–¹æ³•ï¼Œ
ä¸ä¾èµ–å®Œæ•´çš„å®ç°ï¼Œé€‚åˆå¿«é€Ÿäº†è§£ç³»ç»ŸåŠŸèƒ½ã€‚

ä½¿ç”¨æ–¹æ³•:
    uv run python examples/expert_evaluation_simple_demo.py

ä½œè€…: ä¸“å®¶è¯„ä¼°ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
"""

import json
import time
import random
from pathlib import Path
from typing import Dict, List, Any

class SimpleEvaluationDemo:
    """ç®€å•è¯„ä¼°æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.output_dir = Path("simple_demo_output")
        self.output_dir.mkdir(exist_ok=True)
        
        print("ğŸš€ ä¸“å®¶è¯„ä¼°ç³»ç»Ÿç®€å•æ¼”ç¤º")
        print("=" * 50)
        print("æœ¬æ¼”ç¤ºå±•ç¤ºç³»ç»Ÿçš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸºæœ¬æµç¨‹")
    
    def create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        print("\nğŸ“Š æ­¥éª¤1: åˆ›å»ºç¤ºä¾‹æ•°æ®")
        print("-" * 30)
        
        # åˆ›å»ºç¤ºä¾‹QAæ•°æ®
        qa_data = [
            {
                "question_id": "demo_001",
                "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                "context": "è®¡ç®—æœºç§‘å­¦åŸºç¡€",
                "reference_answer": "äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚å®ƒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰æŠ€æœ¯ã€‚",
                "model_answer": "äººå·¥æ™ºèƒ½æ˜¯è®©è®¡ç®—æœºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ç­‰èƒ½åŠ›ã€‚ç°åœ¨å¹¿æ³›åº”ç”¨äºè¯­éŸ³è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚",
                "domain_tags": ["äººå·¥æ™ºèƒ½", "è®¡ç®—æœºç§‘å­¦"],
                "difficulty_level": "beginner",
                "expected_concepts": ["æœºå™¨å­¦ä¹ ", "æ™ºèƒ½ç³»ç»Ÿ", "ç®—æ³•"]
            },
            {
                "question_id": "demo_002",
                "question": "è§£é‡Šæœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆç°è±¡",
                "context": "æœºå™¨å­¦ä¹ ç†è®º",
                "reference_answer": "è¿‡æ‹Ÿåˆæ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°è¾ƒå·®çš„ç°è±¡ã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨æ¨¡å‹è¿‡äºå¤æ‚ï¼Œå­¦ä¹ äº†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°å’Œç»†èŠ‚ï¼Œè€Œä¸æ˜¯æ½œåœ¨çš„æ¨¡å¼ã€‚è§£å†³æ–¹æ³•åŒ…æ‹¬æ­£åˆ™åŒ–ã€äº¤å‰éªŒè¯ã€å¢åŠ è®­ç»ƒæ•°æ®ç­‰ã€‚",
                "model_answer": "è¿‡æ‹Ÿåˆå°±æ˜¯æ¨¡å‹è®°ä½äº†è®­ç»ƒæ•°æ®çš„å…·ä½“å†…å®¹ï¼Œè€Œä¸æ˜¯å­¦ä¼šäº†é€šç”¨è§„å¾‹ã€‚å°±åƒå­¦ç”Ÿæ­»è®°ç¡¬èƒŒè€ƒè¯•é¢˜ç›®ï¼Œé‡åˆ°æ–°é¢˜ç›®å°±ä¸ä¼šåšäº†ã€‚å¯ä»¥é€šè¿‡å‡å°‘æ¨¡å‹å¤æ‚åº¦æˆ–å¢åŠ æ•°æ®æ¥è§£å†³ã€‚",
                "domain_tags": ["æœºå™¨å­¦ä¹ ", "æ¨¡å‹è®­ç»ƒ"],
                "difficulty_level": "intermediate",
                "expected_concepts": ["æ³›åŒ–èƒ½åŠ›", "æ­£åˆ™åŒ–", "äº¤å‰éªŒè¯"]
            },
            {
                "question_id": "demo_003",
                "question": "æè¿°Transformeræ¶æ„çš„æ ¸å¿ƒåˆ›æ–°",
                "context": "æ·±åº¦å­¦ä¹ æ¶æ„",
                "reference_answer": "Transformerçš„æ ¸å¿ƒåˆ›æ–°æ˜¯æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)ï¼Œç‰¹åˆ«æ˜¯è‡ªæ³¨æ„åŠ›(Self-Attention)ã€‚å®ƒæ‘’å¼ƒäº†ä¼ ç»Ÿçš„å¾ªç¯å’Œå·ç§¯ç»“æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†åºåˆ—æ•°æ®ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œå¤„ç†ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚",
                "model_answer": "Transformeræœ€é‡è¦çš„åˆ›æ–°æ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©æ¨¡å‹èƒ½å¤ŸåŒæ—¶å…³æ³¨è¾“å…¥åºåˆ—çš„æ‰€æœ‰ä½ç½®ã€‚è¿™æ¯”ä¼ ç»Ÿçš„RNNæ›´é«˜æ•ˆï¼Œå› ä¸ºå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œè€Œä¸”èƒ½æ›´å¥½åœ°å¤„ç†é•¿æ–‡æœ¬ã€‚",
                "domain_tags": ["æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "ç¥ç»ç½‘ç»œ"],
                "difficulty_level": "advanced",
                "expected_concepts": ["æ³¨æ„åŠ›æœºåˆ¶", "å¹¶è¡Œè®¡ç®—", "åºåˆ—å»ºæ¨¡"]
            }
        ]
        
        # ä¿å­˜ç¤ºä¾‹æ•°æ®
        data_path = self.output_dir / "sample_qa_data.json"
        with open(data_path, 'w', encoding='utf-8') as f:
            json.dump(qa_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç¤ºä¾‹QAæ•°æ®å·²åˆ›å»º: {data_path}")
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(qa_data)} ä¸ªQAé¡¹")
        
        return qa_data
    
    def create_sample_config(self):
        """åˆ›å»ºç¤ºä¾‹é…ç½®"""
        print("\nâš™ï¸ æ­¥éª¤2: åˆ›å»ºç¤ºä¾‹é…ç½®")
        print("-" * 30)
        
        config = {
            "model": {
                "model_path": "/path/to/your/model",
                "device": "auto",
                "quantization": "int8",
                "batch_size": 4
            },
            "evaluation": {
                "dimensions": [
                    "semantic_similarity",
                    "domain_accuracy", 
                    "response_relevance",
                    "factual_correctness",
                    "completeness"
                ],
                "weights": {
                    "semantic_similarity": 0.25,
                    "domain_accuracy": 0.25,
                    "response_relevance": 0.20,
                    "factual_correctness": 0.15,
                    "completeness": 0.15
                },
                "thresholds": {
                    "min_score": 0.6,
                    "confidence_level": 0.95
                }
            },
            "performance": {
                "max_workers": 4,
                "timeout": 300,
                "memory_limit": "8GB",
                "cache_size": "1GB"
            },
            "output": {
                "format": "json",
                "detailed": True,
                "save_intermediate": False
            }
        }
        
        # ä¿å­˜é…ç½®
        config_path = self.output_dir / "sample_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç¤ºä¾‹é…ç½®å·²åˆ›å»º: {config_path}")
        print("ğŸ“‹ é…ç½®åŒ…å«:")
        print(f"   - è¯„ä¼°ç»´åº¦: {len(config['evaluation']['dimensions'])} ä¸ª")
        print(f"   - å·¥ä½œçº¿ç¨‹: {config['performance']['max_workers']} ä¸ª")
        print(f"   - å†…å­˜é™åˆ¶: {config['performance']['memory_limit']}")
        
        return config
    
    def simulate_evaluation(self, qa_data: List[Dict], config: Dict):
        """æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹"""
        print("\nğŸ” æ­¥éª¤3: æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹")
        print("-" * 30)
        
        print("â³ æ­£åœ¨æ‰§è¡Œè¯„ä¼°...")
        
        results = []
        
        for i, qa_item in enumerate(qa_data):
            print(f"ğŸ“ è¯„ä¼°é¡¹ç›® {i+1}/{len(qa_data)}: {qa_item['question_id']}")
            
            # æ¨¡æ‹Ÿè¯„ä¼°è®¡ç®— (ç”Ÿæˆéšæœºä½†åˆç†çš„åˆ†æ•°)
            random.seed(hash(qa_item['question_id']))  # ç¡®ä¿ç»“æœå¯é‡ç°
            
            # æ ¹æ®éš¾åº¦çº§åˆ«è°ƒæ•´åŸºç¡€åˆ†æ•°
            difficulty_multiplier = {
                "beginner": 0.85,
                "intermediate": 0.75,
                "advanced": 0.70,
                "expert": 0.65
            }
            
            base_score = difficulty_multiplier.get(qa_item.get('difficulty_level', 'intermediate'), 0.75)
            
            # ç”Ÿæˆå„ç»´åº¦å¾—åˆ†
            dimension_scores = {}
            for dimension in config['evaluation']['dimensions']:
                # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
                variation = (random.random() - 0.5) * 0.2  # -0.1 åˆ° 0.1 çš„å˜åŒ–
                score = max(0.0, min(1.0, base_score + variation))
                dimension_scores[dimension] = round(score, 3)
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            weights = config['evaluation']['weights']
            overall_score = sum(
                dimension_scores[dim] * weights.get(dim, 0) 
                for dim in dimension_scores
            )
            overall_score = round(overall_score, 3)
            
            # ç”Ÿæˆè¡Œä¸šæŒ‡æ ‡
            industry_metrics = {
                "domain_relevance": round(base_score + random.uniform(-0.05, 0.05), 3),
                "practical_applicability": round(base_score + random.uniform(-0.08, 0.08), 3),
                "innovation_level": round(base_score + random.uniform(-0.1, 0.1), 3),
                "completeness": round(base_score + random.uniform(-0.06, 0.06), 3)
            }
            
            # ç”Ÿæˆæ”¹è¿›å»ºè®®
            suggestions = []
            if dimension_scores.get('semantic_similarity', 0) < 0.8:
                suggestions.append("æé«˜ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼æ€§")
            if dimension_scores.get('domain_accuracy', 0) < 0.8:
                suggestions.append("å¢å¼ºä¸“ä¸šé¢†åŸŸçŸ¥è¯†çš„å‡†ç¡®æ€§")
            if dimension_scores.get('completeness', 0) < 0.8:
                suggestions.append("è¡¥å……æ›´å®Œæ•´çš„ä¿¡æ¯å’Œç»†èŠ‚")
            if not suggestions:
                suggestions.append("ç»§ç»­ä¿æŒå½“å‰çš„é«˜è´¨é‡æ°´å¹³")
            
            result = {
                "question_id": qa_item['question_id'],
                "overall_score": overall_score,
                "dimension_scores": dimension_scores,
                "industry_metrics": industry_metrics,
                "improvement_suggestions": suggestions,
                "confidence_intervals": {
                    "overall_score": [
                        round(overall_score - 0.03, 3),
                        round(overall_score + 0.03, 3)
                    ]
                },
                "evaluation_time": round(random.uniform(0.5, 2.0), 2)
            }
            
            results.append(result)
            
            # æ˜¾ç¤ºå•é¡¹ç»“æœ
            print(f"   ğŸ¯ å¾—åˆ†: {overall_score:.3f}")
            print(f"   â±ï¸  è€—æ—¶: {result['evaluation_time']}ç§’")
        
        print("âœ… è¯„ä¼°å®Œæˆ")
        return results
    
    def analyze_results(self, results: List[Dict]):
        """åˆ†æè¯„ä¼°ç»“æœ"""
        print("\nğŸ“Š æ­¥éª¤4: ç»“æœåˆ†æ")
        print("-" * 30)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        scores = [r['overall_score'] for r in results]
        avg_score = sum(scores) / len(scores)
        max_score = max(scores)
        min_score = min(scores)
        
        # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
        all_dimensions = set()
        for result in results:
            all_dimensions.update(result['dimension_scores'].keys())
        
        dimension_averages = {}
        for dim in all_dimensions:
            dim_scores = [r['dimension_scores'].get(dim, 0) for r in results]
            dimension_averages[dim] = sum(dim_scores) / len(dim_scores)
        
        # åˆ†æç»“æœ
        analysis = {
            "summary": {
                "total_evaluations": len(results),
                "average_score": round(avg_score, 3),
                "max_score": round(max_score, 3),
                "min_score": round(min_score, 3),
                "score_range": round(max_score - min_score, 3)
            },
            "dimension_analysis": {
                dim: round(avg, 3) for dim, avg in dimension_averages.items()
            },
            "performance_distribution": {
                "excellent (â‰¥0.9)": len([s for s in scores if s >= 0.9]),
                "good (0.8-0.9)": len([s for s in scores if 0.8 <= s < 0.9]),
                "fair (0.7-0.8)": len([s for s in scores if 0.7 <= s < 0.8]),
                "poor (<0.7)": len([s for s in scores if s < 0.7])
            },
            "top_performing_dimension": max(dimension_averages.items(), key=lambda x: x[1])[0],
            "lowest_performing_dimension": min(dimension_averages.items(), key=lambda x: x[1])[0]
        }
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        print("ğŸ“ˆ è¯„ä¼°ç»Ÿè®¡:")
        print(f"   æ€»è¯„ä¼°é¡¹ç›®: {analysis['summary']['total_evaluations']}")
        print(f"   å¹³å‡å¾—åˆ†: {analysis['summary']['average_score']}")
        print(f"   æœ€é«˜å¾—åˆ†: {analysis['summary']['max_score']}")
        print(f"   æœ€ä½å¾—åˆ†: {analysis['summary']['min_score']}")
        
        print("\nğŸ“Š ç»´åº¦è¡¨ç°:")
        for dim, score in analysis['dimension_analysis'].items():
            print(f"   {dim}: {score:.3f}")
        
        print("\nğŸ† è¡¨ç°åˆ†å¸ƒ:")
        for level, count in analysis['performance_distribution'].items():
            percentage = (count / len(results)) * 100
            print(f"   {level}: {count}é¡¹ ({percentage:.1f}%)")
        
        print(f"\nğŸ¯ æœ€ä½³ç»´åº¦: {analysis['top_performing_dimension']}")
        print(f"ğŸ“‰ å¾…æ”¹è¿›ç»´åº¦: {analysis['lowest_performing_dimension']}")
        
        return analysis
    
    def generate_report(self, qa_data: List[Dict], results: List[Dict], analysis: Dict):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("\nğŸ“‹ æ­¥éª¤5: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        print("-" * 30)
        
        # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
        report = {
            "report_info": {
                "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "report_type": "ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¼”ç¤ºæŠ¥å‘Š",
                "version": "1.0.0"
            },
            "evaluation_summary": analysis['summary'],
            "dimension_performance": analysis['dimension_analysis'],
            "performance_distribution": analysis['performance_distribution'],
            "detailed_results": results,
            "recommendations": [
                f"é‡ç‚¹æå‡ {analysis['lowest_performing_dimension']} ç»´åº¦çš„è¡¨ç°",
                f"ä¿æŒ {analysis['top_performing_dimension']} ç»´åº¦çš„ä¼˜åŠ¿",
                "å¢åŠ æ›´å¤šæ ·åŒ–çš„è¯„ä¼°æ•°æ®",
                "æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´è¯„ä¼°æƒé‡",
                "å®šæœŸæ›´æ–°è¯„ä¼°æ ‡å‡†å’Œé˜ˆå€¼"
            ],
            "next_steps": [
                "é…ç½®çœŸå®çš„æ¨¡å‹è·¯å¾„",
                "å‡†å¤‡å®é™…çš„QAè¯„ä¼°æ•°æ®",
                "æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´è¯„ä¼°ç»´åº¦",
                "å»ºç«‹å®šæœŸè¯„ä¼°æµç¨‹",
                "é›†æˆåˆ°ç°æœ‰çš„å¼€å‘æµç¨‹ä¸­"
            ]
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_report_path = self.output_dir / "evaluation_report.json"
        with open(json_report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report = self._generate_html_report(report)
        html_report_path = self.output_dir / "evaluation_report.html"
        with open(html_report_path, 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        print(f"âœ… JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {json_report_path}")
        print(f"âœ… HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_report_path}")
        
        return report
    
    def _generate_html_report(self, report: Dict) -> str:
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¼”ç¤ºæŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: #f4f4f4; padding: 20px; border-radius: 5px; }}
        .section {{ margin: 20px 0; }}
        .metric {{ background: #e9f5ff; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        .recommendation {{ background: #f0f8f0; padding: 10px; margin: 5px 0; border-radius: 3px; }}
        table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .score-excellent {{ color: #28a745; font-weight: bold; }}
        .score-good {{ color: #17a2b8; font-weight: bold; }}
        .score-fair {{ color: #ffc107; font-weight: bold; }}
        .score-poor {{ color: #dc3545; font-weight: bold; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¯ ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¼”ç¤ºæŠ¥å‘Š</h1>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {report['report_info']['generated_at']}</p>
        <p><strong>æŠ¥å‘Šç‰ˆæœ¬:</strong> {report['report_info']['version']}</p>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š è¯„ä¼°æ¦‚è¦</h2>
        <div class="metric">æ€»è¯„ä¼°é¡¹ç›®: {report['evaluation_summary']['total_evaluations']}</div>
        <div class="metric">å¹³å‡å¾—åˆ†: {report['evaluation_summary']['average_score']}</div>
        <div class="metric">æœ€é«˜å¾—åˆ†: {report['evaluation_summary']['max_score']}</div>
        <div class="metric">æœ€ä½å¾—åˆ†: {report['evaluation_summary']['min_score']}</div>
    </div>
    
    <div class="section">
        <h2>ğŸ“ˆ ç»´åº¦è¡¨ç°</h2>
        <table>
            <tr><th>è¯„ä¼°ç»´åº¦</th><th>å¹³å‡å¾—åˆ†</th><th>è¡¨ç°ç­‰çº§</th></tr>
        """
        
        for dim, score in report['dimension_performance'].items():
            if score >= 0.9:
                grade_class = "score-excellent"
                grade = "ä¼˜ç§€"
            elif score >= 0.8:
                grade_class = "score-good"
                grade = "è‰¯å¥½"
            elif score >= 0.7:
                grade_class = "score-fair"
                grade = "ä¸€èˆ¬"
            else:
                grade_class = "score-poor"
                grade = "å¾…æ”¹è¿›"
            
            html += f'<tr><td>{dim}</td><td class="{grade_class}">{score}</td><td class="{grade_class}">{grade}</td></tr>'
        
        html += f"""
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ† è¡¨ç°åˆ†å¸ƒ</h2>
        <table>
            <tr><th>è¡¨ç°ç­‰çº§</th><th>é¡¹ç›®æ•°é‡</th></tr>
        """
        
        for level, count in report['performance_distribution'].items():
            html += f"<tr><td>{level}</td><td>{count}</td></tr>"
        
        html += f"""
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ’¡ æ”¹è¿›å»ºè®®</h2>
        """
        
        for rec in report['recommendations']:
            html += f'<div class="recommendation">â€¢ {rec}</div>'
        
        html += f"""
    </div>
    
    <div class="section">
        <h2>ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨</h2>
        """
        
        for step in report['next_steps']:
            html += f'<div class="recommendation">â€¢ {step}</div>'
        
        html += """
    </div>
    
    <div class="section">
        <p><em>æœ¬æŠ¥å‘Šç”±ä¸“å®¶è¯„ä¼°ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ</em></p>
    </div>
</body>
</html>
        """
        
        return html
    
    def demonstrate_cli_usage(self):
        """æ¼”ç¤ºCLIä½¿ç”¨æ–¹æ³•"""
        print("\nğŸ’» æ­¥éª¤6: CLIä½¿ç”¨æ¼”ç¤º")
        print("-" * 30)
        
        cli_examples = {
            "åŸºæœ¬å‘½ä»¤": [
                "# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯",
                "uv run python -m src.expert_evaluation.cli --help",
                "",
                "# åˆå§‹åŒ–é…ç½®æ–‡ä»¶", 
                "uv run python -m src.expert_evaluation.cli init-config",
                "",
                "# éªŒè¯æ•°æ®æ ¼å¼",
                "uv run python -m src.expert_evaluation.cli validate-data sample_qa_data.json",
                "",
                "# æ‰§è¡Œè¯„ä¼°",
                "uv run python -m src.expert_evaluation.cli evaluate sample_qa_data.json"
            ],
            "é«˜çº§ç”¨æ³•": [
                "# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®",
                "uv run python -m src.expert_evaluation.cli -c sample_config.json evaluate sample_qa_data.json",
                "",
                "# ä¿å­˜ç»“æœåˆ°æ–‡ä»¶",
                "uv run python -m src.expert_evaluation.cli evaluate sample_qa_data.json -o results.json",
                "",
                "# ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š",
                "uv run python -m src.expert_evaluation.cli evaluate sample_qa_data.json --detailed",
                "",
                "# ä½¿ç”¨ä¸åŒè¾“å‡ºæ ¼å¼",
                "uv run python -m src.expert_evaluation.cli evaluate sample_qa_data.json -f html"
            ],
            "APIä½¿ç”¨": [
                "# å¯åŠ¨APIæœåŠ¡",
                "uv run uvicorn src.expert_evaluation.api:app --host 0.0.0.0 --port 8000",
                "",
                "# å¥åº·æ£€æŸ¥",
                "curl -X GET 'http://localhost:8000/health'",
                "",
                "# æäº¤è¯„ä¼°ä»»åŠ¡",
                "curl -X POST 'http://localhost:8000/evaluate' \\",
                "  -H 'Content-Type: application/json' \\",
                "  -d @sample_qa_data.json"
            ]
        }
        
        # ä¿å­˜CLIç¤ºä¾‹
        cli_path = self.output_dir / "cli_usage_examples.md"
        with open(cli_path, 'w', encoding='utf-8') as f:
            f.write("# ä¸“å®¶è¯„ä¼°ç³»ç»ŸCLIä½¿ç”¨ç¤ºä¾‹\n\n")
            
            for category, commands in cli_examples.items():
                f.write(f"## {category}\n\n")
                f.write("```bash\n")
                for cmd in commands:
                    f.write(f"{cmd}\n")
                f.write("```\n\n")
        
        print(f"âœ… CLIä½¿ç”¨ç¤ºä¾‹å·²ä¿å­˜: {cli_path}")
        
        # æ˜¾ç¤ºå…³é”®å‘½ä»¤
        print("ğŸ”§ å…³é”®CLIå‘½ä»¤:")
        print("   1. åˆå§‹åŒ–é…ç½®: uv run python -m src.expert_evaluation.cli init-config")
        print("   2. éªŒè¯æ•°æ®: uv run python -m src.expert_evaluation.cli validate-data data.json")
        print("   3. æ‰§è¡Œè¯„ä¼°: uv run python -m src.expert_evaluation.cli evaluate data.json")
        print("   4. å¯åŠ¨API: uv run uvicorn src.expert_evaluation.api:app --port 8000")
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        try:
            # æ‰§è¡Œæ¼”ç¤ºæ­¥éª¤
            qa_data = self.create_sample_data()
            config = self.create_sample_config()
            results = self.simulate_evaluation(qa_data, config)
            analysis = self.analyze_results(results)
            report = self.generate_report(qa_data, results, analysis)
            self.demonstrate_cli_usage()
            
            # ç”Ÿæˆæ¼”ç¤ºæ€»ç»“
            print("\nğŸ‰ æ¼”ç¤ºå®Œæˆæ€»ç»“")
            print("=" * 50)
            print(f"ğŸ“Š è¯„ä¼°äº† {len(qa_data)} ä¸ªQAé¡¹ç›®")
            print(f"ğŸ“ˆ å¹³å‡å¾—åˆ†: {analysis['summary']['average_score']}")
            print(f"ğŸ† æœ€ä½³ç»´åº¦: {analysis['top_performing_dimension']}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            
            # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
            output_files = list(self.output_dir.glob("*"))
            print(f"\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶ ({len(output_files)}ä¸ª):")
            for file_path in output_files:
                print(f"   - {file_path.name}")
            
            print("\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
            print("   1. æŸ¥çœ‹ç”Ÿæˆçš„HTMLæŠ¥å‘Šäº†è§£è¯¦ç»†ç»“æœ")
            print("   2. å‚è€ƒCLIä½¿ç”¨ç¤ºä¾‹å­¦ä¹ å‘½ä»¤è¡Œæ“ä½œ")
            print("   3. æ ¹æ®å®é™…éœ€æ±‚ä¿®æ”¹é…ç½®æ–‡ä»¶")
            print("   4. å‡†å¤‡çœŸå®çš„QAæ•°æ®è¿›è¡Œè¯„ä¼°")
            print("   5. æ¢ç´¢APIæ¥å£è¿›è¡Œç³»ç»Ÿé›†æˆ")
            
            print("\nâœ¨ æ„Ÿè°¢ä½¿ç”¨ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¼”ç¤ºï¼")
            
        except Exception as e:
            print(f"\nâŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    demo = SimpleEvaluationDemo()
    demo.run_complete_demo()

if __name__ == "__main__":
    main()