"""
ä¸­æ–‡å¤„ç†èƒ½åŠ›éªŒè¯æ¼”ç¤º

æœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ChineseCapabilityValidatoréªŒè¯é‡åŒ–æ¨¡å‹çš„ä¸­æ–‡å¤„ç†èƒ½åŠ›ï¼Œ
ç‰¹åˆ«æ˜¯å¯†ç å­¦ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§å’Œæ€è€ƒç»“æ„çš„ä¿æŒã€‚
"""

import sys
import os
import logging
import torch
import torch.nn as nn
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "src"))

from model_exporter import (
    ChineseCapabilityValidator,
    ModelQuantizer,
    QuantizationConfig,
    QuantizationFormat,
    QuantizationBackend
)


class DemoModel(nn.Module):
    """æ¼”ç¤ºç”¨çš„ç®€å•æ¨¡å‹"""
    
    def __init__(self, vocab_size=50000, hidden_size=768):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(hidden_size, 8, 2048, batch_first=True),
            num_layers=6
        )
        self.lm_head = nn.Linear(hidden_size, vocab_size)
        
    def forward(self, input_ids, **kwargs):
        x = self.embedding(input_ids)
        x = self.transformer(x)
        logits = self.lm_head(x)
        
        # è¿”å›ç±»ä¼¼transformersçš„è¾“å‡ºæ ¼å¼
        class ModelOutput:
            def __init__(self, logits):
                self.logits = logits
        
        return ModelOutput(logits)
    
    def generate(self, input_ids, max_length=100, **kwargs):
        """ç®€å•çš„ç”Ÿæˆæ–¹æ³•"""
        batch_size, seq_len = input_ids.shape
        
        # ç”Ÿæˆä¸€äº›éšæœºtokenä½œä¸ºæ¼”ç¤º
        new_tokens = max_length - seq_len
        if new_tokens > 0:
            generated = torch.randint(1, 1000, (batch_size, new_tokens))
            return torch.cat([input_ids, generated], dim=1)
        
        return input_ids


class DemoTokenizer:
    """æ¼”ç¤ºç”¨çš„ç®€å•åˆ†è¯å™¨"""
    
    def __init__(self):
        self.vocab_size = 50000
        self.pad_token_id = 0
        
        # ä¸­æ–‡å¯†ç å­¦æœ¯è¯­æ˜ å°„
        self.crypto_terms = {
            "AES": 100,
            "RSA": 101,
            "SHA-256": 102,
            "æ¤­åœ†æ›²çº¿": 103,
            "æ•°å­—ç­¾å": 104,
            "å¯¹ç§°åŠ å¯†": 105,
            "éå¯¹ç§°åŠ å¯†": 106,
            "å“ˆå¸Œå‡½æ•°": 107,
            "å¯†é’¥ç®¡ç†": 108,
            "åŒºå—é“¾": 109
        }
        
        # æ€è€ƒæ ‡ç­¾
        self.thinking_tokens = {
            "<thinking>": 200,
            "</thinking>": 201
        }
    
    def __call__(self, text, **kwargs):
        """ç¼–ç æ–‡æœ¬"""
        tokens = self.encode(text)
        return {
            "input_ids": torch.tensor([tokens]),
            "attention_mask": torch.tensor([[1] * len(tokens)])
        }
    
    def encode(self, text):
        """ç®€å•çš„ç¼–ç å®ç°"""
        tokens = []
        
        # æ£€æŸ¥å¯†ç å­¦æœ¯è¯­
        for term, token_id in self.crypto_terms.items():
            if term in text:
                tokens.append(token_id)
        
        # æ£€æŸ¥æ€è€ƒæ ‡ç­¾
        for tag, token_id in self.thinking_tokens.items():
            if tag in text:
                tokens.append(token_id)
        
        # æ·»åŠ ä¸€äº›åŸºç¡€token
        tokens.extend([1, 2, 3, 4, 5])
        
        return tokens
    
    def decode(self, tokens, skip_special_tokens=False):
        """ç®€å•çš„è§£ç å®ç°"""
        if isinstance(tokens, torch.Tensor):
            tokens = tokens.tolist()
        
        # æ ¹æ®tokenç”Ÿæˆç›¸åº”çš„ä¸­æ–‡æ–‡æœ¬
        text_parts = []
        
        for token in tokens:
            if token in [100, 101, 102]:  # AES, RSA, SHA-256
                text_parts.append("è¿™æ˜¯ä¸€ä¸ªå…³äºå¯†ç å­¦ç®—æ³•çš„ä¸“ä¸šå›ç­”ã€‚")
            elif token in [103, 104, 105, 106]:  # å…¶ä»–å¯†ç å­¦æœ¯è¯­
                text_parts.append("æ¶‰åŠå¯†ç å­¦ä¸“ä¸šæ¦‚å¿µçš„è¯¦ç»†è§£é‡Šã€‚")
            elif token == 200:  # <thinking>
                text_parts.append("<thinking>")
            elif token == 201:  # </thinking>
                text_parts.append("</thinking>")
            elif token > 500:  # é«˜tokenå€¼è¡¨ç¤ºä¸­æ–‡å†…å®¹
                text_parts.append("åŒ…å«ä¸­æ–‡å¯†ç å­¦æœ¯è¯­çš„ä¸“ä¸šå›ç­”ï¼Œå¦‚AESåŠ å¯†ç®—æ³•ã€RSAéå¯¹ç§°åŠ å¯†ç­‰ã€‚")
        
        if not text_parts:
            text_parts.append("è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡å›ç­”ï¼ŒåŒ…å«å¯†ç å­¦ç›¸å…³å†…å®¹ã€‚")
        
        return " ".join(text_parts)


def demonstrate_chinese_capability_validation():
    """æ¼”ç¤ºä¸­æ–‡å¤„ç†èƒ½åŠ›éªŒè¯"""
    print("=" * 60)
    print("ä¸­æ–‡å¤„ç†èƒ½åŠ›éªŒè¯æ¼”ç¤º")
    print("=" * 60)
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæ¼”ç¤ºæ¨¡å‹å’Œåˆ†è¯å™¨
    print("\n1. åˆ›å»ºæ¼”ç¤ºæ¨¡å‹å’Œåˆ†è¯å™¨...")
    model = DemoModel()
    tokenizer = DemoTokenizer()
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
    
    # åˆ›å»ºéªŒè¯å™¨
    print("\n2. åˆ›å»ºä¸­æ–‡èƒ½åŠ›éªŒè¯å™¨...")
    validator = ChineseCapabilityValidator()
    
    # æ‰§è¡Œä¸­æ–‡èƒ½åŠ›éªŒè¯
    print("\n3. æ‰§è¡Œä¸­æ–‡å¤„ç†èƒ½åŠ›éªŒè¯...")
    validation_results = validator.validate_chinese_capability(model, tokenizer)
    
    # æ˜¾ç¤ºéªŒè¯ç»“æœ
    print("\n4. éªŒè¯ç»“æœ:")
    print("-" * 40)
    print(f"æ€»ä½“å¾—åˆ†: {validation_results['overall_score']:.2%}")
    print(f"ä¸­æ–‡ç¼–ç å‡†ç¡®æ€§: {validation_results['chinese_encoding_accuracy']:.2%}")
    print(f"å¯†ç å­¦æœ¯è¯­å‡†ç¡®æ€§: {validation_results['crypto_term_accuracy']:.2%}")
    print(f"æ€è€ƒç»“æ„ä¿æŒ: {validation_results['thinking_structure_preservation']:.2%}")
    print(f"è¯­ä¹‰è¿è´¯æ€§: {validation_results['semantic_coherence']:.2%}")
    
    # æ˜¾ç¤ºè¯¦ç»†æµ‹è¯•ç»“æœ
    print("\n5. è¯¦ç»†æµ‹è¯•ç»“æœ:")
    print("-" * 40)
    for i, result in enumerate(validation_results['test_results'], 1):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}:")
        print(f"  è¾“å…¥: {result['input'][:50]}...")
        print(f"  å“åº”: {result['response'][:100]}...")
        print(f"  å¾—åˆ†: {result['score']:.2%}")
        print(f"  ç±»åˆ«: {result['category']}")
        print(f"  æˆåŠŸ: {'æ˜¯' if result['success'] else 'å¦'}")
    
    return validation_results


def demonstrate_quantization_comparison():
    """æ¼”ç¤ºé‡åŒ–å‰åçš„ä¸­æ–‡èƒ½åŠ›å¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("é‡åŒ–å‰åä¸­æ–‡èƒ½åŠ›å¯¹æ¯”æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºåŸå§‹æ¨¡å‹
    print("\n1. åˆ›å»ºåŸå§‹æ¨¡å‹...")
    original_model = DemoModel()
    tokenizer = DemoTokenizer()
    
    # åˆ›å»ºé‡åŒ–å™¨å’ŒéªŒè¯å™¨
    quantizer = ModelQuantizer()
    validator = ChineseCapabilityValidator()
    
    # é‡åŒ–æ¨¡å‹
    print("\n2. é‡åŒ–æ¨¡å‹...")
    config = QuantizationConfig(
        format=QuantizationFormat.DYNAMIC,
        backend=QuantizationBackend.PYTORCH
    )
    
    quantized_model, quant_result = quantizer.quantize_model(
        original_model, tokenizer, config
    )
    
    print(f"é‡åŒ–ç»“æœ: {'æˆåŠŸ' if quant_result.success else 'å¤±è´¥'}")
    if quant_result.success:
        print(f"å‹ç¼©æ¯”: {quant_result.compression_ratio:.2f}x")
        print(f"å†…å­˜å‡å°‘: {quant_result.memory_reduction:.2%}")
        print(f"æ¨ç†åŠ é€Ÿ: {quant_result.inference_speedup:.2f}x")
    
    # éªŒè¯åŸå§‹æ¨¡å‹çš„ä¸­æ–‡èƒ½åŠ›
    print("\n3. éªŒè¯åŸå§‹æ¨¡å‹ä¸­æ–‡èƒ½åŠ›...")
    original_results = validator.validate_chinese_capability(original_model, tokenizer)
    
    # éªŒè¯é‡åŒ–æ¨¡å‹çš„ä¸­æ–‡èƒ½åŠ›
    print("\n4. éªŒè¯é‡åŒ–æ¨¡å‹ä¸­æ–‡èƒ½åŠ›...")
    quantized_results = validator.validate_chinese_capability(quantized_model, tokenizer)
    
    # å¯¹æ¯”ç»“æœ
    print("\n5. ä¸­æ–‡èƒ½åŠ›å¯¹æ¯”:")
    print("-" * 50)
    print(f"{'æŒ‡æ ‡':<20} {'åŸå§‹æ¨¡å‹':<12} {'é‡åŒ–æ¨¡å‹':<12} {'å·®å¼‚':<10}")
    print("-" * 50)
    
    metrics = [
        ("æ€»ä½“å¾—åˆ†", "overall_score"),
        ("ä¸­æ–‡ç¼–ç å‡†ç¡®æ€§", "chinese_encoding_accuracy"),
        ("å¯†ç å­¦æœ¯è¯­å‡†ç¡®æ€§", "crypto_term_accuracy"),
        ("æ€è€ƒç»“æ„ä¿æŒ", "thinking_structure_preservation"),
        ("è¯­ä¹‰è¿è´¯æ€§", "semantic_coherence")
    ]
    
    for name, key in metrics:
        original_val = original_results[key]
        quantized_val = quantized_results[key]
        diff = quantized_val - original_val
        
        print(f"{name:<20} {original_val:<12.2%} {quantized_val:<12.2%} {diff:+.2%}")
    
    # åˆ†æç»“æœ
    print("\n6. åˆ†æç»“æœ:")
    print("-" * 40)
    
    overall_diff = quantized_results['overall_score'] - original_results['overall_score']
    if abs(overall_diff) < 0.05:
        print("âœ… é‡åŒ–å¯¹ä¸­æ–‡å¤„ç†èƒ½åŠ›å½±å“å¾ˆå°ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…")
    elif overall_diff > 0:
        print("ğŸ‰ é‡åŒ–åä¸­æ–‡å¤„ç†èƒ½åŠ›æœ‰æ‰€æå‡")
    else:
        print("âš ï¸  é‡åŒ–å¯¹ä¸­æ–‡å¤„ç†èƒ½åŠ›æœ‰ä¸€å®šå½±å“ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # ä¸“é¡¹åˆ†æ
    crypto_diff = quantized_results['crypto_term_accuracy'] - original_results['crypto_term_accuracy']
    if crypto_diff >= 0:
        print("âœ… å¯†ç å­¦æœ¯è¯­å¤„ç†èƒ½åŠ›ä¿æŒè‰¯å¥½")
    else:
        print("âš ï¸  å¯†ç å­¦æœ¯è¯­å¤„ç†èƒ½åŠ›æœ‰æ‰€ä¸‹é™")
    
    thinking_diff = quantized_results['thinking_structure_preservation'] - original_results['thinking_structure_preservation']
    if thinking_diff >= 0:
        print("âœ… æ€è€ƒç»“æ„ä¿æŒèƒ½åŠ›è‰¯å¥½")
    else:
        print("âš ï¸  æ€è€ƒç»“æ„ä¿æŒèƒ½åŠ›æœ‰æ‰€ä¸‹é™")
    
    return original_results, quantized_results


def demonstrate_custom_test_cases():
    """æ¼”ç¤ºè‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹"""
    print("\n" + "=" * 60)
    print("è‡ªå®šä¹‰ä¸­æ–‡å¯†ç å­¦æµ‹è¯•ç”¨ä¾‹æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹å’ŒéªŒè¯å™¨
    model = DemoModel()
    tokenizer = DemoTokenizer()
    validator = ChineseCapabilityValidator()
    
    # å®šä¹‰è‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹
    custom_test_cases = [
        {
            "input": "è¯·è§£é‡ŠAES-256åŠ å¯†ç®—æ³•çš„å·¥ä½œåŸç†ã€‚",
            "expected_keywords": ["å¯¹ç§°åŠ å¯†", "åˆ†ç»„å¯†ç ", "256ä½å¯†é’¥"],
            "category": "ç®—æ³•åŸç†"
        },
        {
            "input": "<thinking>æˆ‘éœ€è¦åˆ†æè¿™ä¸ªå¯†ç å­¦é—®é¢˜çš„å®‰å…¨æ€§</thinking>RSA-2048çš„å®‰å…¨å¼ºåº¦å¦‚ä½•ï¼Ÿ",
            "expected_keywords": ["éå¯¹ç§°åŠ å¯†", "2048ä½", "å®‰å…¨å¼ºåº¦"],
            "category": "å®‰å…¨åˆ†æ"
        },
        {
            "input": "åŒºå—é“¾ä¸­çš„å“ˆå¸Œå‡½æ•°æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ",
            "expected_keywords": ["SHA-256", "å·¥ä½œé‡è¯æ˜", "æ•°æ®å®Œæ•´æ€§"],
            "category": "åº”ç”¨åœºæ™¯"
        },
        {
            "input": "æ¤­åœ†æ›²çº¿å¯†ç å­¦ç›¸æ¯”RSAæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
            "expected_keywords": ["å¯†é’¥é•¿åº¦", "è®¡ç®—æ•ˆç‡", "å®‰å…¨æ€§"],
            "category": "æŠ€æœ¯å¯¹æ¯”"
        }
    ]
    
    print(f"\n1. ä½¿ç”¨ {len(custom_test_cases)} ä¸ªè‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹...")
    
    # æ‰§è¡ŒéªŒè¯
    results = validator.validate_chinese_capability(model, tokenizer, custom_test_cases)
    
    print("\n2. è‡ªå®šä¹‰æµ‹è¯•ç»“æœ:")
    print("-" * 50)
    
    for i, result in enumerate(results['test_results'], 1):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}: {result['category']}")
        print(f"  é—®é¢˜: {result['input']}")
        print(f"  å›ç­”: {result['response']}")
        print(f"  å¾—åˆ†: {result['score']:.2%}")
        print(f"  çŠ¶æ€: {'âœ… æˆåŠŸ' if result['success'] else 'âŒ å¤±è´¥'}")
    
    print(f"\n3. æ€»ä½“è¯„ä¼°:")
    print(f"  å¹³å‡å¾—åˆ†: {results['overall_score']:.2%}")
    print(f"  æˆåŠŸç‡: {sum(1 for r in results['test_results'] if r['success']) / len(results['test_results']):.2%}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    print("ä¸­æ–‡å¤„ç†èƒ½åŠ›éªŒè¯ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    try:
        # åŸºç¡€ä¸­æ–‡èƒ½åŠ›éªŒè¯
        basic_results = demonstrate_chinese_capability_validation()
        
        # é‡åŒ–å‰åå¯¹æ¯”
        original_results, quantized_results = demonstrate_quantization_comparison()
        
        # è‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹
        custom_results = demonstrate_custom_test_cases()
        
        print("\n" + "=" * 60)
        print("æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)
        
        print("\næ€»ç»“:")
        print(f"- åŸºç¡€éªŒè¯å¾—åˆ†: {basic_results['overall_score']:.2%}")
        print(f"- é‡åŒ–åå¾—åˆ†: {quantized_results['overall_score']:.2%}")
        print(f"- è‡ªå®šä¹‰æµ‹è¯•å¾—åˆ†: {custom_results['overall_score']:.2%}")
        
        print("\nå»ºè®®:")
        print("1. åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œå»ºè®®ä½¿ç”¨çœŸå®çš„Qwen3-4B-Thinkingæ¨¡å‹")
        print("2. å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚å®šåˆ¶æµ‹è¯•ç”¨ä¾‹")
        print("3. é‡åŒ–å‰åçš„å¯¹æ¯”æœ‰åŠ©äºé€‰æ‹©åˆé€‚çš„é‡åŒ–ç­–ç•¥")
        print("4. å®šæœŸéªŒè¯æ¨¡å‹çš„ä¸­æ–‡å¤„ç†èƒ½åŠ›ï¼Œç¡®ä¿è´¨é‡ç¨³å®š")
        
    except Exception as e:
        print(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()