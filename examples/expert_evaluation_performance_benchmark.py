#!/usr/bin/env python3
"""
ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•

æœ¬è„šæœ¬æä¾›å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
1. å•é¡¹è¯„ä¼°æ€§èƒ½æµ‹è¯•
2. æ‰¹é‡å¤„ç†æ€§èƒ½æµ‹è¯•
3. å†…å­˜ä½¿ç”¨åˆ†æ
4. å¹¶å‘æ€§èƒ½æµ‹è¯•
5. ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½å¯¹æ¯”

ä½¿ç”¨æ–¹æ³•:
    uv run python examples/expert_evaluation_performance_benchmark.py

ä½œè€…: ä¸“å®¶è¯„ä¼°ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
"""

import json
import time
import sys
import threading
import psutil
import gc
from pathlib import Path
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.expert_evaluation.engine import ExpertEvaluationEngine
from src.expert_evaluation.config import ExpertEvaluationConfig

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.output_dir = Path("benchmark_output")
        self.output_dir.mkdir(exist_ok=True)
        
        # æµ‹è¯•æ•°æ®
        self.test_qa_items = self._generate_test_data()
        
        print("ğŸš€ ä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {len(self.test_qa_items)} ä¸ªQAé¡¹")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _generate_test_data(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        test_data = []
        
        # ä¸åŒå¤æ‚åº¦çš„æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "complexity": "simple",
                "question": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
                "reference_answer": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚",
                "model_answer": "Pythonæ˜¯ä¸€ç§è§£é‡Šå‹ç¼–ç¨‹è¯­è¨€ã€‚"
            },
            {
                "complexity": "medium",
                "question": "è§£é‡Šé¢å‘å¯¹è±¡ç¼–ç¨‹çš„ä¸‰å¤§ç‰¹æ€§",
                "reference_answer": "é¢å‘å¯¹è±¡ç¼–ç¨‹çš„ä¸‰å¤§ç‰¹æ€§æ˜¯å°è£…ã€ç»§æ‰¿å’Œå¤šæ€ã€‚å°è£…æ˜¯å°†æ•°æ®å’Œæ–¹æ³•ç»„åˆåœ¨ä¸€èµ·ï¼Œç»§æ‰¿å…è®¸å­ç±»è·å¾—çˆ¶ç±»çš„å±æ€§å’Œæ–¹æ³•ï¼Œå¤šæ€å…è®¸ä¸åŒç±»çš„å¯¹è±¡å¯¹åŒä¸€æ¶ˆæ¯åšå‡ºä¸åŒçš„å“åº”ã€‚",
                "model_answer": "OOPçš„ä¸‰ä¸ªä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š1ï¼‰å°è£…-éšè—å†…éƒ¨å®ç°ç»†èŠ‚ï¼›2ï¼‰ç»§æ‰¿-ä»£ç é‡ç”¨æœºåˆ¶ï¼›3ï¼‰å¤šæ€-åŒä¸€æ¥å£çš„ä¸åŒå®ç°ã€‚"
            },
            {
                "complexity": "complex",
                "question": "è¯¦ç»†è¯´æ˜åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„CAPå®šç†ï¼Œå¹¶åˆ†æåœ¨å®é™…ç³»ç»Ÿè®¾è®¡ä¸­å¦‚ä½•æƒè¡¡ä¸€è‡´æ€§ã€å¯ç”¨æ€§å’Œåˆ†åŒºå®¹é”™æ€§",
                "reference_answer": "CAPå®šç†æŒ‡å‡ºï¼Œåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œä¸€è‡´æ€§(Consistency)ã€å¯ç”¨æ€§(Availability)å’Œåˆ†åŒºå®¹é”™æ€§(Partition tolerance)ä¸‰è€…ä¸èƒ½åŒæ—¶æ»¡è¶³ï¼Œæœ€å¤šåªèƒ½åŒæ—¶ä¿è¯å…¶ä¸­ä¸¤ä¸ªã€‚ä¸€è‡´æ€§è¦æ±‚æ‰€æœ‰èŠ‚ç‚¹åœ¨åŒä¸€æ—¶é—´çœ‹åˆ°ç›¸åŒçš„æ•°æ®ï¼›å¯ç”¨æ€§è¦æ±‚ç³»ç»Ÿåœ¨åˆç†æ—¶é—´å†…è¿”å›åˆç†çš„å“åº”ï¼›åˆ†åŒºå®¹é”™æ€§è¦æ±‚ç³»ç»Ÿåœ¨ç½‘ç»œåˆ†åŒºæ•…éšœæ—¶ä»èƒ½ç»§ç»­è¿è¡Œã€‚åœ¨å®é™…è®¾è®¡ä¸­ï¼Œç”±äºç½‘ç»œåˆ†åŒºæ˜¯ä¸å¯é¿å…çš„ï¼Œé€šå¸¸éœ€è¦åœ¨CPå’ŒAPä¹‹é—´é€‰æ‹©ã€‚",
                "model_answer": "CAPå®šç†æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿçš„é‡è¦ç†è®ºï¼Œè¯´æ˜äº†ä¸€è‡´æ€§ã€å¯ç”¨æ€§ã€åˆ†åŒºå®¹é”™æ€§ä¸‰è€…çš„æƒè¡¡å…³ç³»ã€‚åœ¨ç½‘ç»œåˆ†åŒºå‘ç”Ÿæ—¶ï¼Œç³»ç»Ÿå¿…é¡»åœ¨ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œé“¶è¡Œç³»ç»Ÿé€šå¸¸é€‰æ‹©ä¸€è‡´æ€§ï¼Œè€Œç¤¾äº¤åª’ä½“ç³»ç»Ÿå¯èƒ½æ›´æ³¨é‡å¯ç”¨æ€§ã€‚"
            }
        ]
        
        # ä¸ºæ¯ç§å¤æ‚åº¦ç”Ÿæˆå¤šä¸ªæµ‹è¯•ç”¨ä¾‹
        for i, case in enumerate(test_cases):
            for j in range(10):  # æ¯ç§å¤æ‚åº¦10ä¸ªç”¨ä¾‹
                test_data.append({
                    "question_id": f"{case['complexity']}_{j:03d}",
                    "question": case["question"],
                    "reference_answer": case["reference_answer"],
                    "model_answer": case["model_answer"],
                    "domain_tags": ["æµ‹è¯•", case["complexity"]],
                    "difficulty_level": case["complexity"],
                    "complexity": case["complexity"]
                })
        
        return test_data
    
    def benchmark_1_single_evaluation_performance(self):
        """åŸºå‡†æµ‹è¯•1: å•é¡¹è¯„ä¼°æ€§èƒ½"""
        print("\nâš¡ åŸºå‡†æµ‹è¯•1: å•é¡¹è¯„ä¼°æ€§èƒ½")
        print("-" * 40)
        
        # ä¸åŒé…ç½®çš„æ€§èƒ½æµ‹è¯•
        configs = {
            "minimal": {
                "evaluation": {
                    "dimensions": ["semantic_similarity"],
                    "weights": {"semantic_similarity": 1.0}
                }
            },
            "standard": {
                "evaluation": {
                    "dimensions": ["semantic_similarity", "domain_accuracy"],
                    "weights": {"semantic_similarity": 0.6, "domain_accuracy": 0.4}
                }
            },
            "comprehensive": {
                "evaluation": {
                    "dimensions": [
                        "semantic_similarity", "domain_accuracy", 
                        "response_relevance", "factual_correctness", "completeness"
                    ],
                    "weights": {
                        "semantic_similarity": 0.25,
                        "domain_accuracy": 0.25,
                        "response_relevance": 0.20,
                        "factual_correctness": 0.15,
                        "completeness": 0.15
                    }
                }
            }
        }
        
        results = {}
        
        for config_name, config_dict in configs.items():
            print(f"ğŸ” æµ‹è¯•é…ç½®: {config_name}")
            
            config = ExpertEvaluationConfig.from_dict(config_dict)
            engine = ExpertEvaluationEngine(config)
            
            # é¢„çƒ­
            engine.evaluate_single_qa(self.test_qa_items[0])
            
            # æ€§èƒ½æµ‹è¯•
            times = []
            memory_usage = []
            
            for qa_item in self.test_qa_items[:10]:  # æµ‹è¯•å‰10ä¸ªé¡¹ç›®
                # è®°å½•å†…å­˜ä½¿ç”¨
                gc.collect()
                memory_before = psutil.Process().memory_info().rss / 1024 / 1024
                
                # æ‰§è¡Œè¯„ä¼°
                start_time = time.time()
                result = engine.evaluate_single_qa(qa_item)
                end_time = time.time()
                
                # è®°å½•ç»“æœ
                evaluation_time = end_time - start_time
                times.append(evaluation_time)
                
                memory_after = psutil.Process().memory_info().rss / 1024 / 1024
                memory_usage.append(memory_after - memory_before)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
            avg_memory = sum(memory_usage) / len(memory_usage)
            
            results[config_name] = {
                "average_time": avg_time,
                "min_time": min_time,
                "max_time": max_time,
                "average_memory_delta": avg_memory,
                "throughput": 1 / avg_time,
                "dimension_count": len(config_dict["evaluation"]["dimensions"])
            }
            
            print(f"   å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’")
            print(f"   ååé‡: {1/avg_time:.1f} QAé¡¹/ç§’")
            print(f"   å†…å­˜å¢é‡: {avg_memory:.1f}MB")
        
        # ä¿å­˜ç»“æœ
        with open(self.output_dir / "single_evaluation_benchmark.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾
        self._plot_single_evaluation_performance(results)
        
        print("âœ… å•é¡¹è¯„ä¼°æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return results
    
    def benchmark_2_batch_processing_performance(self):
        """åŸºå‡†æµ‹è¯•2: æ‰¹é‡å¤„ç†æ€§èƒ½"""
        print("\nğŸ“¦ åŸºå‡†æµ‹è¯•2: æ‰¹é‡å¤„ç†æ€§èƒ½")
        print("-" * 40)
        
        # ä¸åŒæ‰¹é‡å¤§å°çš„æµ‹è¯•
        batch_sizes = [1, 5, 10, 20, 30]
        config = ExpertEvaluationConfig()
        
        results = {}
        
        for batch_size in batch_sizes:
            print(f"ğŸ” æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
            
            # é…ç½®æ‰¹é‡å¤„ç†
            config.batch_size = batch_size
            engine = ExpertEvaluationEngine(config)
            
            # é€‰æ‹©æµ‹è¯•æ•°æ®
            test_data = self.test_qa_items[:batch_size]
            
            # é¢„çƒ­
            if len(test_data) > 0:
                engine.evaluate_single_qa(test_data[0])
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            memory_before = psutil.Process().memory_info().rss / 1024 / 1024
            
            batch_result = engine.evaluate_batch(test_data)
            
            end_time = time.time()
            memory_after = psutil.Process().memory_info().rss / 1024 / 1024
            
            # è®¡ç®—æŒ‡æ ‡
            total_time = end_time - start_time
            throughput = len(test_data) / total_time
            memory_delta = memory_after - memory_before
            avg_time_per_item = total_time / len(test_data)
            
            results[batch_size] = {
                "batch_size": batch_size,
                "total_time": total_time,
                "throughput": throughput,
                "memory_delta": memory_delta,
                "avg_time_per_item": avg_time_per_item,
                "average_score": batch_result.average_overall_score
            }
            
            print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"   ååé‡: {throughput:.1f} QAé¡¹/ç§’")
            print(f"   å¹³å‡æ¯é¡¹: {avg_time_per_item:.3f}ç§’")
            print(f"   å†…å­˜å¢é‡: {memory_delta:.1f}MB")
        
        # ä¿å­˜ç»“æœ
        with open(self.output_dir / "batch_processing_benchmark.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆæ‰¹é‡å¤„ç†æ€§èƒ½å›¾
        self._plot_batch_processing_performance(results)
        
        print("âœ… æ‰¹é‡å¤„ç†æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return results
    
    def benchmark_3_memory_analysis(self):
        """åŸºå‡†æµ‹è¯•3: å†…å­˜ä½¿ç”¨åˆ†æ"""
        print("\nğŸ’¾ åŸºå‡†æµ‹è¯•3: å†…å­˜ä½¿ç”¨åˆ†æ")
        print("-" * 40)
        
        config = ExpertEvaluationConfig()
        engine = ExpertEvaluationEngine(config)
        
        memory_timeline = []
        evaluation_count = 0
        
        # ç›‘æ§å†…å­˜ä½¿ç”¨
        def memory_monitor():
            while evaluation_count < 50:
                memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                memory_timeline.append({
                    "timestamp": time.time(),
                    "memory_mb": memory_mb,
                    "evaluation_count": evaluation_count
                })
                time.sleep(0.1)  # æ¯100msè®°å½•ä¸€æ¬¡
        
        # å¯åŠ¨å†…å­˜ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=memory_monitor)
        monitor_thread.start()
        
        # æ‰§è¡Œè¯„ä¼°ä»»åŠ¡
        print("ğŸ” æ‰§è¡Œ50æ¬¡è¯„ä¼°å¹¶ç›‘æ§å†…å­˜ä½¿ç”¨...")
        
        for i in range(50):
            qa_item = self.test_qa_items[i % len(self.test_qa_items)]
            result = engine.evaluate_single_qa(qa_item)
            evaluation_count = i + 1
            
            if (i + 1) % 10 == 0:
                print(f"   å®Œæˆ {i + 1}/50 æ¬¡è¯„ä¼°")
        
        # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        monitor_thread.join()
        
        # åˆ†æå†…å­˜ä½¿ç”¨
        initial_memory = memory_timeline[0]["memory_mb"]
        peak_memory = max(point["memory_mb"] for point in memory_timeline)
        final_memory = memory_timeline[-1]["memory_mb"]
        
        memory_analysis = {
            "initial_memory_mb": initial_memory,
            "peak_memory_mb": peak_memory,
            "final_memory_mb": final_memory,
            "memory_growth_mb": final_memory - initial_memory,
            "peak_memory_increase_mb": peak_memory - initial_memory,
            "timeline": memory_timeline
        }
        
        print(f"ğŸ“Š å†…å­˜åˆ†æç»“æœ:")
        print(f"   åˆå§‹å†…å­˜: {initial_memory:.1f}MB")
        print(f"   å³°å€¼å†…å­˜: {peak_memory:.1f}MB")
        print(f"   æœ€ç»ˆå†…å­˜: {final_memory:.1f}MB")
        print(f"   å†…å­˜å¢é•¿: {final_memory - initial_memory:.1f}MB")
        print(f"   å³°å€¼å¢é•¿: {peak_memory - initial_memory:.1f}MB")
        
        # ä¿å­˜å†…å­˜åˆ†æç»“æœ
        with open(self.output_dir / "memory_analysis.json", 'w', encoding='utf-8') as f:
            json.dump(memory_analysis, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå†…å­˜ä½¿ç”¨å›¾
        self._plot_memory_usage(memory_timeline)
        
        print("âœ… å†…å­˜ä½¿ç”¨åˆ†æå®Œæˆ")
        return memory_analysis
    
    def benchmark_4_concurrent_performance(self):
        """åŸºå‡†æµ‹è¯•4: å¹¶å‘æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ”„ åŸºå‡†æµ‹è¯•4: å¹¶å‘æ€§èƒ½æµ‹è¯•")
        print("-" * 40)
        
        # ä¸åŒå¹¶å‘çº§åˆ«çš„æµ‹è¯•
        thread_counts = [1, 2, 4, 8]
        test_data = self.test_qa_items[:20]  # ä½¿ç”¨20ä¸ªæµ‹è¯•é¡¹ç›®
        
        results = {}
        
        for thread_count in thread_counts:
            print(f"ğŸ” æµ‹è¯•å¹¶å‘æ•°: {thread_count}")
            
            config = ExpertEvaluationConfig()
            config.max_workers = thread_count
            
            # å¹¶å‘è¯„ä¼°å‡½æ•°
            def evaluate_qa_item(qa_item):
                engine = ExpertEvaluationEngine(config)
                start_time = time.time()
                result = engine.evaluate_single_qa(qa_item)
                end_time = time.time()
                return {
                    "evaluation_time": end_time - start_time,
                    "score": result.overall_score
                }
            
            # æ‰§è¡Œå¹¶å‘æµ‹è¯•
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=thread_count) as executor:
                futures = [executor.submit(evaluate_qa_item, qa_item) for qa_item in test_data]
                concurrent_results = [future.result() for future in as_completed(futures)]
            
            end_time = time.time()
            
            # è®¡ç®—å¹¶å‘æ€§èƒ½æŒ‡æ ‡
            total_time = end_time - start_time
            avg_evaluation_time = sum(r["evaluation_time"] for r in concurrent_results) / len(concurrent_results)
            throughput = len(test_data) / total_time
            
            results[thread_count] = {
                "thread_count": thread_count,
                "total_time": total_time,
                "avg_evaluation_time": avg_evaluation_time,
                "throughput": throughput,
                "efficiency": throughput / thread_count,  # æ¯çº¿ç¨‹ååé‡
                "individual_results": concurrent_results
            }
            
            print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"   ååé‡: {throughput:.1f} QAé¡¹/ç§’")
            print(f"   æ•ˆç‡: {throughput/thread_count:.1f} QAé¡¹/ç§’/çº¿ç¨‹")
        
        # ä¿å­˜å¹¶å‘æµ‹è¯•ç»“æœ
        with open(self.output_dir / "concurrent_performance.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¹¶å‘æ€§èƒ½å›¾
        self._plot_concurrent_performance(results)
        
        print("âœ… å¹¶å‘æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return results
    
    def benchmark_5_configuration_comparison(self):
        """åŸºå‡†æµ‹è¯•5: ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½å¯¹æ¯”"""
        print("\nâš™ï¸ åŸºå‡†æµ‹è¯•5: é…ç½®æ€§èƒ½å¯¹æ¯”")
        print("-" * 40)
        
        # ä¸åŒé…ç½®æ–¹æ¡ˆ
        configurations = {
            "fast": {
                "name": "å¿«é€Ÿé…ç½®",
                "config": {
                    "evaluation": {
                        "dimensions": ["semantic_similarity"],
                        "algorithms": {
                            "semantic_similarity": {"method": "fast_cosine"}
                        }
                    },
                    "performance": {
                        "max_workers": 1,
                        "cache_size": "256MB"
                    }
                }
            },
            "balanced": {
                "name": "å¹³è¡¡é…ç½®",
                "config": {
                    "evaluation": {
                        "dimensions": ["semantic_similarity", "domain_accuracy"],
                        "weights": {"semantic_similarity": 0.6, "domain_accuracy": 0.4}
                    },
                    "performance": {
                        "max_workers": 2,
                        "cache_size": "512MB"
                    }
                }
            },
            "accurate": {
                "name": "é«˜ç²¾åº¦é…ç½®",
                "config": {
                    "evaluation": {
                        "dimensions": [
                            "semantic_similarity", "domain_accuracy", 
                            "response_relevance", "factual_correctness"
                        ],
                        "algorithms": {
                            "semantic_similarity": {"method": "bert_score"}
                        }
                    },
                    "performance": {
                        "max_workers": 4,
                        "cache_size": "1GB"
                    }
                }
            }
        }
        
        test_data = self.test_qa_items[:15]  # ä½¿ç”¨15ä¸ªæµ‹è¯•é¡¹ç›®
        results = {}
        
        for config_id, config_info in configurations.items():
            print(f"ğŸ” æµ‹è¯•é…ç½®: {config_info['name']}")
            
            config = ExpertEvaluationConfig.from_dict(config_info['config'])
            engine = ExpertEvaluationEngine(config)
            
            # é¢„çƒ­
            engine.evaluate_single_qa(test_data[0])
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            memory_before = psutil.Process().memory_info().rss / 1024 / 1024
            
            batch_result = engine.evaluate_batch(test_data)
            
            end_time = time.time()
            memory_after = psutil.Process().memory_info().rss / 1024 / 1024
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            total_time = end_time - start_time
            throughput = len(test_data) / total_time
            memory_delta = memory_after - memory_before
            
            results[config_id] = {
                "name": config_info['name'],
                "total_time": total_time,
                "throughput": throughput,
                "memory_delta": memory_delta,
                "average_score": batch_result.average_overall_score,
                "dimension_count": len(config_info['config']['evaluation']['dimensions']),
                "performance_score": self._calculate_performance_score(total_time, throughput, memory_delta)
            }
            
            print(f"   è€—æ—¶: {total_time:.2f}ç§’")
            print(f"   ååé‡: {throughput:.1f} QAé¡¹/ç§’")
            print(f"   å†…å­˜: {memory_delta:.1f}MB")
            print(f"   å¹³å‡å¾—åˆ†: {batch_result.average_overall_score:.3f}")
        
        # ä¿å­˜é…ç½®å¯¹æ¯”ç»“æœ
        with open(self.output_dir / "configuration_comparison.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆé…ç½®å¯¹æ¯”å›¾
        self._plot_configuration_comparison(results)
        
        print("âœ… é…ç½®æ€§èƒ½å¯¹æ¯”å®Œæˆ")
        return results
    
    def _calculate_performance_score(self, total_time: float, throughput: float, memory_delta: float) -> float:
        """è®¡ç®—ç»¼åˆæ€§èƒ½å¾—åˆ†"""
        # æ—¶é—´å¾—åˆ† (è¶Šå¿«è¶Šå¥½)
        time_score = max(0, 100 - total_time * 10)
        
        # ååé‡å¾—åˆ† (è¶Šé«˜è¶Šå¥½)
        throughput_score = min(100, throughput * 20)
        
        # å†…å­˜å¾—åˆ† (è¶Šå°‘è¶Šå¥½)
        memory_score = max(0, 100 - memory_delta * 2)
        
        # ç»¼åˆå¾—åˆ†
        return (time_score + throughput_score + memory_score) / 3
    
    def _plot_single_evaluation_performance(self, results):
        """ç»˜åˆ¶å•é¡¹è¯„ä¼°æ€§èƒ½å›¾"""
        configs = list(results.keys())
        times = [results[config]["average_time"] for config in configs]
        throughputs = [results[config]["throughput"] for config in configs]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # è¯„ä¼°æ—¶é—´å¯¹æ¯”
        ax1.bar(configs, times, color='skyblue')
        ax1.set_title('å¹³å‡è¯„ä¼°æ—¶é—´å¯¹æ¯”')
        ax1.set_ylabel('æ—¶é—´ (ç§’)')
        ax1.tick_params(axis='x', rotation=45)
        
        # ååé‡å¯¹æ¯”
        ax2.bar(configs, throughputs, color='lightgreen')
        ax2.set_title('ååé‡å¯¹æ¯”')
        ax2.set_ylabel('QAé¡¹/ç§’')
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "single_evaluation_performance.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_batch_processing_performance(self, results):
        """ç»˜åˆ¶æ‰¹é‡å¤„ç†æ€§èƒ½å›¾"""
        batch_sizes = list(results.keys())
        throughputs = [results[size]["throughput"] for size in batch_sizes]
        
        plt.figure(figsize=(10, 6))
        plt.plot(batch_sizes, throughputs, marker='o', linewidth=2, markersize=8)
        plt.title('æ‰¹é‡å¤„ç†æ€§èƒ½ - ååé‡ vs æ‰¹é‡å¤§å°')
        plt.xlabel('æ‰¹é‡å¤§å°')
        plt.ylabel('ååé‡ (QAé¡¹/ç§’)')
        plt.grid(True, alpha=0.3)
        
        plt.savefig(self.output_dir / "batch_processing_performance.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_memory_usage(self, memory_timeline):
        """ç»˜åˆ¶å†…å­˜ä½¿ç”¨å›¾"""
        timestamps = [point["timestamp"] for point in memory_timeline]
        memory_values = [point["memory_mb"] for point in memory_timeline]
        
        # è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´
        start_time = timestamps[0]
        relative_times = [(t - start_time) for t in timestamps]
        
        plt.figure(figsize=(12, 6))
        plt.plot(relative_times, memory_values, linewidth=2)
        plt.title('å†…å­˜ä½¿ç”¨æ—¶é—´çº¿')
        plt.xlabel('æ—¶é—´ (ç§’)')
        plt.ylabel('å†…å­˜ä½¿ç”¨ (MB)')
        plt.grid(True, alpha=0.3)
        
        plt.savefig(self.output_dir / "memory_usage.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_concurrent_performance(self, results):
        """ç»˜åˆ¶å¹¶å‘æ€§èƒ½å›¾"""
        thread_counts = list(results.keys())
        throughputs = [results[count]["throughput"] for count in thread_counts]
        efficiencies = [results[count]["efficiency"] for count in thread_counts]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # æ€»ååé‡
        ax1.plot(thread_counts, throughputs, marker='o', linewidth=2, markersize=8, color='blue')
        ax1.set_title('å¹¶å‘ååé‡')
        ax1.set_xlabel('çº¿ç¨‹æ•°')
        ax1.set_ylabel('æ€»ååé‡ (QAé¡¹/ç§’)')
        ax1.grid(True, alpha=0.3)
        
        # æ¯çº¿ç¨‹æ•ˆç‡
        ax2.plot(thread_counts, efficiencies, marker='s', linewidth=2, markersize=8, color='red')
        ax2.set_title('æ¯çº¿ç¨‹æ•ˆç‡')
        ax2.set_xlabel('çº¿ç¨‹æ•°')
        ax2.set_ylabel('æ•ˆç‡ (QAé¡¹/ç§’/çº¿ç¨‹)')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "concurrent_performance.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_configuration_comparison(self, results):
        """ç»˜åˆ¶é…ç½®å¯¹æ¯”å›¾"""
        configs = list(results.keys())
        names = [results[config]["name"] for config in configs]
        throughputs = [results[config]["throughput"] for config in configs]
        scores = [results[config]["average_score"] for config in configs]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # æ€§èƒ½å¯¹æ¯”
        ax1.bar(names, throughputs, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax1.set_title('é…ç½®æ€§èƒ½å¯¹æ¯”')
        ax1.set_ylabel('ååé‡ (QAé¡¹/ç§’)')
        ax1.tick_params(axis='x', rotation=45)
        
        # å‡†ç¡®æ€§å¯¹æ¯”
        ax2.bar(names, scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax2.set_title('é…ç½®å‡†ç¡®æ€§å¯¹æ¯”')
        ax2.set_ylabel('å¹³å‡å¾—åˆ†')
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "configuration_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        benchmarks = [
            ("å•é¡¹è¯„ä¼°æ€§èƒ½", self.benchmark_1_single_evaluation_performance),
            ("æ‰¹é‡å¤„ç†æ€§èƒ½", self.benchmark_2_batch_processing_performance),
            ("å†…å­˜ä½¿ç”¨åˆ†æ", self.benchmark_3_memory_analysis),
            ("å¹¶å‘æ€§èƒ½æµ‹è¯•", self.benchmark_4_concurrent_performance),
            ("é…ç½®æ€§èƒ½å¯¹æ¯”", self.benchmark_5_configuration_comparison)
        ]
        
        all_results = {}
        
        for benchmark_name, benchmark_func in benchmarks:
            try:
                print(f"\nğŸ¯ å¼€å§‹æ‰§è¡Œ: {benchmark_name}")
                result = benchmark_func()
                all_results[benchmark_name] = result
                print(f"âœ… {benchmark_name} å®Œæˆ")
            except Exception as e:
                print(f"âŒ {benchmark_name} å¤±è´¥: {e}")
                all_results[benchmark_name] = {"error": str(e)}
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        summary = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": {
                "cpu_count": psutil.cpu_count(),
                "memory_total_gb": psutil.virtual_memory().total / 1024**3,
                "python_version": sys.version
            },
            "test_data_size": len(self.test_qa_items),
            "completed_benchmarks": len([r for r in all_results.values() if "error" not in r]),
            "total_benchmarks": len(benchmarks),
            "benchmark_results": all_results
        }
        
        with open(self.output_dir / "benchmark_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“Š å®Œæˆæµ‹è¯•: {summary['completed_benchmarks']}/{summary['total_benchmarks']}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ’¾ ç»¼åˆæŠ¥å‘Š: {self.output_dir / 'benchmark_summary.json'}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        benchmark = PerformanceBenchmark()
        benchmark.run_all_benchmarks()
    except Exception as e:
        print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()