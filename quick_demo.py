#!/usr/bin/env python3
"""
å¿«é€ŸCheckpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æ¼”ç¤º

ç®€åŒ–ç‰ˆæ¼”ç¤ºç¨‹åºï¼Œå±•ç¤ºæ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ¨¡æ‹Ÿcheckpointåˆå¹¶è¿‡ç¨‹
2. åŠ è½½çœŸå®QAæ•°æ®
3. æ‰§è¡Œä¸“å®¶è¯„ä¼°
4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    uv run python quick_demo.py
"""

import json
import time
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

def load_qa_data(data_dir: str = "data/raw", max_items: int = 5) -> List[Dict[str, Any]]:
    """åŠ è½½QAæ•°æ®"""
    print(f"ğŸ“Š ä» {data_dir} åŠ è½½QAæ•°æ®...")
    
    data_path = Path(data_dir)
    all_qa_items = []
    
    # åŠ è½½enhanced QAæ–‡ä»¶
    enhanced_files = list(data_path.glob("enhanced_QA*.md"))
    
    for file_path in enhanced_files:
        print(f"ğŸ“– å¤„ç†æ–‡ä»¶: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–Q&Aå¯¹
        qa_pattern = r'### Q(\d+):\s*(.+?)\n\n<thinking>.*?</thinking>\n\nA\1:\s*(.+?)(?=\n### Q|\n## |$)'
        matches = re.findall(qa_pattern, content, re.DOTALL)
        
        for match in matches:
            q_num, question, answer = match
            question = question.strip()
            answer = answer.strip()
            
            if question and answer:
                qa_item = {
                    "question_id": f"qa_{q_num}",
                    "question": question,
                    "reference_answer": answer,
                    "context": "å¯†ç åº”ç”¨æ ‡å‡†GB/T 39786-2021",
                    "domain_tags": ["å¯†ç å­¦", "ä¿¡æ¯å®‰å…¨", "å›½å®¶æ ‡å‡†"],
                    "difficulty_level": "intermediate"
                }
                all_qa_items.append(qa_item)
                
                if len(all_qa_items) >= max_items:
                    break
        
        if len(all_qa_items) >= max_items:
            break
    
    print(f"âœ… åŠ è½½äº† {len(all_qa_items)} ä¸ªQAé¡¹")
    return all_qa_items

def simulate_model_answers(qa_data: List[Dict[str, Any]]) -> List[str]:
    """æ¨¡æ‹Ÿæ¨¡å‹ç”Ÿæˆç­”æ¡ˆ"""
    print("ğŸ¤– æ¨¡æ‹Ÿæ¨¡å‹ç”Ÿæˆç­”æ¡ˆ...")
    
    model_answers = []
    for qa_item in qa_data:
        # æ¨¡æ‹Ÿä¸åŒè´¨é‡çš„ç­”æ¡ˆ
        question = qa_item["question"]
        
        if "å¯†ç " in question or "åŠ å¯†" in question:
            answer = f"å…³äº{question[:20]}...çš„é—®é¢˜ï¼Œè¿™æ¶‰åŠå¯†ç å­¦çš„æ ¸å¿ƒæ¦‚å¿µã€‚å¯†ç æŠ€æœ¯æ˜¯ä¿¡æ¯å®‰å…¨çš„é‡è¦åŸºç¡€ï¼ŒåŒ…æ‹¬åŠ å¯†ç®—æ³•ã€æ•°å­—ç­¾åã€èº«ä»½è®¤è¯ç­‰å¤šä¸ªæ–¹é¢ã€‚åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„å¯†ç æ–¹æ¡ˆã€‚"
        elif "å®‰å…¨" in question:
            answer = f"é’ˆå¯¹{question[:20]}...çš„å®‰å…¨é—®é¢˜ï¼Œéœ€è¦ä»å¤šä¸ªç»´åº¦è€ƒè™‘ï¼šæŠ€æœ¯å®‰å…¨ã€ç®¡ç†å®‰å…¨ã€ç‰©ç†å®‰å…¨ç­‰ã€‚å»ºç«‹å®Œå–„çš„å®‰å…¨ä½“ç³»éœ€è¦ç»¼åˆè¿ç”¨å„ç§å®‰å…¨æŠ€æœ¯å’Œç®¡ç†æªæ–½ã€‚"
        else:
            answer = f"è¿™æ˜¯ä¸€ä¸ªå…³äº{question[:15]}...çš„ä¸“ä¸šé—®é¢˜ã€‚æ ¹æ®ç›¸å…³æ ‡å‡†å’Œæœ€ä½³å®è·µï¼Œéœ€è¦è€ƒè™‘æŠ€æœ¯å¯è¡Œæ€§ã€å®‰å…¨æ€§ã€æˆæœ¬æ•ˆç›Šç­‰å¤šä¸ªå› ç´ æ¥åˆ¶å®šåˆé€‚çš„è§£å†³æ–¹æ¡ˆã€‚"
        
        model_answers.append(answer)
        print(f"   âœ… {qa_item['question_id']}: {answer[:50]}...")
    
    return model_answers

def evaluate_answers(qa_data: List[Dict[str, Any]], model_answers: List[str]) -> Dict[str, Any]:
    """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
    print("ğŸ“Š æ‰§è¡Œä¸“å®¶è¯„ä¼°...")
    
    results = []
    
    for qa_item, model_answer in zip(qa_data, model_answers):
        # ç®€åŒ–è¯„ä¼°é€»è¾‘
        ref_words = set(qa_item["reference_answer"].lower().split())
        model_words = set(model_answer.lower().split())
        
        # è®¡ç®—è¯æ±‡é‡å åº¦
        overlap = len(ref_words & model_words)
        union = len(ref_words | model_words)
        similarity = overlap / union if union > 0 else 0
        
        # æ£€æŸ¥ä¸“ä¸šæœ¯è¯­
        domain_terms = ["å¯†ç ", "åŠ å¯†", "å®‰å…¨", "ç®—æ³•", "è®¤è¯", "æ ‡å‡†"]
        domain_count = sum(1 for term in domain_terms if term in model_answer)
        
        # è®¡ç®—å„ç»´åº¦å¾—åˆ†
        scores = {
            "semantic_similarity": min(0.9, similarity + 0.3),
            "domain_accuracy": min(0.9, 0.5 + domain_count * 0.1),
            "response_relevance": min(0.9, 0.6 + len(model_answer) / 200),
            "completeness": min(0.9, 0.5 + len(model_answer) / 300),
            "clarity": 0.8 if len(model_answer) > 50 else 0.6
        }
        
        overall_score = sum(scores.values()) / len(scores)
        
        result = {
            "question_id": qa_item["question_id"],
            "question": qa_item["question"],
            "reference_answer": qa_item["reference_answer"],
            "model_answer": model_answer,
            "overall_score": round(overall_score, 3),
            "dimension_scores": {k: round(v, 3) for k, v in scores.items()}
        }
        
        results.append(result)
        print(f"   ğŸ“ˆ {qa_item['question_id']}: {overall_score:.3f}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    scores = [r["overall_score"] for r in results]
    avg_score = sum(scores) / len(scores)
    
    return {
        "summary": {
            "total_evaluations": len(results),
            "average_score": round(avg_score, 3),
            "max_score": round(max(scores), 3),
            "min_score": round(min(scores), 3)
        },
        "individual_results": results
    }

def generate_report(evaluation_results: Dict[str, Any], output_dir: str = "quick_demo_output"):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    print("ğŸ“‹ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # ç”ŸæˆJSONæŠ¥å‘Š
    json_path = output_path / "evaluation_report.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>å¿«é€Ÿè¯„ä¼°æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f8ff; padding: 20px; border-radius: 5px; }}
        .metric {{ background: #f9f9f9; padding: 10px; margin: 5px 0; }}
        table {{ width: 100%; border-collapse: collapse; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¯ Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <h2>ğŸ“Š è¯„ä¼°æ¦‚è¦</h2>
    <div class="metric">æ€»è¯„ä¼°é¡¹ç›®: {evaluation_results['summary']['total_evaluations']}</div>
    <div class="metric">å¹³å‡å¾—åˆ†: {evaluation_results['summary']['average_score']}</div>
    <div class="metric">æœ€é«˜å¾—åˆ†: {evaluation_results['summary']['max_score']}</div>
    <div class="metric">æœ€ä½å¾—åˆ†: {evaluation_results['summary']['min_score']}</div>
    
    <h2>ğŸ“ˆ è¯¦ç»†ç»“æœ</h2>
    <table>
        <tr><th>é—®é¢˜ID</th><th>é—®é¢˜</th><th>å¾—åˆ†</th><th>è¯­ä¹‰ç›¸ä¼¼æ€§</th><th>é¢†åŸŸå‡†ç¡®æ€§</th></tr>
    """
    
    for result in evaluation_results['individual_results']:
        html_content += f"""
        <tr>
            <td>{result['question_id']}</td>
            <td>{result['question'][:50]}...</td>
            <td>{result['overall_score']}</td>
            <td>{result['dimension_scores']['semantic_similarity']}</td>
            <td>{result['dimension_scores']['domain_accuracy']}</td>
        </tr>
        """
    
    html_content += """
    </table>
</body>
</html>
    """
    
    html_path = output_path / "evaluation_report.html"
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ:")
    print(f"   JSON: {json_path}")
    print(f"   HTML: {html_path}")
    
    return str(html_path)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¿«é€ŸCheckpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # æ­¥éª¤1: æ¨¡æ‹Ÿcheckpointåˆå¹¶
        print("\nğŸ“‹ æ­¥éª¤1: æ¨¡æ‹ŸCheckpointåˆå¹¶")
        print("âœ… æ¨¡æ‹Ÿåˆå¹¶LoRA checkpointåˆ°åŸºåº§æ¨¡å‹ (Qwen/Qwen3-4B-Thinking-2507)")
        print("âœ… åˆå¹¶å®Œæˆï¼Œæ¨¡å‹å·²å‡†å¤‡å°±ç»ª")
        
        # æ­¥éª¤2: åŠ è½½QAæ•°æ®
        print("\nğŸ“‹ æ­¥éª¤2: åŠ è½½è¯„ä¼°æ•°æ®")
        qa_data = load_qa_data(max_items=5)
        
        if not qa_data:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°QAæ•°æ®")
            return
        
        # æ­¥éª¤3: ç”Ÿæˆæ¨¡å‹ç­”æ¡ˆ
        print("\nğŸ“‹ æ­¥éª¤3: ç”Ÿæˆæ¨¡å‹ç­”æ¡ˆ")
        model_answers = simulate_model_answers(qa_data)
        
        # æ­¥éª¤4: æ‰§è¡Œè¯„ä¼°
        print("\nğŸ“‹ æ­¥éª¤4: æ‰§è¡Œä¸“å®¶è¯„ä¼°")
        evaluation_results = evaluate_answers(qa_data, model_answers)
        
        # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“‹ æ­¥éª¤5: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        report_path = generate_report(evaluation_results)
        
        # æ€»ç»“
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print(f"ğŸ“Š å¹³å‡è¯„ä¼°å¾—åˆ†: {evaluation_results['summary']['average_score']}")
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()