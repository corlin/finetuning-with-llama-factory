#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å¥—ä»¶ - ä»»åŠ¡13.1å®ç°

æœ¬è„šæœ¬å®ç°äº†å®Œæ•´çš„ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
- å®Œæ•´è®­ç»ƒæµç¨‹çš„è‡ªåŠ¨åŒ–æµ‹è¯•
- å¤šç§é…ç½®åœºæ™¯çš„æµ‹è¯•è¦†ç›–
- æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå›å½’æµ‹è¯•
- ä¸­æ–‡å¯†ç å­¦æ•°æ®çš„è®­ç»ƒæ•ˆæœéªŒè¯
- ä½¿ç”¨uvè¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•å¥—ä»¶

éœ€æ±‚è¦†ç›–ï¼šæ‰€æœ‰éœ€æ±‚çš„éªŒè¯
"""

import os
import sys
import json
import time
import logging
import tempfile
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
import threading
import multiprocessing as mp

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.data_models import TrainingExample, ThinkingExample, CryptoTerm, ChineseMetrics, DifficultyLevel
from src.config_manager import TrainingConfig, DataConfig
from src.system_config import SystemConfig
from src.lora_config_optimizer import LoRAMemoryProfile
from src.parallel_config import ParallelConfig, GPUTopology, CommunicationBackend
from src.gpu_utils import GPUDetector
from src.memory_manager import MemoryManager
from src.training_monitor import TrainingMonitor
from src.chinese_nlp_processor import ChineseNLPProcessor
from src.crypto_term_processor import CryptoTermProcessor
from src.thinking_generator import ThinkingDataGenerator
from src.dataset_splitter import DatasetSplitter
from src.evaluation_framework import ComprehensiveEvaluationFramework
from src.model_exporter import ModelExporter


@dataclass
class IntegrationTestConfig:
    """é›†æˆæµ‹è¯•é…ç½®"""
    test_name: str
    description: str
    gpu_count: int
    batch_size: int
    sequence_length: int
    num_epochs: int
    enable_thinking: bool
    enable_crypto_terms: bool
    enable_chinese_processing: bool
    expected_duration_minutes: int
    memory_limit_mb: Optional[int] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "test_name": self.test_name,
            "description": self.description,
            "gpu_count": self.gpu_count,
            "batch_size": self.batch_size,
            "sequence_length": self.sequence_length,
            "num_epochs": self.num_epochs,
            "enable_thinking": self.enable_thinking,
            "enable_crypto_terms": self.enable_crypto_terms,
            "enable_chinese_processing": self.enable_chinese_processing,
            "expected_duration_minutes": self.expected_duration_minutes,
            "memory_limit_mb": self.memory_limit_mb
        }


@dataclass
class IntegrationTestResult:
    """é›†æˆæµ‹è¯•ç»“æœ"""
    test_name: str
    success: bool
    duration_seconds: float
    error_message: Optional[str] = None
    metrics: Optional[Dict[str, Any]] = None
    memory_usage: Optional[Dict[str, Any]] = None
    performance_metrics: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "test_name": self.test_name,
            "success": self.success,
            "duration_seconds": self.duration_seconds,
            "error_message": self.error_message,
            "metrics": self.metrics,
            "memory_usage": self.memory_usage,
            "performance_metrics": self.performance_metrics
        }


class TestDataGenerator:
    """æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.chinese_processor = ChineseNLPProcessor()
        self.crypto_processor = CryptoTermProcessor()
        self.thinking_generator = ThinkingDataGenerator()
    
    def generate_basic_training_data(self, count: int = 50) -> List[TrainingExample]:
        """ç”ŸæˆåŸºç¡€è®­ç»ƒæ•°æ®"""
        examples = []
        
        basic_qa_pairs = [
            ("ä»€ä¹ˆæ˜¯å¯¹ç§°åŠ å¯†ï¼Ÿ", "å¯¹ç§°åŠ å¯†æ˜¯ä¸€ç§åŠ å¯†æ–¹å¼ï¼ŒåŠ å¯†å’Œè§£å¯†ä½¿ç”¨ç›¸åŒçš„å¯†é’¥ã€‚å¸¸è§çš„å¯¹ç§°åŠ å¯†ç®—æ³•åŒ…æ‹¬AESã€DESç­‰ã€‚"),
            ("RSAç®—æ³•çš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ", "RSAç®—æ³•åŸºäºå¤§æ•´æ•°åˆ†è§£çš„æ•°å­¦éš¾é¢˜ï¼Œä½¿ç”¨å…¬é’¥å’Œç§é’¥è¿›è¡ŒåŠ å¯†è§£å¯†ã€‚"),
            ("ä»€ä¹ˆæ˜¯å“ˆå¸Œå‡½æ•°ï¼Ÿ", "å“ˆå¸Œå‡½æ•°æ˜¯å°†ä»»æ„é•¿åº¦çš„è¾“å…¥æ˜ å°„ä¸ºå›ºå®šé•¿åº¦è¾“å‡ºçš„å‡½æ•°ï¼Œå…·æœ‰å•å‘æ€§å’ŒæŠ—ç¢°æ’æ€§ã€‚"),
            ("æ•°å­—ç­¾åçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ", "æ•°å­—ç­¾åç”¨äºéªŒè¯æ•°æ®çš„å®Œæ•´æ€§å’Œå‘é€è€…çš„èº«ä»½ï¼Œé˜²æ­¢æ•°æ®è¢«ç¯¡æ”¹ã€‚"),
            ("ä»€ä¹ˆæ˜¯æ¤­åœ†æ›²çº¿å¯†ç å­¦ï¼Ÿ", "æ¤­åœ†æ›²çº¿å¯†ç å­¦åŸºäºæ¤­åœ†æ›²çº¿ä¸Šçš„ç¦»æ•£å¯¹æ•°é—®é¢˜ï¼Œæä¾›ä¸RSAç›¸åŒå®‰å…¨çº§åˆ«ä½†å¯†é’¥æ›´çŸ­çš„åŠ å¯†æ–¹æ¡ˆã€‚")
        ]
        
        for i in range(count):
            qa_pair = basic_qa_pairs[i % len(basic_qa_pairs)]
            
            # æ·»åŠ å˜åŒ–
            instruction = f"è¯·è§£é‡Šï¼š{qa_pair[0]}"
            if i % 3 == 0:
                instruction = f"ä½œä¸ºå¯†ç å­¦ä¸“å®¶ï¼Œ{qa_pair[0]}"
            elif i % 3 == 1:
                instruction = f"ä»æŠ€æœ¯è§’åº¦åˆ†æï¼š{qa_pair[0]}"
            
            example = TrainingExample(
                instruction=instruction,
                input="",
                output=qa_pair[1],
                thinking=None,
                crypto_terms=[term.term for term in self.crypto_processor.identify_crypto_terms(qa_pair[1])],
                difficulty_level=DifficultyLevel.BEGINNER if i % 3 == 0 else DifficultyLevel.INTERMEDIATE if i % 3 == 1 else DifficultyLevel.ADVANCED,
                source_file=f"test_data_{i}.md"
            )
            examples.append(example)
        
        return examples
    
    def generate_thinking_training_data(self, count: int = 30) -> List[ThinkingExample]:
        """ç”Ÿæˆæ·±åº¦æ€è€ƒè®­ç»ƒæ•°æ®"""
        examples = []
        
        thinking_scenarios = [
            {
                "instruction": "åˆ†æAES-256åŠ å¯†ç®—æ³•çš„å®‰å…¨æ€§",
                "base_response": "AES-256æ˜¯ç›®å‰æœ€å®‰å…¨çš„å¯¹ç§°åŠ å¯†ç®—æ³•ä¹‹ä¸€ï¼Œä½¿ç”¨256ä½å¯†é’¥ã€‚",
                "thinking_points": [
                    "é¦–å…ˆéœ€è¦äº†è§£AESç®—æ³•çš„åŸºæœ¬ç»“æ„",
                    "åˆ†æ256ä½å¯†é’¥çš„å®‰å…¨å¼ºåº¦",
                    "è€ƒè™‘å·²çŸ¥çš„æ”»å‡»æ–¹æ³•",
                    "è¯„ä¼°åœ¨é‡å­è®¡ç®—å¨èƒä¸‹çš„å®‰å…¨æ€§"
                ]
            },
            {
                "instruction": "è®¾è®¡ä¸€ä¸ªå®‰å…¨çš„å¯†é’¥äº¤æ¢åè®®",
                "base_response": "å¯ä»¥ä½¿ç”¨Diffie-Hellmanå¯†é’¥äº¤æ¢åè®®æ¥å®‰å…¨åœ°äº¤æ¢å¯†é’¥ã€‚",
                "thinking_points": [
                    "åˆ†æå¯†é’¥äº¤æ¢çš„å®‰å…¨éœ€æ±‚",
                    "è€ƒè™‘ä¸­é—´äººæ”»å‡»çš„é˜²æŠ¤",
                    "é€‰æ‹©åˆé€‚çš„æ•°å­¦åŸºç¡€",
                    "è®¾è®¡å…·ä½“çš„åè®®æ­¥éª¤"
                ]
            }
        ]
        
        for i in range(count):
            scenario = thinking_scenarios[i % len(thinking_scenarios)]
            
            # ç”Ÿæˆæ€è€ƒè¿‡ç¨‹
            thinking_process = self.thinking_generator.generate_thinking_process(
                scenario["instruction"],
                context={"thinking_points": scenario["thinking_points"]}
            )
            
            example = ThinkingExample(
                instruction=scenario["instruction"],
                thinking_process=thinking_process,
                final_response=scenario["base_response"],
                crypto_terms=[term.term for term in self.crypto_processor.identify_crypto_terms(scenario["base_response"])],
                reasoning_steps=scenario["thinking_points"]
            )
            examples.append(example)
        
        return examples
    
    def generate_chinese_crypto_data(self, count: int = 40) -> List[TrainingExample]:
        """ç”Ÿæˆä¸­æ–‡å¯†ç å­¦æ•°æ®"""
        examples = []
        
        chinese_crypto_content = [
            ("å¯†ç å­¦ä¸­çš„æ··æ·†å’Œæ‰©æ•£åŸç†", "æ··æ·†æ˜¯æŒ‡åŠ å¯†ç®—æ³•åº”è¯¥ä½¿å¯†æ–‡å’Œå¯†é’¥ä¹‹é—´çš„å…³ç³»å°½å¯èƒ½å¤æ‚ï¼›æ‰©æ•£æ˜¯æŒ‡æ˜æ–‡ä¸­æ¯ä¸€ä½çš„æ”¹å˜éƒ½åº”è¯¥å½±å“å¯†æ–‡ä¸­çš„å¤šä½ã€‚"),
            ("ä¸­å›½å•†ç”¨å¯†ç ç®—æ³•SM4çš„ç‰¹ç‚¹", "SM4æ˜¯ä¸­å›½è‡ªä¸»è®¾è®¡çš„åˆ†ç»„å¯†ç ç®—æ³•ï¼Œé‡‡ç”¨128ä½å¯†é’¥å’Œ128ä½åˆ†ç»„é•¿åº¦ï¼Œå…·æœ‰è‰¯å¥½çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚"),
            ("æ•°å­—è¯ä¹¦åœ¨ç”µå­å•†åŠ¡ä¸­çš„åº”ç”¨", "æ•°å­—è¯ä¹¦é€šè¿‡å…¬é’¥åŸºç¡€è®¾æ–½(PKI)ä¸ºç”µå­å•†åŠ¡æä¾›èº«ä»½è®¤è¯ã€æ•°æ®å®Œæ•´æ€§å’Œä¸å¯å¦è®¤æ€§ä¿éšœã€‚"),
            ("é‡å­å¯†ç å­¦çš„å‘å±•å‰æ™¯", "é‡å­å¯†ç å­¦åˆ©ç”¨é‡å­åŠ›å­¦åŸç†æä¾›ç†è®ºä¸Šæ— æ¡ä»¶å®‰å…¨çš„é€šä¿¡ï¼Œæ˜¯æœªæ¥å¯†ç å­¦å‘å±•çš„é‡è¦æ–¹å‘ã€‚")
        ]
        
        for i in range(count):
            content = chinese_crypto_content[i % len(chinese_crypto_content)]
            
            # ä¸­æ–‡æ–‡æœ¬å¤„ç†
            processed_instruction = self.chinese_processor.preprocess_for_training(f"è¯·è¯¦ç»†è¯´æ˜{content[0]}ã€‚")
            processed_output = self.chinese_processor.preprocess_for_training(content[1])
            
            example = TrainingExample(
                instruction=processed_instruction,
                input="",
                output=processed_output,
                thinking=None,
                crypto_terms=[term.term for term in self.crypto_processor.identify_crypto_terms(content[1])],
                difficulty_level=DifficultyLevel.INTERMEDIATE,
                source_file=f"chinese_crypto_{i}.md"
            )
            examples.append(example)
        
        return examples


class ComprehensiveIntegrationTestSuite:
    """ç»¼åˆé›†æˆæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, output_dir: str = "comprehensive_integration_output"):
        """åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.gpu_detector = GPUDetector()
        self.data_generator = TestDataGenerator()
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.test_results: List[IntegrationTestResult] = []
        self.performance_baselines: Dict[str, Dict[str, float]] = {}
        
        # æµ‹è¯•é…ç½®
        self.test_configurations = self._create_test_configurations()
        
        self.logger.info(f"ç»¼åˆé›†æˆæµ‹è¯•å¥—ä»¶åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("ComprehensiveIntegrationTest")
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        log_file = self.output_dir / "comprehensive_integration_test.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # æ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def _create_test_configurations(self) -> List[IntegrationTestConfig]:
        """åˆ›å»ºæµ‹è¯•é…ç½®"""
        configurations = []
        
        # åŸºç¡€å•GPUæµ‹è¯•
        configurations.append(IntegrationTestConfig(
            test_name="basic_single_gpu",
            description="åŸºç¡€å•GPUè®­ç»ƒæµç¨‹æµ‹è¯•",
            gpu_count=1,
            batch_size=2,
            sequence_length=512,
            num_epochs=1,
            enable_thinking=False,
            enable_crypto_terms=True,
            enable_chinese_processing=True,
            expected_duration_minutes=10
        ))
        
        # æ·±åº¦æ€è€ƒæ•°æ®æµ‹è¯•
        configurations.append(IntegrationTestConfig(
            test_name="thinking_data_training",
            description="æ·±åº¦æ€è€ƒæ•°æ®è®­ç»ƒæµ‹è¯•",
            gpu_count=1,
            batch_size=1,
            sequence_length=1024,
            num_epochs=1,
            enable_thinking=True,
            enable_crypto_terms=True,
            enable_chinese_processing=True,
            expected_duration_minutes=15
        ))
        
        # å†…å­˜ä¼˜åŒ–æµ‹è¯•
        configurations.append(IntegrationTestConfig(
            test_name="memory_optimization",
            description="å†…å­˜ç®¡ç†å’Œä¼˜åŒ–æµ‹è¯•",
            gpu_count=1,
            batch_size=4,
            sequence_length=2048,
            num_epochs=1,
            enable_thinking=False,
            enable_crypto_terms=True,
            enable_chinese_processing=True,
            expected_duration_minutes=20,
            memory_limit_mb=8000
        ))
        
        # ä¸­æ–‡å¯†ç å­¦ä¸“ä¸šæµ‹è¯•
        configurations.append(IntegrationTestConfig(
            test_name="chinese_crypto_expertise",
            description="ä¸­æ–‡å¯†ç å­¦ä¸“ä¸šèƒ½åŠ›æµ‹è¯•",
            gpu_count=1,
            batch_size=2,
            sequence_length=1024,
            num_epochs=2,
            enable_thinking=True,
            enable_crypto_terms=True,
            enable_chinese_processing=True,
            expected_duration_minutes=25
        ))
        
        # å¤šGPUæµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        gpu_count = len(self.gpu_detector.get_all_gpu_info())
        if gpu_count > 1:
            configurations.append(IntegrationTestConfig(
                test_name="multi_gpu_distributed",
                description="å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•",
                gpu_count=min(2, gpu_count),
                batch_size=1,
                sequence_length=512,
                num_epochs=1,
                enable_thinking=False,
                enable_crypto_terms=True,
                enable_chinese_processing=True,
                expected_duration_minutes=15
            ))
        
        return configurations
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
        self.logger.info("å¼€å§‹è¿è¡Œç»¼åˆé›†æˆæµ‹è¯•å¥—ä»¶")
        start_time = time.time()
        
        # æ¸…ç†ä¹‹å‰çš„ç»“æœ
        self.test_results.clear()
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•é…ç½®
        for config in self.test_configurations:
            self.logger.info(f"å¼€å§‹æµ‹è¯•: {config.test_name} - {config.description}")
            
            try:
                result = self._run_single_test(config)
                self.test_results.append(result)
                
                if result.success:
                    self.logger.info(f"æµ‹è¯•æˆåŠŸ: {config.test_name}, è€—æ—¶: {result.duration_seconds:.2f}ç§’")
                else:
                    self.logger.error(f"æµ‹è¯•å¤±è´¥: {config.test_name}, é”™è¯¯: {result.error_message}")
                    
            except Exception as e:
                self.logger.error(f"æµ‹è¯•å¼‚å¸¸: {config.test_name}, å¼‚å¸¸: {e}")
                result = IntegrationTestResult(
                    test_name=config.test_name,
                    success=False,
                    duration_seconds=0,
                    error_message=str(e)
                )
                self.test_results.append(result)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        total_duration = time.time() - start_time
        report = self._generate_test_report(total_duration)
        
        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        self._save_test_report(report)
        
        self.logger.info(f"é›†æˆæµ‹è¯•å¥—ä»¶å®Œæˆï¼Œæ€»è€—æ—¶: {total_duration:.2f}ç§’")
        return report
    
    def _run_single_test(self, config: IntegrationTestConfig) -> IntegrationTestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        start_time = time.time()
        test_output_dir = self.output_dir / config.test_name
        test_output_dir.mkdir(exist_ok=True)
        
        try:
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            training_data = self._generate_test_data(config)
            
            # åˆ›å»ºé…ç½®
            training_config, data_config, lora_config, parallel_config, system_config = \
                self._create_test_configs(config, str(test_output_dir))
            
            # åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨
            memory_manager = None
            if config.memory_limit_mb:
                memory_manager = MemoryManager({
                    "monitoring_interval": 2,
                    "enable_auto_adjustment": True,
                    "initial_batch_size": config.batch_size
                })
                memory_manager.start()
            
            # æ¨¡æ‹Ÿè®­ç»ƒæµç¨‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            success = self._simulate_training_pipeline(
                training_data, training_config, data_config, 
                lora_config, parallel_config, system_config,
                str(test_output_dir)
            )
            
            # æ”¶é›†æŒ‡æ ‡
            metrics = self._collect_test_metrics(config, training_data)
            memory_usage = None
            if memory_manager:
                memory_usage = memory_manager.get_memory_analysis()
                memory_manager.stop()
            
            # æ€§èƒ½æŒ‡æ ‡
            performance_metrics = self._calculate_performance_metrics(
                config, time.time() - start_time
            )
            
            return IntegrationTestResult(
                test_name=config.test_name,
                success=success,
                duration_seconds=time.time() - start_time,
                metrics=metrics,
                memory_usage=memory_usage,
                performance_metrics=performance_metrics
            )
            
        except Exception as e:
            return IntegrationTestResult(
                test_name=config.test_name,
                success=False,
                duration_seconds=time.time() - start_time,
                error_message=str(e)
            )
    
    def _generate_test_data(self, config: IntegrationTestConfig) -> List[Any]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        training_data = []
        
        # åŸºç¡€è®­ç»ƒæ•°æ®
        if not config.enable_thinking:
            basic_data = self.data_generator.generate_basic_training_data(30)
            training_data.extend(basic_data)
        
        # æ·±åº¦æ€è€ƒæ•°æ® - è½¬æ¢ä¸ºTrainingExampleæ ¼å¼
        if config.enable_thinking:
            thinking_data = self.data_generator.generate_thinking_training_data(20)
            for thinking_example in thinking_data:
                training_example = TrainingExample(
                    instruction=thinking_example.instruction,
                    input="",
                    output=thinking_example.final_response,
                    thinking=thinking_example.thinking_process,
                    crypto_terms=thinking_example.crypto_terms,
                    difficulty_level=DifficultyLevel.INTERMEDIATE,
                    source_file="thinking_data.md"
                )
                training_data.append(training_example)
        
        # ä¸­æ–‡å¯†ç å­¦æ•°æ®
        if config.enable_chinese_processing:
            chinese_data = self.data_generator.generate_chinese_crypto_data(25)
            training_data.extend(chinese_data)
        
        return training_data
    
    def _create_test_configs(self, config: IntegrationTestConfig, output_dir: str) -> Tuple[Any, ...]:
        """åˆ›å»ºæµ‹è¯•é…ç½®"""
        # è®­ç»ƒé…ç½®
        training_config = TrainingConfig(
            output_dir=output_dir,
            num_train_epochs=config.num_epochs,
            per_device_train_batch_size=config.batch_size,
            gradient_accumulation_steps=2,
            learning_rate=2e-4,
            warmup_ratio=0.1,
            save_steps=50,
            logging_steps=10,
            fp16=True,
            dataloader_num_workers=2,
            remove_unused_columns=False
        )
        
        # æ•°æ®é…ç½®
        data_config = DataConfig(
            max_samples=config.sequence_length,
            train_split_ratio=0.8,
            eval_split_ratio=0.1,
            test_split_ratio=0.1,
            preserve_thinking_tags=config.enable_thinking,
            preserve_crypto_terms=config.enable_crypto_terms,
            enable_chinese_preprocessing=config.enable_chinese_processing
        )
        
        # LoRAé…ç½®
        lora_config = LoRAMemoryProfile(
            rank=16,
            alpha=32,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
        )
        
        # å¹¶è¡Œé…ç½®
        from src.parallel_config import ParallelStrategy
        parallel_config = ParallelConfig(
            strategy=ParallelStrategy.DATA_PARALLEL if config.gpu_count > 1 else ParallelStrategy.AUTO,
            data_parallel_size=config.gpu_count,
            master_addr="localhost",
            master_port=29500,
            communication_backend=CommunicationBackend.NCCL if config.gpu_count > 1 else CommunicationBackend.GLOO,
            enable_mixed_precision=True,
            gradient_accumulation_steps=2
        )
        
        # ç³»ç»Ÿé…ç½®
        system_config = SystemConfig(
            output_dir=output_dir,
            cache_dir=str(self.output_dir / "cache"),
            enable_multi_gpu=config.gpu_count > 1,
            gpu_ids=list(range(config.gpu_count)),
            enable_zero_optimization=config.gpu_count > 1
        )
        
        return training_config, data_config, lora_config, parallel_config, system_config
    
    def _simulate_training_pipeline(self, training_data: List[Any], 
                                  training_config: TrainingConfig,
                                  data_config: DataConfig,
                                  lora_config: LoRAMemoryProfile,
                                  parallel_config: ParallelConfig,
                                  system_config: SystemConfig,
                                  output_dir: str) -> bool:
        """æ¨¡æ‹Ÿè®­ç»ƒæµæ°´çº¿"""
        try:
            # æ•°æ®é¢„å¤„ç†
            self.logger.info("æ‰§è¡Œæ•°æ®é¢„å¤„ç†...")
            
            # æ•°æ®é›†åˆ†å‰²
            self.logger.info("å¼€å§‹æ•°æ®é›†åˆ†å‰²...")
            splitter = DatasetSplitter()
            splits = splitter.split_dataset(
                training_data,
                custom_ratios=(data_config.train_split_ratio, data_config.eval_split_ratio, data_config.test_split_ratio)
            )
            
            self.logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ: è®­ç»ƒ{len(splits.train_examples)}, éªŒè¯{len(splits.val_examples)}, æµ‹è¯•{len(splits.test_examples)}")
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            self.logger.info("å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹...")
            
            # åˆå§‹åŒ–è®­ç»ƒç›‘æ§
            self.logger.info("åˆå§‹åŒ–è®­ç»ƒç›‘æ§...")
            gpu_ids = system_config.gpu_ids if system_config.gpu_ids else [0]
            self.logger.info(f"ä½¿ç”¨GPU IDs: {gpu_ids}")
            
            try:
                training_monitor = TrainingMonitor(
                    gpu_ids=gpu_ids,
                    log_dir=str(Path(output_dir) / "monitor_logs"),
                    save_interval=10
                )
                self.logger.info("è®­ç»ƒç›‘æ§å™¨åˆ›å»ºæˆåŠŸ")
            except Exception as monitor_error:
                self.logger.error(f"è®­ç»ƒç›‘æ§å™¨åˆ›å»ºå¤±è´¥: {monitor_error}")
                # ç»§ç»­æ‰§è¡Œï¼Œä¸ä½¿ç”¨ç›‘æ§å™¨
                training_monitor = None
            
            # å¯åŠ¨ç›‘æ§
            if training_monitor:
                try:
                    training_monitor.start_monitoring()
                    self.logger.info("è®­ç»ƒç›‘æ§å¯åŠ¨æˆåŠŸ")
                except Exception as start_error:
                    self.logger.error(f"è®­ç»ƒç›‘æ§å¯åŠ¨å¤±è´¥: {start_error}")
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
            for epoch in range(training_config.num_train_epochs):
                self.logger.info(f"æ¨¡æ‹Ÿè®­ç»ƒ Epoch {epoch + 1}/{training_config.num_train_epochs}")
                
                # æ¨¡æ‹Ÿæ‰¹æ¬¡å¤„ç†
                batch_count = len(splits.train_examples) // training_config.per_device_train_batch_size
                for batch_idx in range(min(batch_count, 10)):  # é™åˆ¶æ‰¹æ¬¡æ•°é‡ä»¥èŠ‚çœæ—¶é—´
                    time.sleep(0.1)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
                    
                    if batch_idx % 5 == 0:
                        self.logger.info(f"  å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{min(batch_count, 10)}")
            
            # åœæ­¢ç›‘æ§
            if training_monitor:
                try:
                    training_monitor.stop_monitoring()
                    self.logger.info("è®­ç»ƒç›‘æ§åœæ­¢æˆåŠŸ")
                except Exception as stop_error:
                    self.logger.error(f"è®­ç»ƒç›‘æ§åœæ­¢å¤±è´¥: {stop_error}")
            
            self.logger.info("è®­ç»ƒæµæ°´çº¿æ¨¡æ‹Ÿå®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæµæ°´çº¿æ¨¡æ‹Ÿå¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    def _collect_test_metrics(self, config: IntegrationTestConfig, 
                            training_data: List[Any]) -> Dict[str, Any]:
        """æ”¶é›†æµ‹è¯•æŒ‡æ ‡"""
        metrics = {
            "test_config": config.to_dict(),
            "data_statistics": {
                "total_samples": len(training_data),
                "thinking_samples": sum(1 for item in training_data if hasattr(item, 'thinking_process')),
                "chinese_samples": sum(1 for item in training_data if any('ä¸­' in str(getattr(item, attr, '')) for attr in ['instruction', 'output'])),
                "crypto_samples": sum(1 for item in training_data if hasattr(item, 'crypto_terms') and item.crypto_terms)
            },
            "processing_metrics": {
                "data_generation_success": True,
                "config_creation_success": True,
                "pipeline_simulation_success": True
            }
        }
        
        return metrics
    
    def _calculate_performance_metrics(self, config: IntegrationTestConfig,
                                     duration: float) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        # ä¼°ç®—å¤„ç†çš„æ•°æ®é‡
        estimated_samples = 50  # åŸºäºç”Ÿæˆçš„æµ‹è¯•æ•°æ®
        estimated_tokens = estimated_samples * config.sequence_length
        
        performance_metrics = {
            "samples_per_second": estimated_samples / duration if duration > 0 else 0,
            "tokens_per_second": estimated_tokens / duration if duration > 0 else 0,
            "duration_vs_expected": duration / (config.expected_duration_minutes * 60),
            "memory_efficiency": "simulated",
            "gpu_utilization": "simulated",
            "throughput_score": (estimated_samples * config.sequence_length) / duration if duration > 0 else 0
        }
        
        return performance_metrics
    
    def _generate_test_report(self, total_duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        successful_tests = [r for r in self.test_results if r.success]
        failed_tests = [r for r in self.test_results if not r.success]
        
        report = {
            "test_summary": {
                "total_tests": len(self.test_results),
                "successful_tests": len(successful_tests),
                "failed_tests": len(failed_tests),
                "success_rate": len(successful_tests) / len(self.test_results) if self.test_results else 0,
                "total_duration_seconds": total_duration,
                "average_test_duration": sum(r.duration_seconds for r in self.test_results) / len(self.test_results) if self.test_results else 0
            },
            "test_results": [result.to_dict() for result in self.test_results],
            "performance_analysis": self._analyze_performance(),
            "memory_analysis": self._analyze_memory_usage(),
            "recommendations": self._generate_recommendations(),
            "environment_info": self._collect_environment_info(),
            "generated_at": datetime.now().isoformat()
        }
        
        return report
    
    def _analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½"""
        if not self.test_results:
            return {"error": "æ²¡æœ‰æµ‹è¯•ç»“æœ"}
        
        successful_results = [r for r in self.test_results if r.success and r.performance_metrics]
        
        if not successful_results:
            return {"error": "æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ"}
        
        # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
        durations = [r.duration_seconds for r in successful_results]
        tokens_per_second = [
            r.performance_metrics.get("tokens_per_second", 0) 
            for r in successful_results 
            if r.performance_metrics
        ]
        
        analysis = {
            "average_duration": sum(durations) / len(durations),
            "min_duration": min(durations),
            "max_duration": max(durations),
            "average_tokens_per_second": sum(tokens_per_second) / len(tokens_per_second) if tokens_per_second else 0,
            "performance_variance": self._calculate_variance(durations)
        }
        
        return analysis
    
    def _analyze_memory_usage(self) -> Dict[str, Any]:
        """åˆ†æå†…å­˜ä½¿ç”¨"""
        memory_results = [
            r for r in self.test_results 
            if r.success and r.memory_usage
        ]
        
        if not memory_results:
            return {"info": "æ²¡æœ‰å†…å­˜ä½¿ç”¨æ•°æ®ï¼ˆå¯èƒ½æœªå¯ç”¨å†…å­˜ç›‘æ§ï¼‰"}
        
        # æå–å†…å­˜ç»Ÿè®¡
        peak_utilizations = []
        avg_utilizations = []
        
        for result in memory_results:
            if result.memory_usage and "memory_statistics" in result.memory_usage:
                stats = result.memory_usage["memory_statistics"]
                peak_utilizations.append(stats.get("max_utilization", 0))
                avg_utilizations.append(stats.get("avg_utilization", 0))
        
        analysis = {
            "average_peak_utilization": sum(peak_utilizations) / len(peak_utilizations) if peak_utilizations else 0,
            "average_utilization": sum(avg_utilizations) / len(avg_utilizations) if avg_utilizations else 0,
            "memory_pressure_events": sum(
                result.memory_usage.get("pressure_distribution", {}).get("high", 0) +
                result.memory_usage.get("pressure_distribution", {}).get("critical", 0)
                for result in memory_results
            )
        }
        
        return analysis
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        failed_tests = [r for r in self.test_results if not r.success]
        if failed_tests:
            recommendations.append(f"æœ‰{len(failed_tests)}ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥é”™è¯¯åŸå› å¹¶ä¿®å¤")
        
        # æ€§èƒ½å»ºè®®
        performance_analysis = self._analyze_performance()
        if "average_duration" in performance_analysis:
            avg_duration = performance_analysis["average_duration"]
            if avg_duration > 1800:  # 30åˆ†é’Ÿ
                recommendations.append("æµ‹è¯•æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–è®­ç»ƒæ€§èƒ½æˆ–å‡å°‘æµ‹è¯•æ•°æ®é‡")
        
        # å†…å­˜å»ºè®®
        memory_analysis = self._analyze_memory_usage()
        if "average_peak_utilization" in memory_analysis:
            peak_util = memory_analysis["average_peak_utilization"]
            if peak_util > 0.9:
                recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¯ç”¨æ›´å¤šå†…å­˜ä¼˜åŒ–ç­–ç•¥")
        
        if not recommendations:
            recommendations.append("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        
        return recommendations
    
    def _collect_environment_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç¯å¢ƒä¿¡æ¯"""
        gpu_infos = self.gpu_detector.get_all_gpu_info()
        
        return {
            "python_version": sys.version,
            "pytorch_version": "æ¨¡æ‹Ÿç¯å¢ƒ",
            "cuda_available": len(gpu_infos) > 0,
            "gpu_count": len(gpu_infos),
            "gpu_info": [
                {
                    "gpu_id": gpu.gpu_id,
                    "name": gpu.name,
                    "total_memory": gpu.total_memory,
                    "compute_capability": getattr(gpu, 'compute_capability', 'unknown')
                }
                for gpu in gpu_infos
            ],
            "system_info": {
                "platform": sys.platform,
                "cpu_count": mp.cpu_count()
            }
        }
    
    def _calculate_variance(self, values: List[float]) -> float:
        """è®¡ç®—æ–¹å·®"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance
    
    def _save_test_report(self, report: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        # JSONæ ¼å¼æŠ¥å‘Š
        json_report_path = self.output_dir / "comprehensive_integration_test_report.json"
        with open(json_report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆç®€åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
        text_report_path = self.output_dir / "comprehensive_integration_test_summary.txt"
        with open(text_report_path, 'w', encoding='utf-8') as f:
            f.write("ç»¼åˆç«¯åˆ°ç«¯é›†æˆæµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            summary = report["test_summary"]
            f.write(f"æµ‹è¯•æ€»æ•°: {summary['total_tests']}\n")
            f.write(f"æˆåŠŸæµ‹è¯•: {summary['successful_tests']}\n")
            f.write(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}\n")
            f.write(f"æˆåŠŸç‡: {summary['success_rate']:.2%}\n")
            f.write(f"æ€»è€—æ—¶: {summary['total_duration_seconds']:.2f}ç§’\n\n")
            
            f.write("æµ‹è¯•ç»“æœè¯¦æƒ…:\n")
            f.write("-" * 30 + "\n")
            for result in self.test_results:
                status = "âœ“" if result.success else "âœ—"
                f.write(f"{status} {result.test_name}: {result.duration_seconds:.2f}ç§’\n")
                if not result.success and result.error_message:
                    f.write(f"  é”™è¯¯: {result.error_message}\n")
            
            f.write("\næ”¹è¿›å»ºè®®:\n")
            f.write("-" * 30 + "\n")
            for rec in report["recommendations"]:
                f.write(f"â€¢ {rec}\n")
        
        self.logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {json_report_path}")
        self.logger.info(f"æµ‹è¯•æ‘˜è¦å·²ä¿å­˜: {text_report_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ç»¼åˆç«¯åˆ°ç«¯é›†æˆæµ‹è¯•...")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = ComprehensiveIntegrationTestSuite()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    start_time = time.time()
    report = test_suite.run_all_tests()
    total_time = time.time() - start_time
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print(f"\n{'='*60}")
    print("ç»¼åˆé›†æˆæµ‹è¯•å®Œæˆ!")
    
    summary = report["test_summary"]
    print(f"æµ‹è¯•æ€»æ•°: {summary['total_tests']}")
    print(f"æˆåŠŸæµ‹è¯•: {summary['successful_tests']}")
    print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
    print(f"æˆåŠŸç‡: {summary['success_rate']:.2%}")
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    # æ˜¾ç¤ºå»ºè®®
    print(f"\næ”¹è¿›å»ºè®®:")
    for i, rec in enumerate(report["recommendations"], 1):
        print(f"{i}. {rec}")
    
    # è¿”å›æˆåŠŸçŠ¶æ€
    success_rate = summary['success_rate']
    if success_rate >= 0.8:
        print(f"\nğŸ‰ ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•åŸºæœ¬é€šè¿‡!")
        print("âœ… ä»»åŠ¡13.1 - ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å®ç°å®Œæˆ")
        return True
    else:
        print(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)