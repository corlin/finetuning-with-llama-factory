#!/usr/bin/env python3
"""
é›†æˆå¯¼å‡ºå’Œæµ‹è¯•ç¨‹åº

æœ¬ç¨‹åºç»“åˆäº†æ¨¡å‹å¯¼å‡ºå’Œæµ‹è¯•åŠŸèƒ½ï¼Œæä¾›ä¸€ç«™å¼çš„æ¨¡å‹å¤„ç†è§£å†³æ–¹æ¡ˆã€‚
"""

import os
import sys
import torch
import logging
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

try:
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        BitsAndBytesConfig
    )
    from peft import PeftModel, PeftConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("è­¦å‘Š: transformersæˆ–peftåº“ä¸å¯ç”¨")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IntegratedExportAndTest:
    """é›†æˆå¯¼å‡ºå’Œæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æµ‹è¯•ç”¨ä¾‹
        self.test_cases = [
            {
                "prompt": "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ",
                "expected_keywords": ["å¯¹ç§°åŠ å¯†", "é«˜çº§åŠ å¯†æ ‡å‡†", "åˆ†ç»„å¯†ç "],
                "max_length": 100
            },
            {
                "prompt": "<thinking>æˆ‘éœ€è¦è§£é‡Šè¿™ä¸ªæ¦‚å¿µ</thinking>è¯·è§£é‡Šæ•°å­—ç­¾åçš„ä½œç”¨ã€‚",
                "expected_keywords": ["å“ˆå¸Œå‡½æ•°", "ç§é’¥", "å…¬é’¥", "å®Œæ•´æ€§"],
                "max_length": 120
            },
            {
                "prompt": "æ¤­åœ†æ›²çº¿å¯†ç å­¦æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
                "expected_keywords": ["å¯†é’¥é•¿åº¦", "å®‰å…¨æ€§", "è®¡ç®—æ•ˆç‡"],
                "max_length": 100
            }
        ]
    
    def load_and_merge_checkpoint(
        self, 
        checkpoint_path: str,
        base_model_path: str = "Qwen/Qwen3-4B-Thinking-2507"
    ):
        """åŠ è½½å¹¶åˆå¹¶æ£€æŸ¥ç‚¹"""
        logger.info("æ­¥éª¤1: åŠ è½½å’Œåˆå¹¶æ£€æŸ¥ç‚¹")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºLoRAæ£€æŸ¥ç‚¹
        checkpoint_dir = Path(checkpoint_path)
        adapter_config_path = checkpoint_dir / "adapter_config.json"
        
        if adapter_config_path.exists():
            logger.info("æ£€æµ‹åˆ°LoRAæ£€æŸ¥ç‚¹ï¼Œè¿›è¡Œåˆå¹¶...")
            
            # åŠ è½½LoRAé€‚é…å™¨
            model_with_lora = PeftModel.from_pretrained(
                base_model,
                checkpoint_path,
                torch_dtype=torch.float16
            )
            
            # åˆå¹¶LoRAæƒé‡
            merged_model = model_with_lora.merge_and_unload()
            logger.info("âœ… LoRAæƒé‡åˆå¹¶å®Œæˆ")
            return merged_model, tokenizer
        else:
            logger.info("ä½¿ç”¨åŸºç¡€æ¨¡å‹...")
            return base_model, tokenizer
    
    def export_multiple_formats(self, model, tokenizer, output_dir: str):
        """å¯¼å‡ºå¤šç§æ ¼å¼å¹¶ç«‹å³æµ‹è¯•"""
        logger.info("æ­¥éª¤2: å¯¼å‡ºå¤šç§æ ¼å¼")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        formats_to_export = [
            ("fp16", "FP16ç²¾åº¦æ¨¡å‹"),
            ("int8_simulated", "æ¨¡æ‹ŸINT8é‡åŒ–"),
            ("int4_simulated", "æ¨¡æ‹ŸINT4é‡åŒ–")
        ]
        
        results = {}
        
        for format_name, description in formats_to_export:
            logger.info(f"\nå¯¼å‡º {format_name.upper()} æ ¼å¼: {description}")
            
            format_dir = output_path / format_name
            format_dir.mkdir(exist_ok=True)
            
            try:
                # å¯¼å‡ºæ¨¡å‹
                export_result = self._export_format(model, tokenizer, format_dir, format_name)
                
                if export_result["success"]:
                    # ç«‹å³æµ‹è¯•
                    logger.info(f"æµ‹è¯• {format_name.upper()} æ¨¡å‹...")
                    test_result = self._test_model_immediately(format_dir, format_name)
                    
                    results[format_name] = {
                        **export_result,
                        "test_result": test_result
                    }
                    
                    if test_result["success"]:
                        logger.info(f"âœ… {format_name.upper()} å¯¼å‡ºå’Œæµ‹è¯•æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ {format_name.upper()} å¯¼å‡ºæˆåŠŸä½†æµ‹è¯•å¤±è´¥")
                else:
                    results[format_name] = export_result
                    logger.error(f"âŒ {format_name.upper()} å¯¼å‡ºå¤±è´¥")
                
            except Exception as e:
                logger.error(f"âŒ {format_name.upper()} å¤„ç†å¤±è´¥: {e}")
                results[format_name] = {
                    "success": False,
                    "error": str(e)
                }
        
        return results
    
    def _export_format(self, model, tokenizer, output_dir: Path, format_name: str):
        """å¯¼å‡ºç‰¹å®šæ ¼å¼"""
        try:
            if format_name == "fp16":
                # FP16æ ¼å¼
                model_fp16 = model.half()
                model_fp16.save_pretrained(
                    output_dir,
                    safe_serialization=True,
                    max_shard_size="2GB"
                )
                
            elif format_name == "int8_simulated":
                # æ¨¡æ‹ŸINT8é‡åŒ–
                model_int8 = self._simulate_quantization(model, bits=8)
                model_int8.save_pretrained(
                    output_dir,
                    safe_serialization=True,
                    max_shard_size="1GB"
                )
                
            elif format_name == "int4_simulated":
                # æ¨¡æ‹ŸINT4é‡åŒ–
                model_int4 = self._simulate_quantization(model, bits=4)
                model_int4.save_pretrained(
                    output_dir,
                    safe_serialization=True,
                    max_shard_size="500MB"
                )
            
            # ä¿å­˜tokenizer
            tokenizer.save_pretrained(output_dir)
            
            # è®¡ç®—å¤§å°
            size_mb = self._get_directory_size(output_dir)
            
            # åˆ›å»ºæ ¼å¼ä¿¡æ¯
            format_info = {
                "format": format_name,
                "export_time": datetime.now().isoformat(),
                "size_mb": size_mb,
                "compression_note": self._get_compression_note(format_name)
            }
            
            with open(output_dir / "format_info.json", "w", encoding="utf-8") as f:
                json.dump(format_info, f, indent=2, ensure_ascii=False)
            
            return {
                "success": True,
                "format": format_name,
                "size_mb": size_mb,
                "output_path": str(output_dir)
            }
            
        except Exception as e:
            return {
                "success": False,
                "format": format_name,
                "error": str(e)
            }
    
    def _simulate_quantization(self, model, bits: int):
        """æ¨¡æ‹Ÿé‡åŒ–è¿‡ç¨‹"""
        model_copy = model.cpu()
        
        with torch.no_grad():
            for name, param in model_copy.named_parameters():
                if 'weight' in name and len(param.shape) >= 2:
                    # æ¨¡æ‹Ÿé‡åŒ–
                    param_min = param.min()
                    param_max = param.max()
                    
                    # è®¡ç®—é‡åŒ–çº§åˆ«
                    levels = 2 ** bits - 1
                    scale = (param_max - param_min) / levels
                    
                    # é‡åŒ–å’Œåé‡åŒ–
                    quantized = torch.round((param - param_min) / scale)
                    quantized = torch.clamp(quantized, 0, levels)
                    dequantized = quantized * scale + param_min
                    
                    param.copy_(dequantized)
        
        return model_copy
    
    def _test_model_immediately(self, model_dir: Path, format_name: str):
        """ç«‹å³æµ‹è¯•å¯¼å‡ºçš„æ¨¡å‹"""
        try:
            # åŠ è½½æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(
                str(model_dir),
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                str(model_dir),
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # è¿è¡Œæµ‹è¯•ç”¨ä¾‹
            test_results = []
            total_time = 0
            
            model.eval()
            with torch.no_grad():
                for i, test_case in enumerate(self.test_cases):
                    start_time = time.time()
                    
                    try:
                        inputs = tokenizer(
                            test_case["prompt"],
                            return_tensors="pt",
                            padding=True,
                            truncation=True
                        )
                        
                        inputs = {k: v.to(model.device) for k, v in inputs.items()}
                        
                        outputs = model.generate(
                            **inputs,
                            max_length=inputs["input_ids"].shape[1] + test_case["max_length"],
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=tokenizer.pad_token_id
                        )
                        
                        response = tokenizer.decode(
                            outputs[0][inputs["input_ids"].shape[1]:],
                            skip_special_tokens=True
                        )
                        
                        inference_time = time.time() - start_time
                        total_time += inference_time
                        
                        # è¯„ä¼°å“åº”
                        score = self._evaluate_response(response, test_case)
                        
                        test_results.append({
                            "test_id": i + 1,
                            "prompt": test_case["prompt"][:50] + "...",
                            "response_length": len(response),
                            "inference_time": inference_time,
                            "score": score,
                            "success": True
                        })
                        
                    except Exception as e:
                        test_results.append({
                            "test_id": i + 1,
                            "prompt": test_case["prompt"][:50] + "...",
                            "success": False,
                            "error": str(e)
                        })
            
            # è®¡ç®—æ€»ä½“ç»“æœ
            successful_tests = [t for t in test_results if t["success"]]
            avg_score = sum(t["score"] for t in successful_tests) / len(successful_tests) if successful_tests else 0
            avg_time = total_time / len(successful_tests) if successful_tests else 0
            
            # æ¸…ç†å†…å­˜
            del model
            del tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return {
                "success": True,
                "total_tests": len(self.test_cases),
                "successful_tests": len(successful_tests),
                "success_rate": len(successful_tests) / len(self.test_cases),
                "avg_score": avg_score,
                "avg_inference_time": avg_time,
                "test_details": test_results
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def _evaluate_response(self, response: str, test_case: Dict[str, Any]) -> float:
        """è¯„ä¼°å“åº”è´¨é‡"""
        if not response.strip():
            return 0.0
        
        score = 0.0
        
        # å…³é”®è¯åŒ¹é…
        expected_keywords = test_case.get("expected_keywords", [])
        if expected_keywords:
            matched = sum(1 for kw in expected_keywords if kw in response)
            score += (matched / len(expected_keywords)) * 0.5
        
        # ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        chinese_chars = sum(1 for char in response if '\u4e00' <= char <= '\u9fff')
        chinese_ratio = chinese_chars / len(response) if response else 0
        score += min(chinese_ratio * 2, 1.0) * 0.3
        
        # é•¿åº¦åˆç†æ€§
        if 20 <= len(response) <= 300:
            score += 0.2
        elif len(response) > 10:
            score += 0.1
        
        return min(score, 1.0)
    
    def _get_directory_size(self, directory: Path) -> float:
        """è®¡ç®—ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
        try:
            total_size = sum(
                f.stat().st_size for f in directory.rglob('*') if f.is_file()
            )
            return total_size / 1024 / 1024
        except:
            return 0.0
    
    def _get_compression_note(self, format_name: str) -> str:
        """è·å–å‹ç¼©è¯´æ˜"""
        notes = {
            "fp16": "åŠç²¾åº¦æµ®ç‚¹ï¼Œå¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½",
            "int8_simulated": "æ¨¡æ‹Ÿ8ä½é‡åŒ–ï¼Œçº¦50%å‹ç¼©",
            "int4_simulated": "æ¨¡æ‹Ÿ4ä½é‡åŒ–ï¼Œçº¦75%å‹ç¼©"
        }
        return notes.get(format_name, "æœªçŸ¥æ ¼å¼")
    
    def generate_comprehensive_report(self, results: Dict[str, Any], output_dir: str):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        logger.info("æ­¥éª¤3: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
        
        # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
        report = {
            "export_and_test_time": datetime.now().isoformat(),
            "summary": {
                "total_formats": len(results),
                "successful_exports": sum(1 for r in results.values() if r.get("success", False)),
                "successful_tests": sum(1 for r in results.values() 
                                      if r.get("success", False) and r.get("test_result", {}).get("success", False))
            },
            "results": results,
            "performance_comparison": {},
            "recommendations": []
        }
        
        # æ€§èƒ½å¯¹æ¯”
        successful_results = {k: v for k, v in results.items() if v.get("success", False)}
        
        if successful_results:
            for format_name, result in successful_results.items():
                test_result = result.get("test_result", {})
                
                report["performance_comparison"][format_name] = {
                    "size_mb": result.get("size_mb", 0),
                    "test_success": test_result.get("success", False),
                    "success_rate": test_result.get("success_rate", 0),
                    "avg_score": test_result.get("avg_score", 0),
                    "avg_inference_time": test_result.get("avg_inference_time", 0)
                }
        
        # ç”Ÿæˆå»ºè®®
        if report["summary"]["successful_tests"] > 0:
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_models = {}
            for format_name, perf in report["performance_comparison"].items():
                if perf["test_success"]:
                    if not best_models.get("quality") or perf["avg_score"] > report["performance_comparison"][best_models["quality"]]["avg_score"]:
                        best_models["quality"] = format_name
                    
                    if not best_models.get("speed") or perf["avg_inference_time"] < report["performance_comparison"][best_models["speed"]]["avg_inference_time"]:
                        best_models["speed"] = format_name
                    
                    if not best_models.get("size") or perf["size_mb"] < report["performance_comparison"][best_models["size"]]["size_mb"]:
                        best_models["size"] = format_name
            
            report["recommendations"] = [
                f"æœ€ä½³è´¨é‡: {best_models.get('quality', 'N/A')}",
                f"æœ€å¿«æ¨ç†: {best_models.get('speed', 'N/A')}",
                f"æœ€å°ä½“ç§¯: {best_models.get('size', 'N/A')}"
            ]
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path(output_dir) / "comprehensive_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºæ‘˜è¦
        self._display_final_summary(report)
        
        logger.info(f"ğŸ“Š ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return report
    
    def _display_final_summary(self, report: Dict[str, Any]):
        """æ˜¾ç¤ºæœ€ç»ˆæ‘˜è¦"""
        logger.info("\n" + "=" * 60)
        logger.info("é›†æˆå¯¼å‡ºå’Œæµ‹è¯•æ‘˜è¦")
        logger.info("=" * 60)
        
        summary = report["summary"]
        logger.info(f"æ€»æ ¼å¼æ•°: {summary['total_formats']}")
        logger.info(f"æˆåŠŸå¯¼å‡º: {summary['successful_exports']}")
        logger.info(f"æµ‹è¯•é€šè¿‡: {summary['successful_tests']}")
        
        if "performance_comparison" in report:
            logger.info("\næ€§èƒ½å¯¹æ¯”:")
            logger.info("-" * 40)
            
            for format_name, perf in report["performance_comparison"].items():
                status = "âœ…" if perf["test_success"] else "âŒ"
                logger.info(f"{status} {format_name.upper()}:")
                logger.info(f"   å¤§å°: {perf['size_mb']:.1f}MB")
                if perf["test_success"]:
                    logger.info(f"   è´¨é‡: {perf['avg_score']:.2f}")
                    logger.info(f"   é€Ÿåº¦: {perf['avg_inference_time']:.2f}s")
                    logger.info(f"   æˆåŠŸç‡: {perf['success_rate']:.1%}")
        
        if report.get("recommendations"):
            logger.info("\næ¨è:")
            logger.info("-" * 20)
            for rec in report["recommendations"]:
                logger.info(f"  {rec}")
    
    def run_integrated_demo(
        self,
        checkpoint_path: str = "qwen3_4b_thinking_output/final_model",
        base_model_path: str = "Qwen/Qwen3-4B-Thinking-2507",
        output_dir: str = "integrated_export_test_output"
    ):
        """è¿è¡Œé›†æˆæ¼”ç¤º"""
        logger.info("=" * 60)
        logger.info("é›†æˆå¯¼å‡ºå’Œæµ‹è¯•æ¼”ç¤º")
        logger.info("=" * 60)
        
        if not TRANSFORMERS_AVAILABLE:
            logger.error("transformersåº“ä¸å¯ç”¨")
            return
        
        try:
            # æ­¥éª¤1: åŠ è½½æ¨¡å‹
            model, tokenizer = self.load_and_merge_checkpoint(checkpoint_path, base_model_path)
            
            # æ­¥éª¤2: å¯¼å‡ºå’Œæµ‹è¯•
            results = self.export_multiple_formats(model, tokenizer, output_dir)
            
            # æ­¥éª¤3: ç”ŸæˆæŠ¥å‘Š
            report = self.generate_comprehensive_report(results, output_dir)
            
            logger.info("\nğŸ‰ é›†æˆæ¼”ç¤ºå®Œæˆï¼")
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            logger.info("ğŸ“– æŸ¥çœ‹ comprehensive_report.json äº†è§£è¯¦ç»†ä¿¡æ¯")
            
        except Exception as e:
            logger.error(f"é›†æˆæ¼”ç¤ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é›†æˆå¯¼å‡ºå’Œæµ‹è¯•æ¼”ç¤º")
    parser.add_argument(
        "--checkpoint_path",
        default="qwen3_4b_thinking_output/final_model",
        help="æ£€æŸ¥ç‚¹è·¯å¾„"
    )
    parser.add_argument(
        "--base_model",
        default="Qwen/Qwen3-4B-Thinking-2507",
        help="åŸºç¡€æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--output_dir",
        default="integrated_export_test_output",
        help="è¾“å‡ºç›®å½•"
    )
    
    args = parser.parse_args()
    
    # è¿è¡Œé›†æˆæ¼”ç¤º
    demo = IntegratedExportAndTest()
    demo.run_integrated_demo(
        checkpoint_path=args.checkpoint_path,
        base_model_path=args.base_model,
        output_dir=args.output_dir
    )


if __name__ == "__main__":
    main()