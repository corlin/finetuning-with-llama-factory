#!/usr/bin/env python3
"""
ç›´æ¥åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬
ä½¿ç”¨transformerså’ŒPEFTè¿›è¡Œå¤šGPUè®­ç»ƒ
"""

import os
import sys
import json
import yaml
import torch
import logging
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    log_file = f"distributed_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    class SafeStreamHandler(logging.StreamHandler):
        def emit(self, record):
            try:
                msg = self.format(record)
                try:
                    stream = self.stream
                    stream.write(msg + self.terminator)
                    self.flush()
                except UnicodeEncodeError:
                    safe_msg = msg.replace('ğŸ‰', '[SUCCESS]').replace('âœ…', '[OK]').replace('âŒ', '[FAIL]').replace('ğŸ’¥', '[ERROR]').replace('âš ï¸', '[WARN]')
                    stream.write(safe_msg + self.terminator)
                    self.flush()
            except Exception:
                self.handleError(record)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            SafeStreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def load_config():
    """åŠ è½½é…ç½®"""
    with open('validation_output/training_config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def load_dataset():
    """åŠ è½½æ•°æ®é›†"""
    logger = logging.getLogger(__name__)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    with open('validation_output/train.json', 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    # åŠ è½½éªŒè¯æ•°æ®
    with open('validation_output/val.json', 'r', encoding='utf-8') as f:
        val_data = json.load(f)
    
    logger.info(f"åŠ è½½è®­ç»ƒæ•°æ®: {len(train_data)} ä¸ªæ ·ä¾‹")
    logger.info(f"åŠ è½½éªŒè¯æ•°æ®: {len(val_data)} ä¸ªæ ·ä¾‹")
    
    # è½¬æ¢ä¸ºDatasetæ ¼å¼
    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data)
    
    return train_dataset, val_dataset

def preprocess_function(examples, tokenizer, max_length=2048):
    """æ•°æ®é¢„å¤„ç†å‡½æ•°"""
    inputs = []
    targets = []
    
    for i in range(len(examples['instruction'])):
        instruction = examples['instruction'][i]
        input_text = examples['input'][i] if examples['input'][i] else ""
        output = examples['output'][i]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        if input_text:
            full_input = f"æŒ‡ä»¤: {instruction}\nè¾“å…¥: {input_text}\nå›ç­”: "
        else:
            full_input = f"æŒ‡ä»¤: {instruction}\nå›ç­”: "
        
        # æ„å»ºå®Œæ•´æ–‡æœ¬
        full_text = full_input + output
        
        inputs.append(full_input)
        targets.append(full_text)
    
    # Tokenize
    model_inputs = tokenizer(
        targets,
        max_length=max_length,
        truncation=True,
        padding=False,
        return_tensors=None
    )
    
    # åˆ›å»ºlabels
    labels = model_inputs["input_ids"].copy()
    
    # è®¡ç®—è¾“å…¥éƒ¨åˆ†çš„é•¿åº¦ï¼Œç”¨äºmask
    input_lengths = []
    for inp in inputs:
        input_tokens = tokenizer(inp, add_special_tokens=False)["input_ids"]
        input_lengths.append(len(input_tokens))
    
    # Maskè¾“å…¥éƒ¨åˆ†çš„labels
    for i, input_length in enumerate(input_lengths):
        labels[i][:input_length] = [-100] * input_length
    
    model_inputs["labels"] = labels
    return model_inputs

def setup_model_and_tokenizer(config):
    """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
    logger = logging.getLogger(__name__)
    
    model_name = config['model_name_or_path']
    logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        cache_dir="./cache"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16 if config.get('bf16', True) else torch.float16,
        device_map=None,  # æˆ‘ä»¬æ‰‹åŠ¨å¤„ç†è®¾å¤‡åˆ†é…
        cache_dir="./cache"
    )
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        r=config.get('lora_rank', 8),
        lora_alpha=config.get('lora_alpha', 16),
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=config.get('lora_dropout', 0.1),
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    logger.info(f"æ€»å‚æ•°: {total_params:,}")
    logger.info(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params*100:.2f}%")
    
    return model, tokenizer

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    logger = setup_logging()
    logger.info("å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒ")
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        logger.error("CUDAä¸å¯ç”¨")
        return False
    
    gpu_count = torch.cuda.device_count()
    logger.info(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    # åŠ è½½é…ç½®
    config = load_config()
    logger.info("é…ç½®åŠ è½½å®Œæˆ")
    
    # åŠ è½½æ•°æ®
    train_dataset, val_dataset = load_dataset()
    
    # è®¾ç½®æ¨¡å‹å’Œtokenizer
    model, tokenizer = setup_model_and_tokenizer(config)
    
    # é¢„å¤„ç†æ•°æ®
    logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    train_dataset = train_dataset.map(
        lambda examples: preprocess_function(examples, tokenizer, config.get('cutoff_len', 2048)),
        batched=True,
        remove_columns=train_dataset.column_names
    )
    
    val_dataset = val_dataset.map(
        lambda examples: preprocess_function(examples, tokenizer, config.get('cutoff_len', 2048)),
        batched=True,
        remove_columns=val_dataset.column_names
    )
    
    logger.info("æ•°æ®é¢„å¤„ç†å®Œæˆ")
    
    # è®¾ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config.get('output_dir', './output'),
        overwrite_output_dir=config.get('overwrite_output_dir', True),
        num_train_epochs=config.get('num_train_epochs', 1),
        per_device_train_batch_size=config.get('per_device_train_batch_size', 1),
        per_device_eval_batch_size=config.get('per_device_eval_batch_size', 1),
        gradient_accumulation_steps=config.get('gradient_accumulation_steps', 4),
        learning_rate=config.get('learning_rate', 2e-4),
        weight_decay=config.get('weight_decay', 0.01),
        logging_steps=config.get('logging_steps', 5),
        save_steps=config.get('save_steps', 50),
        eval_steps=config.get('eval_steps', 50),
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        lr_scheduler_type=config.get('lr_scheduler_type', 'cosine'),
        warmup_ratio=config.get('warmup_ratio', 0.1),
        bf16=config.get('bf16', True),
        fp16=config.get('fp16', False),
        dataloader_pin_memory=config.get('dataloader_pin_memory', True),
        remove_unused_columns=False,
        report_to=None,  # ç¦ç”¨wandbç­‰
        seed=42,
        data_seed=42,
        # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        ddp_find_unused_parameters=False,
        ddp_timeout=1800,
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True,
        return_tensors="pt"
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ...")
    try:
        trainer.train()
        logger.info("è®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {training_args.output_dir}")
        
        return True
        
    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)