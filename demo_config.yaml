# 综合微调演示配置文件

# 模型配置
model:
  name: "Qwen/Qwen3-4B-Thinking-2507"
  revision: "main"
  template: "qwen"
  
# 训练配置
training:
  num_epochs: 2
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  lr_scheduler: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01
  
# LoRA配置
lora:
  rank: 8
  alpha: 16
  dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  
# 数据配置
data:
  train_ratio: 0.9
  val_ratio: 0.1
  max_length: 2048
  
# 输出配置
output:
  save_steps: 100
  eval_steps: 100
  logging_steps: 10
  save_total_limit: 3
  
# 系统配置
system:
  mixed_precision: true
  gradient_checkpointing: true
  dataloader_num_workers: 4