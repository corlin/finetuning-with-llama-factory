chinese:
  add_special_tokens: true
  crypto_vocab_path: null
  enable_traditional_conversion: true
  extend_vocab: false
  handle_emoji: true
  normalize_punctuation: true
  padding_side: left
  preserve_thinking_structure: true
  thinking_end_token: </thinking>
  thinking_start_token: <thinking>
  tokenizer_name: Qwen/Qwen3-4B-Thinking-2507
  truncation_side: right
data:
  crypto_vocab_file: null
  data_format: json
  enable_chinese_preprocessing: true
  eval_data_path: ./data/eval
  eval_split_ratio: 0.15
  handle_traditional_chinese: true
  label_column: label
  max_samples: null
  normalize_chinese_text: true
  preserve_crypto_terms: true
  preserve_thinking_tags: true
  remove_duplicates: true
  shuffle_data: true
  test_data_path: ./data/test
  test_split_ratio: 0.15
  text_column: text
  train_data_path: ./data/train
  train_split_ratio: 0.7
  validate_thinking_structure: true
lora:
  bias: none
  inference_mode: false
  lora_alpha: 32
  lora_dropout: 0.1
  r: 16
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  task_type: CAUSAL_LM
  use_dora: false
  use_rslora: false
model:
  bnb_4bit_compute_dtype: float16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  bos_token: <|im_start|>
  cache_dir: null
  device_map: auto
  eos_token: <|im_end|>
  load_in_4bit: false
  load_in_8bit: false
  local_files_only: false
  low_cpu_mem_usage: true
  max_position_embeddings: 32768
  max_seq_length: 2048
  model_name: Qwen/Qwen3-4B-Thinking-2507
  model_revision: main
  output_attentions: false
  output_hidden_states: false
  pad_token: <|endoftext|>
  torch_dtype: auto
  trust_remote_code: true
  unk_token: <|endoftext|>
  use_cache: true
multigpu:
  backend: nccl
  bucket_cap_mb: 25
  cpu_offload: false
  data_parallel: true
  enable_distributed: false
  find_unused_parameters: false
  gradient_as_bucket_view: true
  local_rank: -1
  master_addr: localhost
  master_port: 29500
  model_parallel: false
  nvme_offload: false
  pipeline_parallel: false
  pipeline_parallel_size: 1
  tensor_parallel_size: 1
  world_size: 1
  zero_stage: 2
system:
  cache_dir: ./cache
  cpu_count: null
  cuda_visible_devices: null
  hf_cache_dir: null
  log_file: null
  log_level: INFO
  max_memory_per_gpu: null
  mixed_precision: bf16
  trust_remote_code: true
  use_auth_token: false
training:
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  bf16: true
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  disable_tqdm: false
  eval_steps: 500
  eval_strategy: steps
  fp16: false
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  learning_rate: 0.0002
  load_best_model_at_end: true
  logging_dir: ./logs
  logging_steps: 10
  lr_scheduler_type: cosine
  max_grad_norm: 1.0
  max_seq_length: 2048
  metric_for_best_model: eval_loss
  num_train_epochs: 3
  optim: adamw_torch
  output_dir: ./output
  per_device_eval_batch_size: 1
  per_device_train_batch_size: 1
  preprocessing_num_workers: 4
  remove_unused_columns: false
  report_to: tensorboard
  run_name: qwen3-4b-crypto-finetuning
  save_steps: 500
  save_strategy: steps
  save_total_limit: 3
  seed: 42
  use_cache: false
  warmup_ratio: 0.1
  weight_decay: 0.01
