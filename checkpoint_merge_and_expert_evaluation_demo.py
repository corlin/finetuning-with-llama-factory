#!/usr/bin/env python3
"""
Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°å®Œæ•´æ¼”ç¤º

æœ¬ç¨‹åºæ¼”ç¤ºå®Œæ•´çš„æµç¨‹ï¼š
1. å°†LoRA checkpointä¸åŸºåº§æ¨¡å‹åˆå¹¶
2. ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹å¯¹è¯„ä¼°æ•°æ®è¿›è¡Œä¸“å®¶è¯„ä¼°
3. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    uv run python checkpoint_merge_and_expert_evaluation_demo.py

ä½œè€…: ä¸“å®¶è¯„ä¼°ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
"""

import json
import time
import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import sys

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ£€æŸ¥ä¾èµ–
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("âœ… Transformerså’ŒPEFTåº“å¯ç”¨")
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    logger.warning(f"âš ï¸ Transformersåº“ä¸å¯ç”¨: {e}")

class CheckpointMerger:
    """Checkpointåˆå¹¶å™¨"""
    
    def __init__(self, device: str = "auto"):
        self.device = self._setup_device(device)
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def merge_lora_checkpoint(
        self, 
        checkpoint_path: str, 
        base_model_path: str = "Qwen/Qwen3-4B-Thinking-2507",
        output_path: str = "merged_model_output"
    ) -> Tuple[Any, Any]:
        """
        åˆå¹¶LoRA checkpointåˆ°åŸºåº§æ¨¡å‹
        
        Args:
            checkpoint_path: LoRA checkpointè·¯å¾„
            base_model_path: åŸºåº§æ¨¡å‹è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            (merged_model, tokenizer)
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…transformerså’Œpeftåº“")
        
        try:
            logger.info(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {base_model_path}")
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                base_model_path,
                trust_remote_code=True,
                padding_side="left"
            )
            
            # åŠ è½½åŸºåº§æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )
            
            logger.info(f"ğŸ“¥ åŠ è½½LoRA checkpoint: {checkpoint_path}")
            
            # åŠ è½½LoRAæ¨¡å‹
            model_with_lora = PeftModel.from_pretrained(
                base_model,
                checkpoint_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            
            logger.info("ğŸ”„ åˆå¹¶LoRAæƒé‡åˆ°åŸºåº§æ¨¡å‹...")
            
            # åˆå¹¶æƒé‡
            merged_model = model_with_lora.merge_and_unload()
            
            # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
            output_dir = Path(output_path)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
            
            merged_model.save_pretrained(
                output_path,
                safe_serialization=True,
                max_shard_size="2GB"
            )
            
            tokenizer.save_pretrained(output_path)
            
            # ç”Ÿæˆåˆå¹¶æŠ¥å‘Š
            merge_info = {
                "merge_time": datetime.now().isoformat(),
                "base_model": base_model_path,
                "checkpoint_path": checkpoint_path,
                "output_path": output_path,
                "device_used": self.device,
                "model_dtype": str(merged_model.dtype),
                "success": True
            }
            
            with open(output_dir / "merge_info.json", 'w', encoding='utf-8') as f:
                json.dump(merge_info, f, indent=2, ensure_ascii=False)
            
            logger.info("âœ… æ¨¡å‹åˆå¹¶å®Œæˆ!")
            return merged_model, tokenizer
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå¹¶å¤±è´¥: {e}")
            raise

class QADataProcessor:
    """QAæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        pass
    
    def load_qa_data_from_markdown(self, file_path: str) -> List[Dict[str, Any]]:
        """ä»Markdownæ–‡ä»¶åŠ è½½QAæ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            qa_items = []
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–Q&Aå¯¹
            # åŒ¹é…æ¨¡å¼: ### Qæ•°å­—: é—®é¢˜å†…å®¹ ... Aæ•°å­—: ç­”æ¡ˆå†…å®¹
            qa_pattern = r'### Q(\d+):\s*(.+?)\n\n<thinking>.*?</thinking>\n\nA\1:\s*(.+?)(?=\n### Q|\n## |$)'
            
            matches = re.findall(qa_pattern, content, re.DOTALL)
            
            for match in matches:
                q_num, question, answer = match
                
                # æ¸…ç†æ–‡æœ¬
                question = question.strip()
                answer = answer.strip()
                
                if question and answer:
                    qa_item = {
                        "question_id": f"qa_{q_num}",
                        "question": question,
                        "reference_answer": answer,
                        "context": "å¯†ç åº”ç”¨æ ‡å‡†GB/T 39786-2021",
                        "domain_tags": ["å¯†ç å­¦", "ä¿¡æ¯å®‰å…¨", "å›½å®¶æ ‡å‡†"],
                        "difficulty_level": "intermediate",
                        "expected_concepts": ["å¯†ç åº”ç”¨", "å®‰å…¨è¦æ±‚", "æŠ€æœ¯æ ‡å‡†"]
                    }
                    qa_items.append(qa_item)
            
            logger.info(f"ğŸ“Š ä» {file_path} æå–äº† {len(qa_items)} ä¸ªQAé¡¹")
            return qa_items
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½QAæ•°æ®å¤±è´¥: {e}")
            return []
    
    def load_all_qa_data(self, data_dir: str = "data/raw") -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰QAæ•°æ®æ–‡ä»¶"""
        data_path = Path(data_dir)
        all_qa_items = []
        
        # åŠ è½½enhanced QAæ–‡ä»¶
        enhanced_files = list(data_path.glob("enhanced_QA*.md"))
        
        for file_path in enhanced_files:
            logger.info(f"ğŸ“– å¤„ç†æ–‡ä»¶: {file_path}")
            qa_items = self.load_qa_data_from_markdown(str(file_path))
            all_qa_items.extend(qa_items)
        
        logger.info(f"ğŸ“Š æ€»å…±åŠ è½½äº† {len(all_qa_items)} ä¸ªQAé¡¹")
        return all_qa_items

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model, tokenizer, device: str = "auto"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(model, 'to'):
            self.model = model.to(self.device)
    
    def generate_answer(self, question: str, context: str = "", max_length: int = 512) -> str:
        """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ"""
        try:
            # æ„å»ºprompt
            if context:
                prompt = f"ä¸Šä¸‹æ–‡ï¼š{context}\n\né—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š"
            else:
                prompt = f"é—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š"
            
            # Tokenize
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            )
            
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç 
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            return "ç”Ÿæˆå¤±è´¥"

class ExpertEvaluator:
    """ä¸“å®¶è¯„ä¼°å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.evaluation_dimensions = [
            "semantic_similarity",
            "domain_accuracy", 
            "response_relevance",
            "factual_correctness",
            "completeness",
            "clarity",
            "technical_depth"
        ]
        
        self.dimension_weights = {
            "semantic_similarity": 0.20,
            "domain_accuracy": 0.25,
            "response_relevance": 0.15,
            "factual_correctness": 0.20,
            "completeness": 0.10,
            "clarity": 0.05,
            "technical_depth": 0.05
        }
    
    def evaluate_answer_pair(
        self, 
        question: str, 
        reference_answer: str, 
        model_answer: str,
        context: str = ""
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªQAå¯¹"""
        
        # ç®€åŒ–çš„è¯„ä¼°é€»è¾‘ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼‰
        dimension_scores = {}
        
        # åŸºäºæ–‡æœ¬é•¿åº¦å’Œå…³é”®è¯åŒ¹é…çš„ç®€å•è¯„ä¼°
        ref_words = set(reference_answer.lower().split())
        model_words = set(model_answer.lower().split())
        
        # è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆåŸºäºè¯æ±‡é‡å ï¼‰
        if ref_words and model_words:
            overlap = len(ref_words & model_words)
            union = len(ref_words | model_words)
            jaccard_sim = overlap / union if union > 0 else 0
            dimension_scores["semantic_similarity"] = min(0.95, max(0.3, jaccard_sim + 0.2))
        else:
            dimension_scores["semantic_similarity"] = 0.3
        
        # é¢†åŸŸå‡†ç¡®æ€§ï¼ˆåŸºäºä¸“ä¸šæœ¯è¯­ï¼‰
        domain_terms = ["å¯†ç ", "åŠ å¯†", "å®‰å…¨", "ç®—æ³•", "è®¤è¯", "å®Œæ•´æ€§", "æœºå¯†æ€§", "æ ‡å‡†"]
        ref_domain_count = sum(1 for term in domain_terms if term in reference_answer)
        model_domain_count = sum(1 for term in domain_terms if term in model_answer)
        
        if ref_domain_count > 0:
            domain_accuracy = min(1.0, model_domain_count / ref_domain_count)
        else:
            domain_accuracy = 0.7
        dimension_scores["domain_accuracy"] = max(0.4, domain_accuracy)
        
        # å“åº”ç›¸å…³æ€§ï¼ˆåŸºäºé—®é¢˜å…³é”®è¯ï¼‰
        question_words = set(question.lower().split())
        question_relevance = len(question_words & model_words) / len(question_words) if question_words else 0
        dimension_scores["response_relevance"] = min(0.95, max(0.5, question_relevance + 0.3))
        
        # äº‹å®æ­£ç¡®æ€§ï¼ˆåŸºäºç­”æ¡ˆé•¿åº¦å’Œç»“æ„ï¼‰
        if len(model_answer) > 20 and "ã€‚" in model_answer:
            dimension_scores["factual_correctness"] = min(0.9, max(0.6, len(model_answer) / 200))
        else:
            dimension_scores["factual_correctness"] = 0.5
        
        # å®Œæ•´æ€§ï¼ˆåŸºäºç­”æ¡ˆé•¿åº¦æ¯”è¾ƒï¼‰
        length_ratio = len(model_answer) / len(reference_answer) if reference_answer else 0
        if 0.5 <= length_ratio <= 1.5:
            dimension_scores["completeness"] = min(0.9, 0.7 + length_ratio * 0.1)
        else:
            dimension_scores["completeness"] = max(0.4, 0.8 - abs(length_ratio - 1.0) * 0.3)
        
        # æ¸…æ™°åº¦ï¼ˆåŸºäºå¥å­ç»“æ„ï¼‰
        sentences = model_answer.count("ã€‚") + model_answer.count("ï¼") + model_answer.count("ï¼Ÿ")
        if sentences > 0 and len(model_answer) / sentences < 100:
            dimension_scores["clarity"] = 0.8
        else:
            dimension_scores["clarity"] = 0.6
        
        # æŠ€æœ¯æ·±åº¦ï¼ˆåŸºäºä¸“ä¸šæœ¯è¯­å¯†åº¦ï¼‰
        tech_density = model_domain_count / len(model_answer.split()) if model_answer.split() else 0
        dimension_scores["technical_depth"] = min(0.9, max(0.4, tech_density * 10))
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        overall_score = sum(
            dimension_scores[dim] * self.dimension_weights[dim]
            for dim in dimension_scores
        )
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        suggestions = []
        if dimension_scores["semantic_similarity"] < 0.7:
            suggestions.append("æé«˜ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼æ€§")
        if dimension_scores["domain_accuracy"] < 0.7:
            suggestions.append("å¢åŠ æ›´å¤šä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µ")
        if dimension_scores["completeness"] < 0.7:
            suggestions.append("æä¾›æ›´å®Œæ•´å’Œè¯¦ç»†çš„å›ç­”")
        if dimension_scores["technical_depth"] < 0.6:
            suggestions.append("å¢å¼ºæŠ€æœ¯æ·±åº¦å’Œä¸“ä¸šæ€§")
        
        if not suggestions:
            suggestions.append("ç»§ç»­ä¿æŒå½“å‰çš„é«˜è´¨é‡æ°´å¹³")
        
        return {
            "overall_score": round(overall_score, 3),
            "dimension_scores": {k: round(v, 3) for k, v in dimension_scores.items()},
            "improvement_suggestions": suggestions,
            "evaluation_time": datetime.now().isoformat()
        }
    
    def evaluate_batch(
        self, 
        qa_items: List[Dict[str, Any]], 
        model_answers: List[str]
    ) -> Dict[str, Any]:
        """æ‰¹é‡è¯„ä¼°"""
        
        if len(qa_items) != len(model_answers):
            raise ValueError("QAé¡¹ç›®æ•°é‡ä¸æ¨¡å‹ç­”æ¡ˆæ•°é‡ä¸åŒ¹é…")
        
        individual_results = []
        
        for i, (qa_item, model_answer) in enumerate(zip(qa_items, model_answers)):
            logger.info(f"ğŸ“Š è¯„ä¼°ç¬¬ {i+1}/{len(qa_items)} é¡¹: {qa_item['question_id']}")
            
            result = self.evaluate_answer_pair(
                question=qa_item["question"],
                reference_answer=qa_item["reference_answer"],
                model_answer=model_answer,
                context=qa_item.get("context", "")
            )
            
            result["question_id"] = qa_item["question_id"]
            result["question"] = qa_item["question"]
            result["reference_answer"] = qa_item["reference_answer"]
            result["model_answer"] = model_answer
            
            individual_results.append(result)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        scores = [r["overall_score"] for r in individual_results]
        avg_score = sum(scores) / len(scores)
        max_score = max(scores)
        min_score = min(scores)
        
        # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
        dimension_averages = {}
        for dim in self.evaluation_dimensions:
            dim_scores = [r["dimension_scores"][dim] for r in individual_results]
            dimension_averages[dim] = sum(dim_scores) / len(dim_scores)
        
        return {
            "summary": {
                "total_evaluations": len(individual_results),
                "average_score": round(avg_score, 3),
                "max_score": round(max_score, 3),
                "min_score": round(min_score, 3),
                "score_std": round(
                    (sum((s - avg_score) ** 2 for s in scores) / len(scores)) ** 0.5, 3
                )
            },
            "dimension_averages": {k: round(v, 3) for k, v in dimension_averages.items()},
            "individual_results": individual_results,
            "evaluation_time": datetime.now().isoformat()
        }

class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "evaluation_output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_comprehensive_report(
        self, 
        merge_info: Dict[str, Any],
        evaluation_results: Dict[str, Any],
        qa_data: List[Dict[str, Any]]
    ) -> str:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        
        report_data = {
            "report_info": {
                "generated_at": datetime.now().isoformat(),
                "report_type": "Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æŠ¥å‘Š",
                "version": "1.0.0"
            },
            "merge_summary": merge_info,
            "evaluation_summary": evaluation_results["summary"],
            "dimension_performance": evaluation_results["dimension_averages"],
            "detailed_results": evaluation_results["individual_results"],
            "data_statistics": {
                "total_qa_items": len(qa_data),
                "data_sources": list(set(item.get("context", "æœªçŸ¥") for item in qa_data)),
                "difficulty_distribution": self._analyze_difficulty_distribution(qa_data),
                "domain_distribution": self._analyze_domain_distribution(qa_data)
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_path = self.output_dir / "comprehensive_evaluation_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_path = self.output_dir / "comprehensive_evaluation_report.html"
        html_content = self._generate_html_report(report_data)
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"ğŸ“‹ ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
        logger.info(f"   JSON: {json_path}")
        logger.info(f"   HTML: {html_path}")
        
        return str(html_path)
    
    def _analyze_difficulty_distribution(self, qa_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†æéš¾åº¦åˆ†å¸ƒ"""
        distribution = {}
        for item in qa_data:
            difficulty = item.get("difficulty_level", "unknown")
            distribution[difficulty] = distribution.get(difficulty, 0) + 1
        return distribution
    
    def _analyze_domain_distribution(self, qa_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†æé¢†åŸŸåˆ†å¸ƒ"""
        distribution = {}
        for item in qa_data:
            tags = item.get("domain_tags", [])
            for tag in tags:
                distribution[tag] = distribution.get(tag, 0) + 1
        return distribution
    
    def _generate_html_report(self, report_data: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; text-align: center; }}
        .section {{ margin: 30px 0; padding: 20px; border: 1px solid #e0e0e0; border-radius: 8px; }}
        .metric {{ background: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #007bff; }}
        .score-excellent {{ color: #28a745; font-weight: bold; }}
        .score-good {{ color: #17a2b8; font-weight: bold; }}
        .score-fair {{ color: #ffc107; font-weight: bold; }}
        .score-poor {{ color: #dc3545; font-weight: bold; }}
        table {{ width: 100%; border-collapse: collapse; margin: 15px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f2f2f2; font-weight: bold; }}
        .progress-bar {{ width: 100%; height: 20px; background-color: #e9ecef; border-radius: 10px; overflow: hidden; }}
        .progress-fill {{ height: 100%; background: linear-gradient(90deg, #28a745, #20c997); transition: width 0.3s ease; }}
        .chart-container {{ margin: 20px 0; }}
        .recommendation {{ background: #d4edda; border: 1px solid #c3e6cb; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .warning {{ background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; margin: 10px 0; border-radius: 5px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¯ Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æŠ¥å‘Š</h1>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {report_data['report_info']['generated_at']}</p>
        <p><strong>æŠ¥å‘Šç‰ˆæœ¬:</strong> {report_data['report_info']['version']}</p>
    </div>
    
    <div class="section">
        <h2>ğŸ”„ æ¨¡å‹åˆå¹¶ä¿¡æ¯</h2>
        <div class="metric">åŸºåº§æ¨¡å‹: {report_data['merge_summary'].get('base_model', 'N/A')}</div>
        <div class="metric">Checkpointè·¯å¾„: {report_data['merge_summary'].get('checkpoint_path', 'N/A')}</div>
        <div class="metric">åˆå¹¶æ—¶é—´: {report_data['merge_summary'].get('merge_time', 'N/A')}</div>
        <div class="metric">ä½¿ç”¨è®¾å¤‡: {report_data['merge_summary'].get('device_used', 'N/A')}</div>
        <div class="metric">æ¨¡å‹ç²¾åº¦: {report_data['merge_summary'].get('model_dtype', 'N/A')}</div>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š è¯„ä¼°æ¦‚è¦</h2>
        <div class="metric">æ€»è¯„ä¼°é¡¹ç›®: {report_data['evaluation_summary']['total_evaluations']}</div>
        <div class="metric">å¹³å‡å¾—åˆ†: <span class="score-{self._get_score_class(report_data['evaluation_summary']['average_score'])}">{report_data['evaluation_summary']['average_score']}</span></div>
        <div class="metric">æœ€é«˜å¾—åˆ†: <span class="score-{self._get_score_class(report_data['evaluation_summary']['max_score'])}">{report_data['evaluation_summary']['max_score']}</span></div>
        <div class="metric">æœ€ä½å¾—åˆ†: <span class="score-{self._get_score_class(report_data['evaluation_summary']['min_score'])}">{report_data['evaluation_summary']['min_score']}</span></div>
        <div class="metric">å¾—åˆ†æ ‡å‡†å·®: {report_data['evaluation_summary']['score_std']}</div>
    </div>
    
    <div class="section">
        <h2>ğŸ“ˆ ç»´åº¦è¡¨ç°åˆ†æ</h2>
        <table>
            <tr><th>è¯„ä¼°ç»´åº¦</th><th>å¹³å‡å¾—åˆ†</th><th>è¡¨ç°ç­‰çº§</th><th>å¾—åˆ†å¯è§†åŒ–</th></tr>
        """
        
        for dim, score in report_data['dimension_performance'].items():
            score_class = self._get_score_class(score)
            grade = self._get_grade_text(score)
            progress_width = int(score * 100)
            
            html += f"""
            <tr>
                <td>{self._translate_dimension(dim)}</td>
                <td class="{score_class}">{score}</td>
                <td class="{score_class}">{grade}</td>
                <td>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {progress_width}%"></div>
                    </div>
                </td>
            </tr>
            """
        
        html += f"""
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š æ•°æ®ç»Ÿè®¡</h2>
        <div class="metric">æ•°æ®æ¥æº: {', '.join(report_data['data_statistics']['data_sources'])}</div>
        
        <h3>éš¾åº¦åˆ†å¸ƒ</h3>
        <table>
            <tr><th>éš¾åº¦çº§åˆ«</th><th>é¢˜ç›®æ•°é‡</th></tr>
        """
        
        for difficulty, count in report_data['data_statistics']['difficulty_distribution'].items():
            html += f"<tr><td>{difficulty}</td><td>{count}</td></tr>"
        
        html += f"""
        </table>
        
        <h3>é¢†åŸŸåˆ†å¸ƒ</h3>
        <table>
            <tr><th>é¢†åŸŸæ ‡ç­¾</th><th>å‡ºç°æ¬¡æ•°</th></tr>
        """
        
        for domain, count in report_data['data_statistics']['domain_distribution'].items():
            html += f"<tr><td>{domain}</td><td>{count}</td></tr>"
        
        # æ·»åŠ æ”¹è¿›å»ºè®®
        html += """
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ’¡ æ”¹è¿›å»ºè®®</h2>
        """
        
        avg_score = report_data['evaluation_summary']['average_score']
        if avg_score < 0.7:
            html += '<div class="warning">âš ï¸ æ•´ä½“è¡¨ç°éœ€è¦æ”¹è¿›ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´æ¨¡å‹å‚æ•°</div>'
        elif avg_score < 0.8:
            html += '<div class="recommendation">ğŸ“ˆ è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥é€šè¿‡é’ˆå¯¹æ€§ä¼˜åŒ–è¿›ä¸€æ­¥æå‡</div>'
        else:
            html += '<div class="recommendation">ğŸ‰ è¡¨ç°ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒå½“å‰æ°´å¹³</div>'
        
        # åŸºäºç»´åº¦è¡¨ç°ç»™å‡ºå…·ä½“å»ºè®®
        poor_dimensions = [dim for dim, score in report_data['dimension_performance'].items() if score < 0.7]
        if poor_dimensions:
            html += f'<div class="recommendation">ğŸ¯ é‡ç‚¹æ”¹è¿›ç»´åº¦: {", ".join([self._translate_dimension(d) for d in poor_dimensions])}</div>'
        
        html += """
    </div>
    
    <div class="section">
        <h2>ğŸ” è¯¦ç»†ç»“æœ</h2>
        <p>è¯¦ç»†çš„å•é¡¹è¯„ä¼°ç»“æœè¯·æŸ¥çœ‹JSONæŠ¥å‘Šæ–‡ä»¶ã€‚</p>
    </div>
    
    <div class="section">
        <p><em>æœ¬æŠ¥å‘Šç”±ä¸“å®¶è¯„ä¼°ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ - {}</em></p>
    </div>
</body>
</html>
        """.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        
        return html
    
    def _get_score_class(self, score: float) -> str:
        """è·å–åˆ†æ•°å¯¹åº”çš„CSSç±»"""
        if score >= 0.9:
            return "excellent"
        elif score >= 0.8:
            return "good"
        elif score >= 0.7:
            return "fair"
        else:
            return "poor"
    
    def _get_grade_text(self, score: float) -> str:
        """è·å–åˆ†æ•°å¯¹åº”çš„ç­‰çº§æ–‡æœ¬"""
        if score >= 0.9:
            return "ä¼˜ç§€"
        elif score >= 0.8:
            return "è‰¯å¥½"
        elif score >= 0.7:
            return "ä¸€èˆ¬"
        else:
            return "å¾…æ”¹è¿›"
    
    def _translate_dimension(self, dimension: str) -> str:
        """ç¿»è¯‘ç»´åº¦åç§°"""
        translations = {
            "semantic_similarity": "è¯­ä¹‰ç›¸ä¼¼æ€§",
            "domain_accuracy": "é¢†åŸŸå‡†ç¡®æ€§",
            "response_relevance": "å“åº”ç›¸å…³æ€§",
            "factual_correctness": "äº‹å®æ­£ç¡®æ€§",
            "completeness": "å®Œæ•´æ€§",
            "clarity": "æ¸…æ™°åº¦",
            "technical_depth": "æŠ€æœ¯æ·±åº¦"
        }
        return translations.get(dimension, dimension)

class ComprehensiveDemo:
    """ç»¼åˆæ¼”ç¤ºç±»"""
    
    def __init__(self, output_dir: str = "comprehensive_demo_output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.merger = CheckpointMerger()
        self.qa_processor = QADataProcessor()
        self.evaluator = ExpertEvaluator()
        self.report_generator = ReportGenerator(str(self.output_dir))
        
        logger.info(f"ğŸš€ ç»¼åˆæ¼”ç¤ºåˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def run_complete_pipeline(
        self,
        checkpoint_path: str = "qwen3_4b_thinking_output/final_model",
        base_model_path: str = "Qwen/Qwen3-4B-Thinking-2507",
        qa_data_dir: str = "data/raw"
    ):
        """è¿è¡Œå®Œæ•´çš„pipeline"""
        
        try:
            logger.info("ğŸ¯ å¼€å§‹æ‰§è¡Œå®Œæ•´çš„Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æµç¨‹")
            
            # æ­¥éª¤1: åˆå¹¶æ¨¡å‹
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤1: åˆå¹¶LoRA Checkpointåˆ°åŸºåº§æ¨¡å‹")
            logger.info("="*60)
            
            merged_model_path = self.output_dir / "merged_model"
            
            if not TRANSFORMERS_AVAILABLE:
                logger.warning("âš ï¸ Transformersä¸å¯ç”¨ï¼Œè·³è¿‡æ¨¡å‹åˆå¹¶æ­¥éª¤")
                merge_info = {
                    "merge_time": datetime.now().isoformat(),
                    "base_model": base_model_path,
                    "checkpoint_path": checkpoint_path,
                    "output_path": str(merged_model_path),
                    "success": False,
                    "error": "Transformersåº“ä¸å¯ç”¨"
                }
                model = None
                tokenizer = None
            else:
                model, tokenizer = self.merger.merge_lora_checkpoint(
                    checkpoint_path=checkpoint_path,
                    base_model_path=base_model_path,
                    output_path=str(merged_model_path)
                )
                
                # è¯»å–åˆå¹¶ä¿¡æ¯
                with open(merged_model_path / "merge_info.json", 'r', encoding='utf-8') as f:
                    merge_info = json.load(f)
            
            # æ­¥éª¤2: åŠ è½½QAæ•°æ®
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤2: åŠ è½½è¯„ä¼°æ•°æ®")
            logger.info("="*60)
            
            qa_data = self.qa_processor.load_all_qa_data(qa_data_dir)
            
            if not qa_data:
                logger.error("âŒ æ²¡æœ‰åŠ è½½åˆ°QAæ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
                return
            
            # æ­¥éª¤3: ç”Ÿæˆæ¨¡å‹ç­”æ¡ˆ
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤3: ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ")
            logger.info("="*60)
            
            model_answers = []
            
            if model and tokenizer:
                model_evaluator = ModelEvaluator(model, tokenizer, self.merger.device)
                
                for i, qa_item in enumerate(qa_data):
                    logger.info(f"ğŸ¤– ç”Ÿæˆç­”æ¡ˆ {i+1}/{len(qa_data)}: {qa_item['question_id']}")
                    
                    try:
                        answer = model_evaluator.generate_answer(
                            question=qa_item["question"],
                            context=qa_item.get("context", ""),
                            max_length=256
                        )
                        model_answers.append(answer)
                        
                        # æ˜¾ç¤ºç”Ÿæˆçš„ç­”æ¡ˆï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
                        display_answer = answer[:100] + "..." if len(answer) > 100 else answer
                        logger.info(f"   ç”Ÿæˆç­”æ¡ˆ: {display_answer}")
                        
                    except Exception as e:
                        logger.error(f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
                        model_answers.append("ç”Ÿæˆå¤±è´¥")
            else:
                logger.warning("âš ï¸ æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç­”æ¡ˆ")
                # ä½¿ç”¨æ¨¡æ‹Ÿç­”æ¡ˆè¿›è¡Œæ¼”ç¤º
                for qa_item in qa_data:
                    simulated_answer = f"è¿™æ˜¯å¯¹é—®é¢˜'{qa_item['question'][:20]}...'çš„æ¨¡æ‹Ÿå›ç­”ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ˜¯åˆå¹¶åæ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆã€‚"
                    model_answers.append(simulated_answer)
            
            # æ­¥éª¤4: ä¸“å®¶è¯„ä¼°
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤4: æ‰§è¡Œä¸“å®¶è¯„ä¼°")
            logger.info("="*60)
            
            evaluation_results = self.evaluator.evaluate_batch(qa_data, model_answers)
            
            logger.info(f"ğŸ“Š è¯„ä¼°å®Œæˆ:")
            logger.info(f"   æ€»é¡¹ç›®æ•°: {evaluation_results['summary']['total_evaluations']}")
            logger.info(f"   å¹³å‡å¾—åˆ†: {evaluation_results['summary']['average_score']}")
            logger.info(f"   æœ€é«˜å¾—åˆ†: {evaluation_results['summary']['max_score']}")
            logger.info(f"   æœ€ä½å¾—åˆ†: {evaluation_results['summary']['min_score']}")
            
            # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤5: ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š")
            logger.info("="*60)
            
            report_path = self.report_generator.generate_comprehensive_report(
                merge_info=merge_info,
                evaluation_results=evaluation_results,
                qa_data=qa_data
            )
            
            # æ­¥éª¤6: æ€»ç»“
            logger.info("\n" + "="*60)
            logger.info("ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆ")
            logger.info("="*60)
            
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            logger.info(f"ğŸ“‹ HTMLæŠ¥å‘Š: {report_path}")
            logger.info(f"ğŸ“Š å¹³å‡è¯„ä¼°å¾—åˆ†: {evaluation_results['summary']['average_score']}")
            
            # æ˜¾ç¤ºæœ€ä½³å’Œæœ€å·®è¡¨ç°çš„ç»´åº¦
            dim_scores = evaluation_results['dimension_averages']
            best_dim = max(dim_scores.items(), key=lambda x: x[1])
            worst_dim = min(dim_scores.items(), key=lambda x: x[1])
            
            logger.info(f"ğŸ† æœ€ä½³ç»´åº¦: {best_dim[0]} ({best_dim[1]:.3f})")
            logger.info(f"ğŸ“‰ å¾…æ”¹è¿›ç»´åº¦: {worst_dim[0]} ({worst_dim[1]:.3f})")
            
            # ç”Ÿæˆç®€è¦å»ºè®®
            avg_score = evaluation_results['summary']['average_score']
            if avg_score >= 0.8:
                logger.info("âœ¨ æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼Œå¯ä»¥è€ƒè™‘éƒ¨ç½²ä½¿ç”¨")
            elif avg_score >= 0.7:
                logger.info("ğŸ“ˆ æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
            else:
                logger.info("âš ï¸ æ¨¡å‹è¡¨ç°éœ€è¦æ”¹è¿›ï¼Œå»ºè®®é‡æ–°è®­ç»ƒ")
            
            logger.info("\nğŸ¯ æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šè¯·æ‰“å¼€ç”Ÿæˆçš„HTMLæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°å®Œæ•´æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥å¿…è¦çš„è·¯å¾„
    checkpoint_path = "qwen3_4b_thinking_output/final_model"
    qa_data_dir = "data/raw"
    
    if not Path(checkpoint_path).exists():
        logger.error(f"âŒ Checkpointè·¯å¾„ä¸å­˜åœ¨: {checkpoint_path}")
        return
    
    if not Path(qa_data_dir).exists():
        logger.error(f"âŒ QAæ•°æ®ç›®å½•ä¸å­˜åœ¨: {qa_data_dir}")
        return
    
    try:
        # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
        demo = ComprehensiveDemo()
        demo.run_complete_pipeline(
            checkpoint_path=checkpoint_path,
            qa_data_dir=qa_data_dir
        )
        
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()