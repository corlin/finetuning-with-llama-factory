#!/usr/bin/env python3
"""
å…¨é¢åŠŸèƒ½éªŒè¯è„šæœ¬ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

æœ¬è„šæœ¬ä½¿ç”¨ data/raw ä½œä¸ºæºæ•°æ®ï¼Œæ‰§è¡Œç›¸å…³å·²å®ç°çš„åŠŸèƒ½åŸºäºLlamaFactoryè¿›è¡Œå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒçš„å…¨é¢éªŒè¯ã€‚

éªŒè¯å†…å®¹åŒ…æ‹¬ï¼š
1. ç¯å¢ƒå’ŒGPUæ£€æµ‹
2. æ•°æ®å¤„ç†å’Œè½¬æ¢
3. æ•°æ®é›†åˆ†å‰²
4. å¹¶è¡Œé…ç½®ä¼˜åŒ–
5. è®­ç»ƒæµæ°´çº¿æ‰§è¡Œ
6. ç›‘æ§å’Œè¯„ä¼°
7. å®Œæ•´çš„ç«¯åˆ°ç«¯å·¥ä½œæµéªŒè¯
"""

import os
import sys
import json
import yaml
import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
    from rich.panel import Panel
    from rich.text import Text
    from rich import box
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("Richåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºæœ¬è¾“å‡ºæ ¼å¼")

# å¦‚æœRichä¸å¯ç”¨ï¼Œåˆ›å»ºç®€å•çš„æ›¿ä»£ç±»
if not RICH_AVAILABLE:
    class Console:
        def print(self, *args, **kwargs):
            print(*args)
    
    class Table:
        def __init__(self, title=""):
            self.title = title
            self.rows = []
        def add_column(self, *args, **kwargs):
            pass
        def add_row(self, *args):
            self.rows.append(args)

console = Console() if RICH_AVAILABLE else Console()

class ComprehensiveValidatorSync:
    """å…¨é¢åŠŸèƒ½éªŒè¯å™¨ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, raw_data_dir: str = "data/raw", output_dir: str = "validation_output"):
        self.raw_data_dir = Path(raw_data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.setup_logging()
        
        # éªŒè¯ç»“æœå­˜å‚¨
        self.validation_results = {}
        self.start_time = datetime.now()
        
        console.print(f"ğŸš€ å¼€å§‹å…¨é¢åŠŸèƒ½éªŒè¯")
        console.print(f"æºæ•°æ®ç›®å½•: {self.raw_data_dir}")
        console.print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.output_dir / f"validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢éªŒè¯"""
        try:
            console.print("\nğŸ“‹ éªŒè¯è®¡åˆ’")
            validation_steps = [
                ("ç¯å¢ƒå’ŒGPUæ£€æµ‹", self.validate_environment_and_gpu),
                ("æ•°æ®å¤„ç†å’Œè½¬æ¢", self.validate_data_processing),
                ("æ•°æ®é›†åˆ†å‰²", self.validate_dataset_splitting),
                ("å¹¶è¡Œé…ç½®ä¼˜åŒ–", self.validate_parallel_configuration),
                ("å†…å­˜ç®¡ç†", self.validate_memory_management),
                ("è®­ç»ƒé…ç½®ç”Ÿæˆ", self.validate_training_configuration),
                ("LlamaFactoryé›†æˆ", self.validate_llamafactory_integration),
                ("ç›‘æ§å’Œè¯„ä¼°æ¡†æ¶", self.validate_monitoring_evaluation),
                ("ç«¯åˆ°ç«¯æµæ°´çº¿", self.validate_end_to_end_pipeline)
            ]
            
            for step_name, step_func in validation_steps:
                console.print(f"  âœ“ {step_name}")
            
            console.print(f"\nâ±ï¸  é¢„è®¡éªŒè¯æ—¶é—´: 15-30åˆ†é’Ÿ")
            
            # æ‰§è¡ŒéªŒè¯æ­¥éª¤
            for i, (step_name, step_func) in enumerate(validation_steps, 1):
                console.print(f"\næ­¥éª¤ {i}/{len(validation_steps)}: {step_name}")
                
                try:
                    result = step_func()
                    self.validation_results[step_name] = {
                        "status": "success",
                        "result": result,
                        "timestamp": datetime.now().isoformat()
                    }
                    console.print(f"âœ… {step_name} - æˆåŠŸ")
                    
                except Exception as e:
                    self.validation_results[step_name] = {
                        "status": "failed",
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    }
                    console.print(f"âŒ {step_name} - å¤±è´¥: {e}")
                    self.logger.error(f"éªŒè¯æ­¥éª¤å¤±è´¥ {step_name}: {e}", exc_info=True)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = self.generate_final_report()
            
            return final_report
            
        except Exception as e:
            console.print(f"ğŸ’¥ éªŒè¯è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            self.logger.error(f"éªŒè¯è¿‡ç¨‹å¤±è´¥: {e}", exc_info=True)
            raise
    
    def validate_environment_and_gpu(self) -> Dict[str, Any]:
        """éªŒè¯ç¯å¢ƒå’ŒGPUé…ç½®"""
        console.print("  ğŸ” æ£€æµ‹ç³»ç»Ÿç¯å¢ƒ...")
        
        result = {
            "python_version": sys.version,
            "platform": sys.platform,
            "gpu_info": [],
            "cuda_available": False,
            "torch_available": False
        }
        
        # æ£€æŸ¥PyTorchå’ŒCUDA
        try:
            import torch
            result["torch_available"] = True
            result["torch_version"] = torch.__version__
            result["cuda_available"] = torch.cuda.is_available()
            
            if torch.cuda.is_available():
                result["cuda_version"] = torch.version.cuda
                result["gpu_count"] = torch.cuda.device_count()
                
                # è·å–GPUä¿¡æ¯
                for i in range(torch.cuda.device_count()):
                    gpu_props = torch.cuda.get_device_properties(i)
                    gpu_info = {
                        "id": i,
                        "name": gpu_props.name,
                        "memory_total": gpu_props.total_memory / 1024**3,  # GB
                        "compute_capability": f"{gpu_props.major}.{gpu_props.minor}"
                    }
                    result["gpu_info"].append(gpu_info)
                    
        except ImportError:
            result["torch_error"] = "PyTorchæœªå®‰è£…"
        
        # æ£€æŸ¥å…¶ä»–ä¾èµ–
        dependencies = ["transformers", "datasets", "peft", "accelerate"]
        result["dependencies"] = {}
        
        for dep in dependencies:
            try:
                __import__(dep)
                result["dependencies"][dep] = "å·²å®‰è£…"
            except ImportError:
                result["dependencies"][dep] = "æœªå®‰è£…"
        
        # æ˜¾ç¤ºGPUä¿¡æ¯
        if result["gpu_info"] and RICH_AVAILABLE:
            table = Table(title="GPUä¿¡æ¯")
            table.add_column("GPU ID", style="cyan")
            table.add_column("åç§°", style="green")
            table.add_column("å†…å­˜", style="yellow")
            table.add_column("è®¡ç®—èƒ½åŠ›", style="blue")
            
            for gpu in result["gpu_info"]:
                table.add_row(
                    str(gpu["id"]),
                    gpu["name"],
                    f"{gpu['memory_total']:.1f}GB",
                    gpu["compute_capability"]
                )
            
            console.print(table)
        elif result["gpu_info"]:
            console.print("GPUä¿¡æ¯:")
            for gpu in result["gpu_info"]:
                console.print(f"  GPU {gpu['id']}: {gpu['name']} ({gpu['memory_total']:.1f}GB)")
        
        return result
    
    def validate_data_processing(self) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®å¤„ç†å’Œè½¬æ¢"""
        console.print("  ğŸ“„ å¤„ç†åŸå§‹æ•°æ®...")
        
        # ç®€å•çš„æ•°æ®å¤„ç†éªŒè¯
        processed_data = []
        file_stats = {}
        
        for md_file in self.raw_data_dir.glob("*.md"):
            console.print(f"    å¤„ç†æ–‡ä»¶: {md_file.name}")
            
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ç®€å•è§£æï¼šæŸ¥æ‰¾Q&Aæ¨¡å¼
                lines = content.split('\n')
                questions = []
                current_q = None
                current_a = None
                current_thinking = None
                
                for line in lines:
                    line = line.strip()
                    if line.startswith('### Q') or line.startswith('##') and 'Q' in line:
                        if current_q and current_a:
                            questions.append({
                                "instruction": current_q,
                                "output": current_a,
                                "thinking": current_thinking,
                                "source_file": md_file.name
                            })
                        current_q = line
                        current_a = None
                        current_thinking = None
                    elif line.startswith('<thinking>'):
                        current_thinking = ""
                        in_thinking = True
                    elif line.startswith('</thinking>'):
                        in_thinking = False
                    elif line.startswith('A') and ':' in line and current_q:
                        current_a = line
                    elif current_thinking is not None and 'in_thinking' in locals() and in_thinking:
                        current_thinking += line + "\n"
                    elif current_a and line and not line.startswith('#'):
                        current_a += " " + line
                
                # æ·»åŠ æœ€åä¸€ä¸ªé—®é¢˜
                if current_q and current_a:
                    questions.append({
                        "instruction": current_q,
                        "output": current_a,
                        "thinking": current_thinking,
                        "source_file": md_file.name
                    })
                
                processed_data.extend(questions)
                file_stats[md_file.name] = {
                    "examples_count": len(questions),
                    "has_thinking": sum(1 for q in questions if q.get('thinking')),
                    "avg_length": sum(len(q['instruction'] + q['output']) for q in questions) / len(questions) if questions else 0
                }
                
            except Exception as e:
                console.print(f"    å¤„ç†æ–‡ä»¶å¤±è´¥ {md_file.name}: {e}")
                file_stats[md_file.name] = {"error": str(e)}
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        processed_file = self.output_dir / "processed_data.json"
        with open(processed_file, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
        
        result = {
            "total_examples": len(processed_data),
            "file_stats": file_stats,
            "processed_file": str(processed_file),
            "thinking_examples": sum(1 for ex in processed_data if ex.get('thinking')),
        }
        
        # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
        if RICH_AVAILABLE:
            table = Table(title="æ•°æ®å¤„ç†ç»Ÿè®¡")
            table.add_column("æ–‡ä»¶", style="cyan")
            table.add_column("æ ·ä¾‹æ•°", style="green")
            table.add_column("æ€è€ƒæ•°æ®", style="yellow")
            table.add_column("å¹³å‡é•¿åº¦", style="blue")
            
            for filename, stats in file_stats.items():
                if "error" not in stats:
                    table.add_row(
                        filename,
                        str(stats["examples_count"]),
                        str(stats["has_thinking"]),
                        f"{stats['avg_length']:.0f}"
                    )
            
            console.print(table)
        else:
            console.print("æ•°æ®å¤„ç†ç»Ÿè®¡:")
            for filename, stats in file_stats.items():
                if "error" not in stats:
                    console.print(f"  {filename}: {stats['examples_count']}ä¸ªæ ·ä¾‹, {stats['has_thinking']}ä¸ªæ€è€ƒæ•°æ®")
        
        return result
    
    def validate_dataset_splitting(self) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®é›†åˆ†å‰²"""
        console.print("  âœ‚ï¸  åˆ†å‰²æ•°æ®é›†...")
        
        # åŠ è½½å¤„ç†åçš„æ•°æ®
        processed_file = self.output_dir / "processed_data.json"
        if not processed_file.exists():
            raise FileNotFoundError("æœªæ‰¾åˆ°å¤„ç†åçš„æ•°æ®æ–‡ä»¶")
        
        with open(processed_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not data:
            raise ValueError("å¤„ç†åçš„æ•°æ®ä¸ºç©º")
        
        # ç®€å•åˆ†å‰²
        import random
        random.seed(42)
        random.shuffle(data)
        
        total = len(data)
        train_size = int(total * 0.7)
        val_size = int(total * 0.15)
        
        splits = {
            "train": data[:train_size],
            "val": data[train_size:train_size + val_size],
            "test": data[train_size + val_size:]
        }
        
        # ä¿å­˜åˆ†å‰²ç»“æœ
        for split_name, split_data in splits.items():
            split_file = self.output_dir / f"{split_name}.json"
            with open(split_file, 'w', encoding='utf-8') as f:
                json.dump(split_data, f, ensure_ascii=False, indent=2)
        
        result = {
            "splits": {name: len(data) for name, data in splits.items()},
            "split_files": [str(self.output_dir / f"{name}.json") for name in splits.keys()]
        }
        
        # æ˜¾ç¤ºåˆ†å‰²ç»Ÿè®¡
        if RICH_AVAILABLE:
            table = Table(title="æ•°æ®é›†åˆ†å‰²")
            table.add_column("åˆ†å‰²", style="cyan")
            table.add_column("æ ·ä¾‹æ•°", style="green")
            table.add_column("æ¯”ä¾‹", style="yellow")
            
            for name, data in splits.items():
                table.add_row(
                    name,
                    str(len(data)),
                    f"{len(data)/total*100:.1f}%"
                )
            
            console.print(table)
        else:
            console.print("æ•°æ®é›†åˆ†å‰²:")
            for name, data in splits.items():
                console.print(f"  {name}: {len(data)}ä¸ªæ ·ä¾‹ ({len(data)/total*100:.1f}%)")
        
        return result
    
    def validate_parallel_configuration(self) -> Dict[str, Any]:
        """éªŒè¯å¹¶è¡Œé…ç½®ä¼˜åŒ–"""
        console.print("  âš¡ é…ç½®å¹¶è¡Œè®­ç»ƒç­–ç•¥...")
        
        result = {
            "gpu_count": 0,
            "parallel_strategy": "single_gpu",
            "lora_config": {
                "rank": 8,
                "alpha": 16,
                "dropout": 0.1,
                "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"]
            }
        }
        
        # æ£€æŸ¥GPUæ•°é‡
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                result["gpu_count"] = gpu_count
                
                if gpu_count > 1:
                    result["parallel_strategy"] = "data_parallel"
                    result["distributed_config"] = {
                        "backend": "nccl",
                        "world_size": gpu_count,
                        "enable_ddp": True
                    }
                
                # æ ¹æ®GPUå†…å­˜è°ƒæ•´LoRAé…ç½®
                if gpu_count > 0:
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    if gpu_memory < 12:  # å°äº12GB
                        result["lora_config"]["rank"] = 4
                        result["lora_config"]["alpha"] = 8
                    elif gpu_memory > 24:  # å¤§äº24GB
                        result["lora_config"]["rank"] = 16
                        result["lora_config"]["alpha"] = 32
                        
        except ImportError:
            result["error"] = "PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPU"
        
        # ä¿å­˜é…ç½®
        config_file = self.output_dir / "parallel_config.yaml"
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(result, f, default_flow_style=False, allow_unicode=True)
        
        console.print(f"  ğŸ“ å¹¶è¡Œé…ç½®å·²ä¿å­˜åˆ°: {config_file}")
        
        return result
    
    def validate_memory_management(self) -> Dict[str, Any]:
        """éªŒè¯å†…å­˜ç®¡ç†"""
        console.print("  ğŸ§  éªŒè¯å†…å­˜ç®¡ç†...")
        
        result = {
            "system_memory": {},
            "gpu_memory": {},
            "optimization_suggestions": []
        }
        
        try:
            import psutil
            
            # ç³»ç»Ÿå†…å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            result["system_memory"] = {
                "total": memory.total / 1024**3,  # GB
                "available": memory.available / 1024**3,
                "percent": memory.percent
            }
            
        except ImportError:
            result["system_memory"]["error"] = "psutilæœªå®‰è£…"
        
        try:
            import torch
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    memory_info = torch.cuda.mem_get_info(i)
                    result["gpu_memory"][f"gpu_{i}"] = {
                        "free": memory_info[0] / 1024**3,
                        "total": memory_info[1] / 1024**3,
                        "used": (memory_info[1] - memory_info[0]) / 1024**3
                    }
                    
                    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
                    if memory_info[0] / memory_info[1] < 0.8:  # å¯ç”¨å†…å­˜å°‘äº80%
                        result["optimization_suggestions"].append(f"GPU {i}: å»ºè®®å‡å°‘batch_sizeæˆ–å¯ç”¨æ¢¯åº¦ç´¯ç§¯")
                        
        except ImportError:
            result["gpu_memory"]["error"] = "PyTorchæœªå®‰è£…"
        
        if not result["optimization_suggestions"]:
            result["optimization_suggestions"].append("å†…å­˜ä½¿ç”¨æ­£å¸¸")
        
        return result
    
    def validate_training_configuration(self) -> Dict[str, Any]:
        """éªŒè¯è®­ç»ƒé…ç½®ç”Ÿæˆ"""
        console.print("  âš™ï¸  ç”Ÿæˆè®­ç»ƒé…ç½®...")
        
        # åŠ è½½å¹¶è¡Œé…ç½®
        config_file = self.output_dir / "parallel_config.yaml"
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                parallel_config = yaml.safe_load(f)
        else:
            parallel_config = {}
        
        # ç”ŸæˆLlamaFactoryå…¼å®¹çš„è®­ç»ƒé…ç½®
        training_config = {
            "model_name_or_path": "Qwen/Qwen3-4B-Thinking-2507",
            "stage": "sft",
            "do_train": True,
            "finetuning_type": "lora",
            "lora_target": "all",
            "dataset": "custom_dataset",
            "template": "qwen",
            "cutoff_len": 2048,
            "max_samples": 100,  # éªŒè¯ç”¨ï¼Œè®¾ç½®è¾ƒå°å€¼
            "overwrite_cache": True,
            "preprocessing_num_workers": 4,
            "output_dir": str(self.output_dir / "training_output"),
            "logging_steps": 5,
            "save_steps": 50,
            "plot_loss": True,
            "overwrite_output_dir": True,
            "per_device_train_batch_size": 1,
            "gradient_accumulation_steps": 4,
            "learning_rate": 2e-4,
            "num_train_epochs": 1,  # éªŒè¯ç”¨ï¼Œè®¾ç½®è¾ƒå°å€¼
            "lr_scheduler_type": "cosine",
            "warmup_ratio": 0.1,
            "bf16": True,
            "ddp_timeout": 180000000,
            "include_num_input_tokens_seen": True
        }
        
        # æ·»åŠ LoRAé…ç½®
        lora_config = parallel_config.get("lora_config", {})
        training_config.update({
            "lora_rank": lora_config.get("rank", 8),
            "lora_alpha": lora_config.get("alpha", 16),
            "lora_dropout": lora_config.get("dropout", 0.1)
        })
        
        # æ·»åŠ åˆ†å¸ƒå¼é…ç½®
        if parallel_config.get("gpu_count", 0) > 1:
            training_config.update({
                "ddp_find_unused_parameters": False,
                "dataloader_pin_memory": False
            })
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        train_config_file = self.output_dir / "training_config.yaml"
        with open(train_config_file, 'w', encoding='utf-8') as f:
            yaml.dump(training_config, f, default_flow_style=False, allow_unicode=True)
        
        result = {
            "training_config": training_config,
            "config_file": str(train_config_file)
        }
        
        console.print(f"  ğŸ“ è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {train_config_file}")
        
        return result
    
    def validate_llamafactory_integration(self) -> Dict[str, Any]:
        """éªŒè¯LlamaFactoryé›†æˆ"""
        console.print("  ğŸ¦™ éªŒè¯LlamaFactoryé›†æˆ...")
        
        result = {
            "llamafactory_available": False,
            "dataset_config": {},
            "compatibility_check": {}
        }
        
        try:
            # æ£€æŸ¥LlamaFactoryæ˜¯å¦å¯ç”¨
            try:
                import llamafactory
                result["llamafactory_available"] = True
                result["llamafactory_version"] = getattr(llamafactory, '__version__', 'unknown')
            except ImportError:
                result["llamafactory_available"] = False
                result["error"] = "LlamaFactoryæœªå®‰è£…"
            
            # å‡†å¤‡æ•°æ®é›†é…ç½®
            dataset_config = {
                "custom_dataset": {
                    "file_name": str(self.output_dir / "train.json"),
                    "formatting": "alpaca",
                    "columns": {
                        "prompt": "instruction",
                        "query": "input", 
                        "response": "output"
                    }
                }
            }
            
            result["dataset_config"] = dataset_config
            
            # ä¿å­˜æ•°æ®é›†é…ç½®
            dataset_info_file = self.output_dir / "dataset_info.json"
            with open(dataset_info_file, 'w', encoding='utf-8') as f:
                json.dump(dataset_config, f, ensure_ascii=False, indent=2)
            
            result["dataset_info_file"] = str(dataset_info_file)
            
            # åŸºæœ¬å…¼å®¹æ€§æ£€æŸ¥
            train_file = self.output_dir / "train.json"
            config_file = self.output_dir / "training_config.yaml"
            
            result["compatibility_check"] = {
                "train_data_exists": train_file.exists(),
                "config_exists": config_file.exists(),
                "dataset_info_exists": dataset_info_file.exists()
            }
            
        except Exception as e:
            result["error"] = str(e)
        
        return result
    
    def validate_monitoring_evaluation(self) -> Dict[str, Any]:
        """éªŒè¯ç›‘æ§å’Œè¯„ä¼°æ¡†æ¶"""
        console.print("  ğŸ“Š éªŒè¯ç›‘æ§å’Œè¯„ä¼°æ¡†æ¶...")
        
        result = {
            "monitoring_available": False,
            "evaluation_available": False,
            "test_results": {}
        }
        
        try:
            # æ£€æŸ¥ç›‘æ§ç›¸å…³ä¾èµ–
            monitoring_deps = ["matplotlib", "seaborn", "pandas"]
            result["monitoring_dependencies"] = {}
            
            for dep in monitoring_deps:
                try:
                    __import__(dep)
                    result["monitoring_dependencies"][dep] = "å·²å®‰è£…"
                except ImportError:
                    result["monitoring_dependencies"][dep] = "æœªå®‰è£…"
            
            result["monitoring_available"] = all(
                status == "å·²å®‰è£…" for status in result["monitoring_dependencies"].values()
            )
            
            # ç®€å•çš„è¯„ä¼°æµ‹è¯•
            test_question = "ä»€ä¹ˆæ˜¯å¯¹ç§°åŠ å¯†ï¼Ÿ"
            test_answer = "å¯¹ç§°åŠ å¯†æ˜¯ä¸€ç§åŠ å¯†æ–¹å¼ï¼Œä½¿ç”¨ç›¸åŒçš„å¯†é’¥è¿›è¡ŒåŠ å¯†å’Œè§£å¯†ã€‚"
            test_reference = "å¯¹ç§°åŠ å¯†æ˜¯æŒ‡åŠ å¯†å’Œè§£å¯†ä½¿ç”¨åŒä¸€ä¸ªå¯†é’¥çš„åŠ å¯†ç®—æ³•ã€‚"
            
            # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—
            def simple_similarity(text1, text2):
                words1 = set(text1.split())
                words2 = set(text2.split())
                intersection = words1.intersection(words2)
                union = words1.union(words2)
                return len(intersection) / len(union) if union else 0
            
            similarity_score = simple_similarity(test_answer, test_reference)
            
            result["test_results"] = {
                "question": test_question,
                "answer": test_answer,
                "reference": test_reference,
                "similarity_score": similarity_score
            }
            
            result["evaluation_available"] = True
            
        except Exception as e:
            result["error"] = str(e)
        
        return result
    
    def validate_end_to_end_pipeline(self) -> Dict[str, Any]:
        """éªŒè¯ç«¯åˆ°ç«¯æµæ°´çº¿"""
        console.print("  ğŸ”„ éªŒè¯ç«¯åˆ°ç«¯æµæ°´çº¿...")
        
        result = {
            "pipeline_components": {},
            "readiness_check": {},
            "next_steps": []
        }
        
        # æ£€æŸ¥å„ä¸ªç»„ä»¶çš„å°±ç»ªçŠ¶æ€
        components = {
            "processed_data": self.output_dir / "processed_data.json",
            "train_split": self.output_dir / "train.json",
            "val_split": self.output_dir / "val.json",
            "test_split": self.output_dir / "test.json",
            "training_config": self.output_dir / "training_config.yaml",
            "dataset_info": self.output_dir / "dataset_info.json",
            "parallel_config": self.output_dir / "parallel_config.yaml"
        }
        
        for component, file_path in components.items():
            result["pipeline_components"][component] = {
                "exists": file_path.exists(),
                "path": str(file_path)
            }
        
        # å°±ç»ªçŠ¶æ€æ£€æŸ¥
        all_ready = all(info["exists"] for info in result["pipeline_components"].values())
        result["readiness_check"]["all_components_ready"] = all_ready
        
        # æ£€æŸ¥è®­ç»ƒç¯å¢ƒ
        try:
            import torch
            result["readiness_check"]["torch_available"] = True
            result["readiness_check"]["cuda_available"] = torch.cuda.is_available()
            result["readiness_check"]["gpu_count"] = torch.cuda.device_count() if torch.cuda.is_available() else 0
        except ImportError:
            result["readiness_check"]["torch_available"] = False
        
        # ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè®®
        if all_ready:
            result["next_steps"].append("æ‰€æœ‰ç»„ä»¶å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ")
            if result["readiness_check"].get("gpu_count", 0) > 1:
                result["next_steps"].append("æ£€æµ‹åˆ°å¤šGPUï¼Œå¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
            result["next_steps"].append("è¿è¡Œå‘½ä»¤: llamafactory-cli train training_config.yaml")
        else:
            missing = [comp for comp, info in result["pipeline_components"].items() if not info["exists"]]
            result["next_steps"].append(f"ç¼ºå°‘ç»„ä»¶: {', '.join(missing)}")
        
        return result
    
    def generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆéªŒè¯æŠ¥å‘Š"""
        console.print("\nğŸ“‹ ç”ŸæˆéªŒè¯æŠ¥å‘Š")
        
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        # ç»Ÿè®¡éªŒè¯ç»“æœ
        total_steps = len(self.validation_results)
        successful_steps = sum(1 for result in self.validation_results.values() 
                              if result["status"] == "success")
        failed_steps = total_steps - successful_steps
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "validation_summary": {
                "start_time": self.start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration.total_seconds(),
                "total_steps": total_steps,
                "successful_steps": successful_steps,
                "failed_steps": failed_steps,
                "success_rate": successful_steps / total_steps * 100 if total_steps > 0 else 0
            },
            "detailed_results": self.validation_results,
            "recommendations": self.generate_recommendations(),
            "system_info": {
                "python_version": sys.version,
                "platform": sys.platform,
                "working_directory": str(Path.cwd()),
                "output_directory": str(self.output_dir)
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
        self.display_final_summary(report)
        
        console.print(f"\nğŸ“„ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return report
    
    def generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºéªŒè¯ç»“æœç”Ÿæˆå»ºè®®
        failed_steps = [step_name for step_name, result in self.validation_results.items() 
                       if result["status"] == "failed"]
        
        if failed_steps:
            recommendations.append("éœ€è¦ä¿®å¤çš„é—®é¢˜:")
            for step in failed_steps:
                error = self.validation_results[step].get("error", "æœªçŸ¥é”™è¯¯")
                recommendations.append(f"  - {step}: {error}")
        
        # æ£€æŸ¥å…³é”®ä¾èµ–
        env_result = self.validation_results.get("ç¯å¢ƒå’ŒGPUæ£€æµ‹", {}).get("result", {})
        if not env_result.get("torch_available", False):
            recommendations.append("å®‰è£…PyTorch: pip install torch torchvision torchaudio")
        
        if not env_result.get("cuda_available", False):
            recommendations.append("ç¡®ä¿CUDAç¯å¢ƒæ­£ç¡®é…ç½®")
        
        # æ£€æŸ¥LlamaFactory
        llama_result = self.validation_results.get("LlamaFactoryé›†æˆ", {}).get("result", {})
        if not llama_result.get("llamafactory_available", False):
            recommendations.append("å®‰è£…LlamaFactory: pip install llamafactory")
        
        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("ğŸ‰ æ‰€æœ‰éªŒè¯æ­¥éª¤éƒ½æˆåŠŸå®Œæˆï¼")
            recommendations.append("ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡ŒLlamaFactoryå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒ")
            recommendations.append("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
            recommendations.append("  llamafactory-cli train validation_output/training_config.yaml")
        else:
            recommendations.append("\nä¿®å¤ä¸Šè¿°é—®é¢˜åï¼Œé‡æ–°è¿è¡ŒéªŒè¯è„šæœ¬")
        
        return recommendations
    
    def display_final_summary(self, report: Dict[str, Any]):
        """æ˜¾ç¤ºæœ€ç»ˆæ‘˜è¦"""
        summary = report["validation_summary"]
        
        console.print("\n" + "="*50)
        console.print("éªŒè¯æ‘˜è¦")
        console.print("="*50)
        
        if RICH_AVAILABLE:
            table = Table(title="éªŒè¯ç»“æœ", box=box.ROUNDED)
            table.add_column("æŒ‡æ ‡", style="cyan")
            table.add_column("å€¼", style="green")
            
            table.add_row("æ€»éªŒè¯æ­¥éª¤", str(summary["total_steps"]))
            table.add_row("æˆåŠŸæ­¥éª¤", str(summary["successful_steps"]))
            table.add_row("å¤±è´¥æ­¥éª¤", str(summary["failed_steps"]))
            table.add_row("æˆåŠŸç‡", f"{summary['success_rate']:.1f}%")
            table.add_row("æ€»è€—æ—¶", f"{summary['duration_seconds']:.1f}ç§’")
            
            console.print(table)
        else:
            console.print(f"æ€»éªŒè¯æ­¥éª¤: {summary['total_steps']}")
            console.print(f"æˆåŠŸæ­¥éª¤: {summary['successful_steps']}")
            console.print(f"å¤±è´¥æ­¥éª¤: {summary['failed_steps']}")
            console.print(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
            console.print(f"æ€»è€—æ—¶: {summary['duration_seconds']:.1f}ç§’")
        
        # æ˜¾ç¤ºå»ºè®®
        if report["recommendations"]:
            console.print("\nğŸ’¡ å»ºè®®:")
            for i, rec in enumerate(report["recommendations"], 1):
                console.print(f"  {i}. {rec}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” LlamaFactoryå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒå…¨é¢åŠŸèƒ½éªŒè¯")
    print("ä½¿ç”¨ data/raw ä½œä¸ºæºæ•°æ®è¿›è¡Œå®Œæ•´åŠŸèƒ½éªŒè¯\n")
    
    try:
        # åˆ›å»ºéªŒè¯å™¨
        validator = ComprehensiveValidatorSync()
        
        # è¿è¡ŒéªŒè¯
        report = validator.run_comprehensive_validation()
        
        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        if report["validation_summary"]["success_rate"] >= 80:
            console.print("\nğŸ‰ éªŒè¯å®Œæˆï¼ç³»ç»ŸåŠŸèƒ½åŸºæœ¬æ­£å¸¸ã€‚")
        else:
            console.print("\nâš ï¸  éªŒè¯å®Œæˆï¼Œä½†å‘ç°ä¸€äº›é—®é¢˜éœ€è¦ä¿®å¤ã€‚")
        
        return report
        
    except KeyboardInterrupt:
        console.print("\nâ¹ï¸  éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
        return None
    except Exception as e:
        console.print(f"\nğŸ’¥ éªŒè¯å¤±è´¥: {e}")
        logging.error(f"éªŒè¯å¤±è´¥: {e}", exc_info=True)
        return None


if __name__ == "__main__":
    # è¿è¡ŒéªŒè¯
    main()