#!/usr/bin/env python3
"""
å®ç”¨æ¨¡å‹å¯¼å‡ºæ¼”ç¤ºç¨‹åº

æœ¬ç¨‹åºä½¿ç”¨ç°æœ‰çš„æ¨¡å‹å¯¼å‡ºåŠŸèƒ½ï¼Œæ¼”ç¤ºå¦‚ä½•ï¼š
1. åŠ è½½å¾®è°ƒåçš„æ£€æŸ¥ç‚¹
2. ä½¿ç”¨è‡ªç ”æ¨¡å‹å¯¼å‡ºå™¨è¿›è¡Œé‡åŒ–
3. éªŒè¯ä¸­æ–‡å¤„ç†èƒ½åŠ›
4. ç”Ÿæˆéƒ¨ç½²åŒ…

ä½¿ç”¨æ–¹æ³•:
    python practical_model_export_demo.py
"""

import os
import sys
import torch
import logging
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥è‡ªç ”æ¨¡å—
try:
    from model_exporter import (
        ModelExporter, 
        QuantizationConfig, 
        QuantizationFormat, 
        QuantizationBackend,
        ModelQuantizer,
        ChineseCapabilityValidator
    )
    MODEL_EXPORTER_AVAILABLE = True
    logger.info("âœ… è‡ªç ”æ¨¡å‹å¯¼å‡ºå™¨åŠ è½½æˆåŠŸ")
except ImportError as e:
    MODEL_EXPORTER_AVAILABLE = False
    logger.error(f"âŒ è‡ªç ”æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel, PeftConfig
    TRANSFORMERS_AVAILABLE = True
    logger.info("âœ… Transformersåº“å¯ç”¨")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("âš ï¸ Transformersåº“ä¸å¯ç”¨")


class PracticalModelExporter:
    """å®ç”¨æ¨¡å‹å¯¼å‡ºå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        if MODEL_EXPORTER_AVAILABLE:
            self.exporter = ModelExporter()
            self.quantizer = ModelQuantizer()
            self.validator = ChineseCapabilityValidator()
        else:
            self.exporter = None
            self.quantizer = None
            self.validator = None
    
    def load_checkpoint_model(
        self, 
        checkpoint_path: str,
        base_model_path: str = "Qwen/Qwen3-4B-Thinking-2507"
    ) -> tuple:
        """
        åŠ è½½æ£€æŸ¥ç‚¹æ¨¡å‹
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            
        Returns:
            (model, tokenizer)
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("éœ€è¦transformerså’Œpeftåº“")
        
        self.logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        checkpoint_dir = Path(checkpoint_path)
        if not checkpoint_dir.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_path}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºLoRAæ£€æŸ¥ç‚¹
        adapter_config_path = checkpoint_dir / "adapter_config.json"
        if adapter_config_path.exists():
            self.logger.info("æ£€æµ‹åˆ°LoRAæ£€æŸ¥ç‚¹ï¼Œè¿›è¡Œåˆå¹¶...")
            return self._load_lora_checkpoint(checkpoint_path, base_model_path)
        else:
            self.logger.info("åŠ è½½å®Œæ•´æ¨¡å‹æ£€æŸ¥ç‚¹...")
            return self._load_full_checkpoint(checkpoint_path)
    
    def _load_lora_checkpoint(self, checkpoint_path: str, base_model_path: str) -> tuple:
        """åŠ è½½LoRAæ£€æŸ¥ç‚¹"""
        try:
            # åŠ è½½åŸºç¡€æ¨¡å‹
            self.logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                base_model_path,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # åŠ è½½LoRAé€‚é…å™¨
            self.logger.info("åŠ è½½LoRAé€‚é…å™¨...")
            model_with_lora = PeftModel.from_pretrained(
                base_model,
                checkpoint_path,
                torch_dtype=torch.float16
            )
            
            # åˆå¹¶LoRAæƒé‡
            self.logger.info("åˆå¹¶LoRAæƒé‡...")
            merged_model = model_with_lora.merge_and_unload()
            
            return merged_model, tokenizer
            
        except Exception as e:
            self.logger.error(f"LoRAæ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_full_checkpoint(self, checkpoint_path: str) -> tuple:
        """åŠ è½½å®Œæ•´æ¨¡å‹æ£€æŸ¥ç‚¹"""
        try:
            model = AutoModelForCausalLM.from_pretrained(
                checkpoint_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            tokenizer = AutoTokenizer.from_pretrained(
                checkpoint_path,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            return model, tokenizer
            
        except Exception as e:
            self.logger.error(f"å®Œæ•´æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def export_quantized_models(
        self,
        model,
        tokenizer,
        output_dir: str,
        formats: List[str] = None
    ) -> Dict[str, Any]:
        """
        å¯¼å‡ºé‡åŒ–æ¨¡å‹
        
        Args:
            model: æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            output_dir: è¾“å‡ºç›®å½•
            formats: é‡åŒ–æ ¼å¼åˆ—è¡¨
            
        Returns:
            å¯¼å‡ºç»“æœ
        """
        if not MODEL_EXPORTER_AVAILABLE:
            return self._fallback_export(model, tokenizer, output_dir, formats)
        
        if formats is None:
            formats = ["int8", "int4", "dynamic"]
        
        results = {}
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        for format_name in formats:
            try:
                self.logger.info(f"å¼€å§‹å¯¼å‡º {format_name.upper()} æ ¼å¼...")
                
                # åˆ›å»ºé‡åŒ–é…ç½®
                config = self._create_quantization_config(format_name)
                
                # æ‰§è¡Œé‡åŒ–
                quantized_model, quant_result = self.quantizer.quantize_model(
                    model=model,
                    tokenizer=tokenizer,
                    config=config
                )
                
                if quant_result.success:
                    # ä¿å­˜é‡åŒ–æ¨¡å‹
                    format_dir = output_path / format_name
                    format_dir.mkdir(exist_ok=True)
                    
                    # ä¿å­˜æ¨¡å‹
                    if hasattr(quantized_model, 'save_pretrained'):
                        quantized_model.save_pretrained(format_dir)
                    else:
                        torch.save(quantized_model.state_dict(), format_dir / "model.pth")
                    
                    # ä¿å­˜tokenizer
                    tokenizer.save_pretrained(format_dir)
                    
                    # éªŒè¯ä¸­æ–‡èƒ½åŠ›
                    chinese_validation = self.validator.validate_chinese_capability(
                        quantized_model, tokenizer
                    )
                    
                    results[format_name] = {
                        "success": True,
                        "quantization_result": quant_result.to_dict(),
                        "chinese_validation": chinese_validation,
                        "output_path": str(format_dir)
                    }
                    
                    self.logger.info(f"âœ… {format_name.upper()} å¯¼å‡ºæˆåŠŸ")
                else:
                    results[format_name] = {
                        "success": False,
                        "error": quant_result.error_message
                    }
                    self.logger.error(f"âŒ {format_name.upper()} å¯¼å‡ºå¤±è´¥: {quant_result.error_message}")
                
            except Exception as e:
                results[format_name] = {
                    "success": False,
                    "error": str(e)
                }
                self.logger.error(f"âŒ {format_name.upper()} å¯¼å‡ºå¼‚å¸¸: {e}")
        
        # ç”Ÿæˆå¯¼å‡ºæŠ¥å‘Š
        self._generate_export_report(results, output_path)
        
        return results
    
    def _create_quantization_config(self, format_name: str) -> QuantizationConfig:
        """åˆ›å»ºé‡åŒ–é…ç½®"""
        if format_name == "int8":
            return QuantizationConfig(
                format=QuantizationFormat.INT8,
                backend=QuantizationBackend.BITSANDBYTES,
                load_in_8bit=True
            )
        elif format_name == "int4":
            return QuantizationConfig(
                format=QuantizationFormat.INT4,
                backend=QuantizationBackend.BITSANDBYTES,
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
        elif format_name == "gptq":
            return QuantizationConfig(
                format=QuantizationFormat.GPTQ,
                backend=QuantizationBackend.GPTQ,
                bits=4,
                group_size=128
            )
        elif format_name == "dynamic":
            return QuantizationConfig(
                format=QuantizationFormat.DYNAMIC,
                backend=QuantizationBackend.PYTORCH
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–æ ¼å¼: {format_name}")
    
    def _fallback_export(
        self,
        model,
        tokenizer,
        output_dir: str,
        formats: List[str]
    ) -> Dict[str, Any]:
        """å¤‡ç”¨å¯¼å‡ºæ–¹æ³•ï¼ˆå½“è‡ªç ”æ¨¡å—ä¸å¯ç”¨æ—¶ï¼‰"""
        self.logger.warning("ä½¿ç”¨å¤‡ç”¨å¯¼å‡ºæ–¹æ³•...")
        
        results = {}
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åªå¯¼å‡ºFP16ç‰ˆæœ¬ä½œä¸ºåŸºå‡†
        try:
            fp16_dir = output_path / "fp16"
            fp16_dir.mkdir(exist_ok=True)
            
            # è½¬æ¢ä¸ºFP16å¹¶ä¿å­˜
            model_fp16 = model.half()
            model_fp16.save_pretrained(fp16_dir)
            tokenizer.save_pretrained(fp16_dir)
            
            # ç®€å•çš„æ¨ç†æµ‹è¯•
            test_result = self._simple_inference_test(model_fp16, tokenizer)
            
            results["fp16"] = {
                "success": True,
                "format": "FP16",
                "output_path": str(fp16_dir),
                "test_result": test_result,
                "note": "å¤‡ç”¨å¯¼å‡ºï¼Œä»…FP16æ ¼å¼"
            }
            
            self.logger.info("âœ… FP16å¤‡ç”¨å¯¼å‡ºæˆåŠŸ")
            
        except Exception as e:
            results["fp16"] = {
                "success": False,
                "error": str(e)
            }
            self.logger.error(f"âŒ å¤‡ç”¨å¯¼å‡ºå¤±è´¥: {e}")
        
        return results
    
    def _simple_inference_test(self, model, tokenizer) -> Dict[str, Any]:
        """ç®€å•æ¨ç†æµ‹è¯•"""
        test_prompts = [
            "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ",
            "è¯·è§£é‡Šæ•°å­—ç­¾åçš„ä½œç”¨ã€‚"
        ]
        
        results = []
        model.eval()
        
        with torch.no_grad():
            for prompt in test_prompts:
                try:
                    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                    inputs = {k: v.to(model.device) for k, v in inputs.items()}
                    
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs["input_ids"].shape[1] + 50,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id
                    )
                    
                    response = tokenizer.decode(
                        outputs[0][inputs["input_ids"].shape[1]:],
                        skip_special_tokens=True
                    )
                    
                    results.append({
                        "prompt": prompt,
                        "response": response[:100] + "..." if len(response) > 100 else response,
                        "success": True
                    })
                    
                except Exception as e:
                    results.append({
                        "prompt": prompt,
                        "response": "",
                        "success": False,
                        "error": str(e)
                    })
        
        success_rate = sum(1 for r in results if r["success"]) / len(results)
        
        return {
            "success_rate": success_rate,
            "test_cases": results
        }
    
    def _generate_export_report(self, results: Dict[str, Any], output_path: Path):
        """ç”Ÿæˆå¯¼å‡ºæŠ¥å‘Š"""
        report = {
            "export_time": datetime.now().isoformat(),
            "exporter_version": "practical_demo_v1.0",
            "total_formats": len(results),
            "successful_exports": sum(1 for r in results.values() if r.get("success", False)),
            "results": results,
            "summary": {
                "formats_attempted": list(results.keys()),
                "successful_formats": [k for k, v in results.items() if v.get("success", False)],
                "failed_formats": [k for k, v in results.items() if not v.get("success", False)]
            }
        }
        
        with open(output_path / "export_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"å¯¼å‡ºæŠ¥å‘Šå·²ä¿å­˜: {output_path / 'export_report.json'}")
    
    def generate_usage_examples(self, output_dir: str, results: Dict[str, Any]):
        """ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹"""
        output_path = Path(output_dir)
        
        # ç”ŸæˆPythonä½¿ç”¨ç¤ºä¾‹
        example_script = f'''#!/usr/bin/env python3
"""
æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹è„šæœ¬
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path

def load_model(model_path, use_quantization=False):
    """åŠ è½½æ¨¡å‹"""
    print(f"åŠ è½½æ¨¡å‹: {{model_path}}")
    
    if use_quantization:
        from transformers import BitsAndBytesConfig
        
        # INT8é‡åŒ–é…ç½®
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def test_basic_inference(model, tokenizer):
    """åŸºç¡€æ¨ç†æµ‹è¯•"""
    print("\\n=== åŸºç¡€æ¨ç†æµ‹è¯• ===")
    
    prompts = [
        "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ",
        "è¯·è§£é‡ŠRSAç®—æ³•çš„å·¥ä½œåŸç†ã€‚",
        "æ•°å­—ç­¾åæœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ"
    ]
    
    for prompt in prompts:
        print(f"\\nè¾“å…¥: {{prompt}}")
        
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=inputs["input_ids"].shape[1] + 100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id
            )
        
        response = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )
        
        print(f"è¾“å‡º: {{response}}")
        print("-" * 50)

def test_thinking_inference(model, tokenizer):
    """æ·±åº¦æ€è€ƒæ¨ç†æµ‹è¯•"""
    print("\\n=== æ·±åº¦æ€è€ƒæ¨ç†æµ‹è¯• ===")
    
    thinking_prompt = '''<thinking>
ç”¨æˆ·è¯¢é—®å…³äºæ¤­åœ†æ›²çº¿å¯†ç å­¦çš„é—®é¢˜ã€‚æˆ‘éœ€è¦ï¼š
1. è§£é‡Šæ¤­åœ†æ›²çº¿å¯†ç å­¦çš„åŸºæœ¬æ¦‚å¿µ
2. è¯´æ˜å…¶ç›¸æ¯”RSAçš„ä¼˜åŠ¿
3. æåŠå®é™…åº”ç”¨åœºæ™¯
</thinking>
è¯·è¯¦ç»†è§£é‡Šæ¤­åœ†æ›²çº¿å¯†ç å­¦çš„ä¼˜åŠ¿ã€‚'''
    
    print(f"è¾“å…¥: {{thinking_prompt}}")
    
    inputs = tokenizer(thinking_prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=inputs["input_ids"].shape[1] + 200,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id
        )
    
    response = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    )
    
    print(f"è¾“å‡º: {{response}}")

def main():
    """ä¸»å‡½æ•°"""
    # æ¨¡å‹è·¯å¾„é…ç½®
    base_dir = Path("{output_dir}")
    
    # å¯ç”¨çš„æ¨¡å‹æ ¼å¼
    available_formats = {results.keys()}
    successful_formats = [k for k, v in {results}.items() if v.get("success", False)]
    
    print("å¯ç”¨çš„æ¨¡å‹æ ¼å¼:")
    for fmt in successful_formats:
        print(f"- {{fmt}}: {{base_dir / fmt}}")
    
    # é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„æ ¼å¼è¿›è¡Œæµ‹è¯•
    if successful_formats:
        test_format = successful_formats[0]
        model_path = base_dir / test_format
        
        print(f"\\nä½¿ç”¨æ ¼å¼: {{test_format}}")
        
        try:
            # åŠ è½½æ¨¡å‹
            use_quant = test_format in ["int8", "int4"]
            model, tokenizer = load_model(str(model_path), use_quantization=use_quant)
            
            # è¿è¡Œæµ‹è¯•
            test_basic_inference(model, tokenizer)
            test_thinking_inference(model, tokenizer)
            
            print("\\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {{e}}")
    else:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹æ ¼å¼")

if __name__ == "__main__":
    main()
'''
        
        with open(output_path / "usage_examples.py", "w", encoding="utf-8") as f:
            f.write(example_script)
        
        self.logger.info("ä½¿ç”¨ç¤ºä¾‹å·²ç”Ÿæˆ: usage_examples.py")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("å®ç”¨æ¨¡å‹å¯¼å‡ºæ¼”ç¤ºç¨‹åº")
    logger.info("=" * 60)
    
    # é…ç½®å‚æ•°
    checkpoint_path = "qwen3_4b_thinking_output/final_model"
    base_model_path = "Qwen/Qwen3-4B-Thinking-2507"
    output_dir = "practical_export_output"
    
    exporter = PracticalModelExporter()
    
    try:
        # æ£€æŸ¥ç¯å¢ƒ
        logger.info("æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
        if torch.cuda.is_available():
            logger.info(f"âœ… CUDAå¯ç”¨: {torch.cuda.get_device_name()}")
        else:
            logger.warning("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        
        # æ­¥éª¤1: åŠ è½½æ£€æŸ¥ç‚¹æ¨¡å‹
        logger.info(f"\\næ­¥éª¤1: åŠ è½½æ£€æŸ¥ç‚¹æ¨¡å‹")
        logger.info(f"æ£€æŸ¥ç‚¹è·¯å¾„: {checkpoint_path}")
        
        if not TRANSFORMERS_AVAILABLE:
            logger.error("âŒ transformersåº“ä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")
            logger.info("è¯·å®‰è£…: pip install transformers peft torch")
            return
        
        model, tokenizer = exporter.load_checkpoint_model(
            checkpoint_path=checkpoint_path,
            base_model_path=base_model_path
        )
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ­¥éª¤2: å¯¼å‡ºé‡åŒ–æ¨¡å‹
        logger.info(f"\\næ­¥éª¤2: å¯¼å‡ºé‡åŒ–æ¨¡å‹")
        
        # æ ¹æ®å¯ç”¨æ€§é€‰æ‹©å¯¼å‡ºæ ¼å¼
        if MODEL_EXPORTER_AVAILABLE:
            formats = ["int8", "int4", "dynamic"]
            logger.info("ä½¿ç”¨è‡ªç ”é‡åŒ–å™¨")
        else:
            formats = ["fp16"]
            logger.info("ä½¿ç”¨å¤‡ç”¨å¯¼å‡ºæ–¹æ³•")
        
        export_results = exporter.export_quantized_models(
            model=model,
            tokenizer=tokenizer,
            output_dir=output_dir,
            formats=formats
        )
        
        # æ­¥éª¤3: ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹
        logger.info(f"\\næ­¥éª¤3: ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹å’Œæ–‡æ¡£")
        exporter.generate_usage_examples(output_dir, export_results)
        
        # æ­¥éª¤4: æ˜¾ç¤ºç»“æœæ‘˜è¦
        logger.info(f"\\næ­¥éª¤4: ç»“æœæ‘˜è¦")
        logger.info("=" * 40)
        
        successful_exports = [k for k, v in export_results.items() if v.get("success", False)]
        failed_exports = [k for k, v in export_results.items() if not v.get("success", False)]
        
        if successful_exports:
            logger.info(f"âœ… æˆåŠŸå¯¼å‡ºæ ¼å¼: {', '.join(successful_exports)}")
            
            for format_name in successful_exports:
                result = export_results[format_name]
                output_path = result.get("output_path", "æœªçŸ¥")
                logger.info(f"  {format_name.upper()}: {output_path}")
                
                # æ˜¾ç¤ºé‡åŒ–ç»“æœ
                if "quantization_result" in result:
                    quant_result = result["quantization_result"]
                    if quant_result.get("success", False):
                        compression = quant_result.get("compression_ratio", 1.0)
                        accuracy = quant_result.get("accuracy_preserved", 0.0)
                        logger.info(f"    å‹ç¼©æ¯”: {compression:.2f}x, ç²¾åº¦ä¿æŒ: {accuracy:.1%}")
                
                # æ˜¾ç¤ºä¸­æ–‡éªŒè¯ç»“æœ
                if "chinese_validation" in result:
                    chinese_result = result["chinese_validation"]
                    overall_score = chinese_result.get("overall_score", 0.0)
                    logger.info(f"    ä¸­æ–‡èƒ½åŠ›è¯„åˆ†: {overall_score:.1%}")
        
        if failed_exports:
            logger.warning(f"âš ï¸ å¤±è´¥æ ¼å¼: {', '.join(failed_exports)}")
            for format_name in failed_exports:
                error = export_results[format_name].get("error", "æœªçŸ¥é”™è¯¯")
                logger.warning(f"  {format_name.upper()}: {error}")
        
        logger.info(f"\\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"ğŸ“– æŸ¥çœ‹ export_report.json äº†è§£è¯¦ç»†ä¿¡æ¯")
        logger.info(f"ğŸš€ è¿è¡Œ usage_examples.py æµ‹è¯•æ¨¡å‹")
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()