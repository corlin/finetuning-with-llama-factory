#!/usr/bin/env python3
"""
æµ‹è¯•æ›´æ–°åçš„è®­ç»ƒæµæ°´çº¿
éªŒè¯è‡ªç ”è®­ç»ƒå¼•æ“é›†æˆæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
import logging
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append('src')

from training_pipeline import TrainingPipelineOrchestrator
from data_models import TrainingExample, ThinkingExample
from config_manager import TrainingConfig, DataConfig
from lora_config_optimizer import LoRAMemoryProfile
from parallel_config import ParallelConfig, ParallelStrategy
from gpu_utils import GPUDetector


def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    test_data = [
        ThinkingExample(
            instruction="ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ",
            thinking_process="æˆ‘éœ€è¦è§£é‡ŠAESåŠ å¯†ç®—æ³•çš„åŸºæœ¬æ¦‚å¿µã€å·¥ä½œåŸç†å’Œåº”ç”¨åœºæ™¯ã€‚",
            final_response="AESï¼ˆAdvanced Encryption Standardï¼‰æ˜¯ä¸€ç§å¯¹ç§°åŠ å¯†ç®—æ³•...",
            crypto_terms=["AES", "å¯¹ç§°åŠ å¯†", "åŠ å¯†ç®—æ³•"],
            reasoning_steps=["å®šä¹‰AES", "è§£é‡Šå·¥ä½œåŸç†", "è¯´æ˜åº”ç”¨åœºæ™¯"]
        ),
        TrainingExample(
            instruction="è§£é‡ŠRSAç®—æ³•çš„å·¥ä½œåŸç†",
            input="",
            output="<thinking>\nRSAæ˜¯ä¸€ç§éå¯¹ç§°åŠ å¯†ç®—æ³•ï¼ŒåŸºäºå¤§æ•°åˆ†è§£çš„æ•°å­¦éš¾é¢˜ã€‚\n</thinking>\n\nRSAç®—æ³•æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„éå¯¹ç§°åŠ å¯†ç®—æ³•...",
            thinking="RSAæ˜¯ä¸€ç§éå¯¹ç§°åŠ å¯†ç®—æ³•ï¼ŒåŸºäºå¤§æ•°åˆ†è§£çš„æ•°å­¦éš¾é¢˜ã€‚",
            crypto_terms=["RSA", "éå¯¹ç§°åŠ å¯†", "å…¬é’¥", "ç§é’¥"],
            difficulty_level=3,
            source_file="test_data.md"
        )
    ]
    return test_data


def create_test_configs():
    """åˆ›å»ºæµ‹è¯•é…ç½®"""
    # è®­ç»ƒé…ç½®
    training_config = TrainingConfig(
        output_dir="test_output",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        learning_rate=1e-4,
        num_train_epochs=1,
        warmup_ratio=0.1,
        save_steps=10,
        logging_steps=5,
        fp16=True
    )
    
    # æ•°æ®é…ç½®
    data_config = DataConfig(
        max_samples=2048,
        train_split_ratio=0.8,
        eval_split_ratio=0.2
    )
    
    # LoRAé…ç½®
    lora_config = LoRAMemoryProfile(
        rank=8,
        alpha=16,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
    )
    
    # å¹¶è¡Œé…ç½®
    gpu_detector = GPUDetector()
    gpu_infos = gpu_detector.get_all_gpu_info()
    num_gpus = min(len(gpu_infos), 1)  # æµ‹è¯•æ—¶åªä½¿ç”¨1ä¸ªGPU
    
    parallel_config = ParallelConfig(
        strategy=ParallelStrategy.DATA_PARALLEL,
        data_parallel_size=num_gpus,
        master_addr="localhost",
        master_port=29500
    )
    
    return training_config, data_config, lora_config, parallel_config


def test_training_pipeline():
    """æµ‹è¯•è®­ç»ƒæµæ°´çº¿"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æ›´æ–°åçš„è®­ç»ƒæµæ°´çº¿...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®å’Œé…ç½®
        test_data = create_test_data()
        training_config, data_config, lora_config, parallel_config = create_test_configs()
        
        # åˆ›å»ºæµæ°´çº¿ç¼–æ’å™¨
        pipeline = TrainingPipelineOrchestrator(
            pipeline_id="test_pipeline",
            output_dir="test_pipeline_output"
        )
        
        # é…ç½®æµæ°´çº¿
        pipeline.configure_pipeline(
            training_data=test_data,
            training_config=training_config,
            data_config=data_config,
            lora_config=lora_config,
            parallel_config=parallel_config
        )
        
        print("âœ… æµæ°´çº¿é…ç½®å®Œæˆ")
        
        # æµ‹è¯•å„ä¸ªé˜¶æ®µ
        print("\nğŸ”§ æµ‹è¯•åˆå§‹åŒ–é˜¶æ®µ...")
        if pipeline._stage_initialization():
            print("âœ… åˆå§‹åŒ–é˜¶æ®µé€šè¿‡")
        else:
            print("âŒ åˆå§‹åŒ–é˜¶æ®µå¤±è´¥")
            return False
        
        print("\nğŸ”§ æµ‹è¯•æ•°æ®å‡†å¤‡é˜¶æ®µ...")
        if pipeline._stage_data_preparation():
            print("âœ… æ•°æ®å‡†å¤‡é˜¶æ®µé€šè¿‡")
        else:
            print("âŒ æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥")
            return False
        
        print("\nğŸ”§ æµ‹è¯•é…ç½®ç”Ÿæˆé˜¶æ®µ...")
        if pipeline._stage_config_generation():
            print("âœ… é…ç½®ç”Ÿæˆé˜¶æ®µé€šè¿‡")
        else:
            print("âŒ é…ç½®ç”Ÿæˆé˜¶æ®µå¤±è´¥")
            return False
        
        print("\nğŸ”§ æµ‹è¯•ç¯å¢ƒè®¾ç½®é˜¶æ®µ...")
        if pipeline._stage_environment_setup():
            print("âœ… ç¯å¢ƒè®¾ç½®é˜¶æ®µé€šè¿‡")
        else:
            print("âŒ ç¯å¢ƒè®¾ç½®é˜¶æ®µå¤±è´¥")
            return False
        
        print("\nğŸ”§ æµ‹è¯•è®­ç»ƒæ‰§è¡Œé˜¶æ®µï¼ˆè·³è¿‡å®é™…è®­ç»ƒï¼‰...")
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸æ‰§è¡Œå®é™…çš„è®­ç»ƒï¼Œåªæµ‹è¯•é…ç½®æ˜¯å¦æ­£ç¡®
        print("âš ï¸ è·³è¿‡å®é™…è®­ç»ƒæ‰§è¡Œä»¥èŠ‚çœæ—¶é—´")
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é˜¶æ®µé€šè¿‡ï¼")
        
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        output_dir = Path("test_pipeline_output")
        if output_dir.exists():
            print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            for file_path in output_dir.rglob("*"):
                if file_path.is_file():
                    print(f"  - {file_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_gpu_detection():
    """æµ‹è¯•GPUæ£€æµ‹åŠŸèƒ½"""
    print("\nğŸ”§ æµ‹è¯•GPUæ£€æµ‹åŠŸèƒ½...")
    
    try:
        gpu_detector = GPUDetector()
        
        # æµ‹è¯•CUDAå¯ç”¨æ€§æ£€æµ‹
        cuda_available = gpu_detector.detect_cuda_availability()
        print(f"CUDAå¯ç”¨æ€§: {cuda_available}")
        
        # æµ‹è¯•GPUä¿¡æ¯è·å–
        gpu_infos = gpu_detector.get_all_gpu_info()
        print(f"æ£€æµ‹åˆ° {len(gpu_infos)} ä¸ªGPU")
        
        for gpu_info in gpu_infos:
            print(f"  GPU {gpu_info.gpu_id}: {gpu_info.name}")
            print(f"    æ€»å†…å­˜: {gpu_info.total_memory}MB")
            print(f"    å¯ç”¨å†…å­˜: {gpu_info.free_memory}MB")
        
        # æµ‹è¯•GPUæ‹“æ‰‘æ£€æµ‹
        topology = gpu_detector.detect_gpu_topology()
        print(f"GPUæ‹“æ‰‘ç±»å‹: {topology.topology_type}")
        print(f"GPUæ•°é‡: {topology.num_gpus}")
        
        print("âœ… GPUæ£€æµ‹åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ GPUæ£€æµ‹åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_memory_manager():
    """æµ‹è¯•å†…å­˜ç®¡ç†å™¨åŠŸèƒ½"""
    print("\nğŸ”§ æµ‹è¯•å†…å­˜ç®¡ç†å™¨åŠŸèƒ½...")
    
    try:
        from memory_manager import MemoryManager
        
        # åˆ›å»ºå†…å­˜ç®¡ç†å™¨
        memory_manager = MemoryManager({
            "monitoring_interval": 1,
            "enable_auto_adjustment": True,
            "initial_batch_size": 4
        })
        
        # æµ‹è¯•å¯åŠ¨å’Œåœæ­¢
        if memory_manager.start():
            print("âœ… å†…å­˜ç®¡ç†å™¨å¯åŠ¨æˆåŠŸ")
            
            # è·å–å½“å‰å†…å­˜çŠ¶æ€
            memory_status = memory_manager.get_current_memory_status()
            if memory_status:
                print(f"å½“å‰GPUå†…å­˜ä½¿ç”¨: {memory_status.allocated_memory}MB / {memory_status.total_memory}MB")
                print(f"å†…å­˜å‹åŠ›çº§åˆ«: {memory_status.pressure_level.value}")
            
            # åœæ­¢å†…å­˜ç®¡ç†å™¨
            if memory_manager.stop():
                print("âœ… å†…å­˜ç®¡ç†å™¨åœæ­¢æˆåŠŸ")
            else:
                print("âš ï¸ å†…å­˜ç®¡ç†å™¨åœæ­¢å¤±è´¥")
        else:
            print("âš ï¸ å†…å­˜ç®¡ç†å™¨å¯åŠ¨å¤±è´¥ï¼ˆå¯èƒ½æ²¡æœ‰GPUï¼‰")
        
        print("âœ… å†…å­˜ç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ å†…å­˜ç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ›´æ–°åçš„è®­ç»ƒæµæ°´çº¿é›†æˆ")
    print("=" * 60)
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæµ‹è¯•è¾“å‡ºç›®å½•
    os.makedirs("test_pipeline_output", exist_ok=True)
    
    success_count = 0
    total_tests = 3
    
    # æµ‹è¯•GPUæ£€æµ‹
    if test_gpu_detection():
        success_count += 1
    
    # æµ‹è¯•å†…å­˜ç®¡ç†å™¨
    if test_memory_manager():
        success_count += 1
    
    # æµ‹è¯•è®­ç»ƒæµæ°´çº¿
    if test_training_pipeline():
        success_count += 1
    
    print("\n" + "=" * 60)
    print(f"ğŸ¯ æµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    
    if success_count == total_tests:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è‡ªç ”è®­ç»ƒå¼•æ“é›†æˆæˆåŠŸ")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)