#!/usr/bin/env python3
"""
ç›´æ¥ä½¿ç”¨å·²å®ç°æ¨¡å—è¿›è¡Œå¾®è°ƒçš„è„šæœ¬
ä¸ä¾èµ–LlamaFactoryï¼Œç›´æ¥ä½¿ç”¨PyTorchå’Œå·²å®ç°çš„åŠŸèƒ½æ¨¡å—
ä½¿ç”¨uvåŒ…ç®¡ç†å™¨è¿›è¡Œä¾èµ–ç®¡ç†
"""

import os
import sys
import json
import logging
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    get_cosine_schedule_with_warmup,
    TrainingArguments, Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
import numpy as np
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from pathlib import Path
import time
from datetime import datetime
import re

# å¯¼å…¥å·²å®ç°çš„æ¨¡å—
sys.path.append('src')
from data_models import TrainingExample, ThinkingExample, ChineseMetrics
from config_manager import TrainingConfig
from memory_manager import MemoryManager, MemoryPressureLevel
from gpu_utils import GPUDetector
from parallel_strategy_recommender import ParallelStrategyRecommender, ParallelStrategy
from training_monitor import TrainingMonitor
from chinese_nlp_processor import ChineseNLPProcessor
from crypto_term_processor import CryptoTermProcessor


@dataclass
class DirectTrainingConfig:
    """ç›´æ¥è®­ç»ƒé…ç½®"""
    model_name: str = "Qwen/Qwen3-4B-Thinking-2507"  # ç›®æ ‡å¾®è°ƒæ¨¡å‹
    data_path: str = "data/raw"  # åŸå§‹markdownæ•°æ®ç›®å½•
    output_dir: str = "qwen3_4b_thinking_output"
    max_seq_length: int = 2048  # Thinkingæ¨¡å‹éœ€è¦æ›´é•¿åºåˆ—
    batch_size: int = 1  # 4Bæ¨¡å‹éœ€è¦æ›´å°æ‰¹æ¬¡
    gradient_accumulation_steps: int = 8  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
    learning_rate: float = 1e-4  # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
    num_epochs: int = 15
    warmup_ratio: float = 0.1
    save_steps: int = 50  # æ›´é¢‘ç¹ä¿å­˜
    logging_steps: int = 5
    
    # LoRAé…ç½® - é’ˆå¯¹4Bæ¨¡å‹ä¼˜åŒ–
    lora_r: int = 240  # å¢åŠ rank
    lora_alpha: int = 480  # å¢åŠ alpha
    lora_dropout: float = 0.1
    target_modules: List[str] = None
    
    # å†…å­˜ä¼˜åŒ– - 4Bæ¨¡å‹éœ€è¦æ›´æ¿€è¿›çš„ä¼˜åŒ–
    use_gradient_checkpointing: bool = True
    use_fp16: bool = True
    
    def __post_init__(self):
        if self.target_modules is None:
            # Qwen3-4B-Thinkingæ¨¡å‹çš„æ³¨æ„åŠ›å’ŒMLPå±‚
            self.target_modules = [
                #"q_proj","v_proj"
                "q_proj", "k_proj", "v_proj", "o_proj",  # æ³¨æ„åŠ›å±‚
                "gate_proj", "up_proj", "down_proj"      # MLPå±‚
            ]


class CryptoQADataset(Dataset):
    """å¯†ç å­¦QAæ•°æ®é›† - ä½¿ç”¨ç°æœ‰é¢„å¤„ç†æ–¹æ³•"""
    
    def __init__(self, data_path: str, tokenizer, max_length: int = 1024):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # åˆå§‹åŒ–ä¸­æ–‡å¤„ç†å™¨
        try:
            self.chinese_processor = ChineseNLPProcessor()
            print("âœ… åˆå§‹åŒ–ä¸­æ–‡NLPå¤„ç†å™¨")
        except Exception as e:
            print(f"âš ï¸ ä¸­æ–‡NLPå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.chinese_processor = None
        
        # åˆå§‹åŒ–å¯†ç å­¦æœ¯è¯­å¤„ç†å™¨
        try:
            self.crypto_processor = CryptoTermProcessor()
            print("âœ… åˆå§‹åŒ–å¯†ç å­¦æœ¯è¯­å¤„ç†å™¨")
        except Exception as e:
            print(f"âš ï¸ å¯†ç å­¦æœ¯è¯­å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.crypto_processor = None
        
        # å¤„ç†åŸå§‹markdownæ•°æ®
        self.data = self.process_raw_markdown_data(data_path)
        
        print(f"âœ… å¤„ç†äº† {len(self.data)} æ¡è®­ç»ƒæ•°æ®")
    
    def process_raw_markdown_data(self, data_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†åŸå§‹markdownæ•°æ® - ä½¿ç”¨ç°æœ‰é¢„å¤„ç†æ¨¡å—"""
        processed_data = []
        
        # è·å–æ‰€æœ‰markdownæ–‡ä»¶
        markdown_files = []
        if os.path.isdir(data_path):
            for file in os.listdir(data_path):
                if file.endswith('.md'):
                    markdown_files.append(os.path.join(data_path, file))
        else:
            markdown_files = [data_path]
        
        print(f"ğŸ”„ å¤„ç† {len(markdown_files)} ä¸ªmarkdownæ–‡ä»¶...")
        
        for file_path in markdown_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # è§£æmarkdownå†…å®¹
                qa_pairs = self.parse_markdown_content(content, file_path)
                
                # ä½¿ç”¨ç°æœ‰æ¨¡å—è¿›è¡Œæ•°æ®å¢å¼º
                enhanced_qa_pairs = self.enhance_qa_pairs(qa_pairs)
                
                processed_data.extend(enhanced_qa_pairs)
                print(f"  âœ… {os.path.basename(file_path)}: æå–äº† {len(qa_pairs)} ä¸ªQAå¯¹ï¼Œå¢å¼ºå {len(enhanced_qa_pairs)} ä¸ª")
                
            except Exception as e:
                print(f"  âŒ {file_path}: å¤„ç†å¤±è´¥ - {e}")
        
        return processed_data
    
    def parse_markdown_content(self, content: str, file_path: str = "") -> List[Dict[str, Any]]:
        """è§£æmarkdownå†…å®¹æå–QAå¯¹ - æ”¹è¿›çš„è§£æé€»è¾‘"""
        qa_pairs = []
        
        # åˆ†å‰²å†…å®¹ä¸ºé—®é¢˜å—
        import re
        
        # æ›´ç²¾ç¡®çš„é—®é¢˜æ¨¡å¼åŒ¹é…
        question_pattern = r'### Q(\d+)[:\s]+(.*?)(?=### Q\d+|$)'
        matches = re.findall(question_pattern, content, re.DOTALL)
        
        print(f"    ğŸ” åœ¨ {os.path.basename(file_path)} ä¸­æ‰¾åˆ° {len(matches)} ä¸ªé—®é¢˜å—")
        
        for question_num, question_block in matches:
            try:
                qa_pair = self.extract_qa_from_block(question_block.strip(), question_num, file_path)
                if qa_pair:
                    qa_pairs.append(qa_pair)
            except Exception as e:
                print(f"    âš ï¸ è§£æé—®é¢˜ Q{question_num} å¤±è´¥: {e}")
                continue
        
        return qa_pairs
    
    def extract_qa_from_block(self, block: str, question_num: str, file_path: str = "") -> Optional[Dict[str, Any]]:
        """ä»é—®é¢˜å—ä¸­æå–QAå¯¹ - æ”¹è¿›çš„æå–é€»è¾‘"""
        lines = block.split('\n')
        if not lines:
            return None
        
        # ç¬¬ä¸€è¡Œæ˜¯é—®é¢˜
        instruction = lines[0].strip()
        if not instruction:
            return None
        
        # æŸ¥æ‰¾thinkingéƒ¨åˆ†å’Œç­”æ¡ˆéƒ¨åˆ†
        thinking_content = ""
        answer_content = ""
        
        in_thinking = False
        thinking_lines = []
        answer_lines = []
        current_section = "none"
        
        for line in lines[1:]:
            original_line = line
            line = line.strip()
            
            if line.startswith('<thinking>'):
                in_thinking = True
                current_section = "thinking"
                thinking_lines.append(original_line)
            elif line.startswith('</thinking>'):
                in_thinking = False
                current_section = "answer"
                thinking_lines.append(original_line)
            elif in_thinking:
                thinking_lines.append(original_line)
            elif line.startswith(f'A{question_num}:') or (line.startswith('A') and ':' in line and current_section != "thinking"):
                # ç­”æ¡ˆå¼€å§‹
                current_section = "answer"
                if ':' in line:
                    answer_content_start = line.split(':', 1)[1].strip()
                    if answer_content_start:
                        answer_lines.append(answer_content_start)
            elif current_section == "answer" and line:
                answer_lines.append(original_line.rstrip())
            elif not in_thinking and line and not line.startswith('#') and not line.startswith('###'):
                # å¯èƒ½æ˜¯ç­”æ¡ˆå†…å®¹
                if current_section == "none":
                    current_section = "answer"
                if current_section == "answer":
                    answer_lines.append(original_line.rstrip())
        
        # ç»„è£…thinkingå†…å®¹
        if thinking_lines:
            thinking_content = '\n'.join(thinking_lines).strip()
        
        # ç»„è£…ç­”æ¡ˆå†…å®¹
        if answer_lines:
            answer_content = '\n'.join(answer_lines).strip()
        
        # æ„å»ºå®Œæ•´è¾“å‡º
        if thinking_content and answer_content:
            output = thinking_content + '\n\n' + answer_content
        elif thinking_content:
            output = thinking_content
        elif answer_content:
            output = answer_content
        else:
            return None
        
        # ä¼°ç®—éš¾åº¦çº§åˆ«
        difficulty = self.estimate_difficulty(instruction, output)
        
        return {
            'instruction': instruction,
            'input': '',
            'output': output,
            'system': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯†ç å­¦ä¸“å®¶ï¼Œè¯·ä»”ç»†æ€è€ƒåå›ç­”é—®é¢˜ã€‚',
            'difficulty': difficulty,
            'source_file': os.path.basename(file_path),
            'question_id': f"{os.path.basename(file_path)}_Q{question_num}"
        }
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output_text = item.get('output', '')
        system = item.get('system', 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯†ç å­¦ä¸“å®¶ï¼Œè¯·ä»”ç»†æ€è€ƒåå›ç­”é—®é¢˜ã€‚')
        
        # æ„å»ºå®Œæ•´çš„å¯¹è¯æ ¼å¼ - é€‚é…Qwen3-4B-Thinking
        if input_text:
            prompt = f"<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{instruction}\n{input_text}<|im_end|>\n<|im_start|>assistant\n"
        else:
            prompt = f"<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n"
        
        # ç¡®ä¿thinkingæ ‡ç­¾æ ¼å¼æ­£ç¡®
        if '<thinking>' in output_text and '</thinking>' in output_text:
            # ä¿æŒthinkingæ ¼å¼ä¸å˜
            full_text = prompt + output_text + "<|im_end|>"
        else:
            # å¦‚æœæ²¡æœ‰thinkingæ ‡ç­¾ï¼Œç›´æ¥æ·»åŠ 
            full_text = prompt + output_text + "<|im_end|>"
        
        # åˆ†è¯
        encoding = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding="max_length",
            return_tensors="pt"
        )
        
        # è®¡ç®—æ ‡ç­¾ï¼ˆåªå¯¹assistantéƒ¨åˆ†è®¡ç®—æŸå¤±ï¼‰
        prompt_encoding = self.tokenizer(
            prompt,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors="pt"
        )
        
        labels = encoding["input_ids"].clone()
        prompt_length = prompt_encoding["input_ids"].shape[1]
        labels[:, :prompt_length] = -100  # å¿½ç•¥promptéƒ¨åˆ†çš„æŸå¤±
        
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": labels.squeeze()
        }
    
    def enhance_qa_pairs(self, qa_pairs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ç°æœ‰æ¨¡å—å¢å¼ºQAå¯¹"""
        enhanced_pairs = []
        
        for qa_pair in qa_pairs:
            try:
                # åŸºç¡€QAå¯¹
                enhanced_pair = qa_pair.copy()
                
                # ä½¿ç”¨ä¸­æ–‡NLPå¤„ç†å™¨è¿›è¡Œæ–‡æœ¬è´¨é‡åˆ†æ
                if self.chinese_processor:
                    instruction_metrics = self.chinese_processor.analyze_text_quality(qa_pair['instruction'])
                    output_metrics = self.chinese_processor.analyze_text_quality(qa_pair['output'])
                    
                    enhanced_pair['chinese_metrics'] = {
                        'instruction_quality': instruction_metrics.overall_score if instruction_metrics else 0.0,
                        'output_quality': output_metrics.overall_score if output_metrics else 0.0
                    }
                
                # ä½¿ç”¨å¯†ç å­¦æœ¯è¯­å¤„ç†å™¨è¿›è¡Œæœ¯è¯­åˆ†æ
                if self.crypto_processor:
                    # åˆ†æé—®é¢˜ä¸­çš„å¯†ç å­¦æœ¯è¯­
                    instruction_terms = self.crypto_processor.extract_crypto_terms(qa_pair['instruction'])
                    output_terms = self.crypto_processor.extract_crypto_terms(qa_pair['output'])
                    
                    enhanced_pair['crypto_terms'] = {
                        'instruction_terms': [term.term for term in instruction_terms],
                        'output_terms': [term.term for term in output_terms],
                        'total_terms': len(instruction_terms) + len(output_terms)
                    }
                    
                    # æ ¹æ®æœ¯è¯­å¤æ‚åº¦è°ƒæ•´éš¾åº¦
                    if instruction_terms or output_terms:
                        avg_complexity = np.mean([term.complexity for term in instruction_terms + output_terms])
                        enhanced_pair['difficulty'] = max(enhanced_pair['difficulty'], int(avg_complexity))
                
                enhanced_pairs.append(enhanced_pair)
                
            except Exception as e:
                print(f"    âš ï¸ å¢å¼ºQAå¯¹å¤±è´¥: {e}")
                # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œè‡³å°‘ä¿ç•™åŸå§‹æ•°æ®
                enhanced_pairs.append(qa_pair)
        
        return enhanced_pairs
    
    def estimate_difficulty(self, instruction: str, output: str) -> int:
        """ä¼°ç®—é—®é¢˜éš¾åº¦çº§åˆ«"""
        difficulty_score = 1
        
        # åŸºäºæ–‡æœ¬é•¿åº¦çš„åˆæ­¥ä¼°ç®—
        total_length = len(instruction) + len(output)
        if total_length > 1000:
            difficulty_score = max(difficulty_score, 3)
        elif total_length > 500:
            difficulty_score = max(difficulty_score, 2)
        
        # åŸºäºthinkingæ ‡ç­¾çš„å­˜åœ¨
        if '<thinking>' in output and '</thinking>' in output:
            difficulty_score = max(difficulty_score, 2)
            
            # åˆ†æthinkingå†…å®¹çš„å¤æ‚åº¦
            thinking_match = re.search(r'<thinking>(.*?)</thinking>', output, re.DOTALL)
            if thinking_match:
                thinking_content = thinking_match.group(1)
                # å¦‚æœthinkingå†…å®¹å¾ˆé•¿æˆ–åŒ…å«å¤šä¸ªæ­¥éª¤ï¼Œæé«˜éš¾åº¦
                if len(thinking_content) > 500:
                    difficulty_score = max(difficulty_score, 3)
                if thinking_content.count('æ­¥éª¤') > 3 or thinking_content.count('åˆ†æ') > 2:
                    difficulty_score = max(difficulty_score, 3)
        
        # åŸºäºä¸“ä¸šæœ¯è¯­å¯†åº¦
        crypto_keywords = [
            'åŠ å¯†', 'è§£å¯†', 'å¯†é’¥', 'å“ˆå¸Œ', 'ç­¾å', 'è¯ä¹¦', 'ç®—æ³•', 'åè®®',
            'å¯¹ç§°', 'éå¯¹ç§°', 'å…¬é’¥', 'ç§é’¥', 'æ•°å­—ç­¾å', 'æ¶ˆæ¯è®¤è¯',
            'AES', 'RSA', 'SHA', 'MD5', 'DES', 'ECC'
        ]
        
        combined_text = instruction + output
        keyword_count = sum(1 for keyword in crypto_keywords if keyword in combined_text)
        if keyword_count > 5:
            difficulty_score = max(difficulty_score, 3)
        elif keyword_count > 2:
            difficulty_score = max(difficulty_score, 2)
        
        return min(difficulty_score, 4)  # æœ€é«˜éš¾åº¦ä¸º4


class DirectTrainer:
    """ç›´æ¥è®­ç»ƒå™¨"""
    
    def __init__(self, config: DirectTrainingConfig):
        self.config = config
        self.setup_logging()
        
        # åˆå§‹åŒ–GPUæ£€æµ‹å™¨
        self.gpu_detector = GPUDetector()
        self.gpu_info = self.gpu_detector.get_all_gpu_info()
        print(f"âœ… æ£€æµ‹åˆ° {len(self.gpu_info)} ä¸ªGPU")
        
        # åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨
        try:
            self.memory_manager = MemoryManager()
            print("âœ… åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨")
        except Exception as e:
            print(f"âš ï¸ å†…å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.memory_manager = None
        
        # åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨ï¼ˆæš‚æ—¶è·³è¿‡ä»¥é¿å…ä¾èµ–é—®é¢˜ï¼‰
        print("âš ï¸ è·³è¿‡è®­ç»ƒç›‘æ§å™¨åˆå§‹åŒ–ï¼ˆé¿å…ä¾èµ–é—®é¢˜ï¼‰")
        self.training_monitor = None
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"âœ… ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config.output_dir, exist_ok=True)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"{self.config.output_dir}/training.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {self.config.model_name}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            torch_dtype=torch.float16 if self.config.use_fp16 else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.config.use_gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°é‡: {self.model.num_parameters():,}")
        
        # é…ç½®LoRA
        self.setup_lora()
    
    def setup_lora(self):
        """è®¾ç½®LoRAé…ç½®"""
        print("ğŸ”„ é…ç½®LoRA...")
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.target_modules,
            bias="none"
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        print("âœ… LoRAé…ç½®å®Œæˆ")
    
    def create_dataset(self):
        """åˆ›å»ºæ•°æ®é›†"""
        print("ğŸ”„ åˆ›å»ºæ•°æ®é›†...")
        
        self.train_dataset = CryptoQADataset(
            self.config.data_path,
            self.tokenizer,
            self.config.max_seq_length
        )
        
        print(f"âœ… è®­ç»ƒæ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œæ ·æœ¬æ•°: {len(self.train_dataset)}")
        
        # æ•°æ®é›†ç»Ÿè®¡åˆ†æ
        self.analyze_dataset_statistics()
    
    def create_dataloader(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        self.train_dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0,  # Windowså…¼å®¹
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œæ‰¹æ¬¡æ•°: {len(self.train_dataloader)}")
    
    def setup_optimizer_and_scheduler(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # ä¼˜åŒ–å™¨
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=0.01
        )
        
        # è®¡ç®—æ€»æ­¥æ•°
        total_steps = len(self.train_dataloader) * self.config.num_epochs // self.config.gradient_accumulation_steps
        warmup_steps = int(total_steps * self.config.warmup_ratio)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        print(f"âœ… ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è®¾ç½®å®Œæˆï¼Œæ€»æ­¥æ•°: {total_steps}, é¢„çƒ­æ­¥æ•°: {warmup_steps}")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        self.model.train()
        global_step = 0
        total_loss = 0
        
        for epoch in range(self.config.num_epochs):
            print(f"\nğŸ“Š Epoch {epoch + 1}/{self.config.num_epochs}")
            
            epoch_loss = 0
            epoch_steps = 0
            
            for step, batch in enumerate(self.train_dataloader):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(**batch)
                loss = outputs.loss / self.config.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                total_loss += loss.item()
                epoch_loss += loss.item()
                epoch_steps += 1
                
                # æ¢¯åº¦ç´¯ç§¯
                if (step + 1) % self.config.gradient_accumulation_steps == 0:
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    
                    # ä¼˜åŒ–å™¨æ­¥éª¤
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    
                    global_step += 1
                    
                    # æ—¥å¿—è®°å½•
                    if global_step % self.config.logging_steps == 0:
                        avg_loss = total_loss / self.config.logging_steps
                        lr = self.scheduler.get_last_lr()[0]
                        
                        print(f"Step {global_step}: Loss = {avg_loss:.4f}, LR = {lr:.2e}")
                        
                        # å†…å­˜ç›‘æ§
                        if self.memory_manager and torch.cuda.is_available():
                            try:
                                memory_info = self.memory_manager.get_memory_snapshot(0)
                                print(f"GPUå†…å­˜: {memory_info.allocated_memory}MB / {memory_info.total_memory}MB")
                            except Exception as e:
                                pass
                        
                        total_loss = 0
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if global_step % self.config.save_steps == 0:
                        self.save_checkpoint(global_step)
            
            # Epochç»“æŸç»Ÿè®¡
            avg_epoch_loss = epoch_loss / epoch_steps if epoch_steps > 0 else 0
            print(f"Epoch {epoch + 1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        self.save_training_statistics(global_step)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_final_model()
    
    def save_checkpoint(self, step: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(self.config.output_dir, f"checkpoint-{step}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        self.model.save_pretrained(checkpoint_dir)
        self.tokenizer.save_pretrained(checkpoint_dir)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        torch.save({
            'step': step,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
        }, os.path.join(checkpoint_dir, 'training_state.pt'))
        
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        final_dir = os.path.join(self.config.output_dir, "final_model")
        os.makedirs(final_dir, exist_ok=True)
        
        self.model.save_pretrained(final_dir)
        self.tokenizer.save_pretrained(final_dir)
        
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_dir}")
    
    def analyze_dataset_statistics(self):
        """åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡åˆ†æ:")
        
        # åŸºç¡€ç»Ÿè®¡
        total_samples = len(self.train_dataset.data)
        
        # éš¾åº¦åˆ†å¸ƒ
        difficulty_dist = {}
        source_file_dist = {}
        crypto_terms_dist = {}
        
        total_instruction_length = 0
        total_output_length = 0
        thinking_count = 0
        
        for item in self.train_dataset.data:
            # éš¾åº¦åˆ†å¸ƒ
            difficulty = item.get('difficulty', 1)
            difficulty_dist[difficulty] = difficulty_dist.get(difficulty, 0) + 1
            
            # æ¥æºæ–‡ä»¶åˆ†å¸ƒ
            source_file = item.get('source_file', 'unknown')
            source_file_dist[source_file] = source_file_dist.get(source_file, 0) + 1
            
            # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
            total_instruction_length += len(item.get('instruction', ''))
            total_output_length += len(item.get('output', ''))
            
            # thinkingæ•°æ®ç»Ÿè®¡
            if '<thinking>' in item.get('output', ''):
                thinking_count += 1
            
            # å¯†ç å­¦æœ¯è¯­ç»Ÿè®¡
            crypto_terms = item.get('crypto_terms', {})
            total_terms = crypto_terms.get('total_terms', 0)
            if total_terms > 0:
                crypto_terms_dist[total_terms] = crypto_terms_dist.get(total_terms, 0) + 1
        
        print(f"  ğŸ“ˆ æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  ğŸ“ å¹³å‡é—®é¢˜é•¿åº¦: {total_instruction_length / total_samples:.1f} å­—ç¬¦")
        print(f"  ğŸ“ å¹³å‡ç­”æ¡ˆé•¿åº¦: {total_output_length / total_samples:.1f} å­—ç¬¦")
        print(f"  ğŸ§  åŒ…å«thinkingçš„æ ·æœ¬: {thinking_count} ({thinking_count/total_samples*100:.1f}%)")
        
        print(f"  ğŸ“Š éš¾åº¦åˆ†å¸ƒ:")
        for difficulty in sorted(difficulty_dist.keys()):
            count = difficulty_dist[difficulty]
            percentage = count / total_samples * 100
            print(f"    éš¾åº¦{difficulty}: {count} æ ·æœ¬ ({percentage:.1f}%)")
        
        print(f"  ğŸ“ æ¥æºæ–‡ä»¶åˆ†å¸ƒ:")
        for source_file in sorted(source_file_dist.keys()):
            count = source_file_dist[source_file]
            percentage = count / total_samples * 100
            print(f"    {source_file}: {count} æ ·æœ¬ ({percentage:.1f}%)")
        
        if crypto_terms_dist:
            print(f"  ğŸ” å¯†ç å­¦æœ¯è¯­åˆ†å¸ƒ:")
            for term_count in sorted(crypto_terms_dist.keys()):
                count = crypto_terms_dist[term_count]
                print(f"    {term_count}ä¸ªæœ¯è¯­: {count} æ ·æœ¬")
    
    def save_training_statistics(self, final_step: int):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'training_config': {
                'model_name': self.config.model_name,
                'data_path': self.config.data_path,
                'max_seq_length': self.config.max_seq_length,
                'batch_size': self.config.batch_size,
                'learning_rate': self.config.learning_rate,
                'num_epochs': self.config.num_epochs,
                'lora_r': self.config.lora_r,
                'lora_alpha': self.config.lora_alpha
            },
            'dataset_stats': {
                'total_samples': len(self.train_dataset.data),
                'total_batches': len(self.train_dataloader),
                'final_training_steps': final_step
            },
            'model_stats': {
                'total_parameters': self.model.num_parameters(),
                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            },
            'training_completed_at': datetime.now().isoformat()
        }
        
        stats_file = os.path.join(self.config.output_dir, 'training_statistics.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
    
    def validate_data_quality(self):
        """éªŒè¯æ•°æ®è´¨é‡"""
        print("\nğŸ” æ•°æ®è´¨é‡éªŒè¯:")
        
        issues = []
        valid_samples = 0
        
        for i, item in enumerate(self.train_dataset.data):
            try:
                # æ£€æŸ¥å¿…è¦å­—æ®µ
                if not item.get('instruction'):
                    issues.append(f"æ ·æœ¬ {i}: ç¼ºå°‘instruction")
                    continue
                
                if not item.get('output'):
                    issues.append(f"æ ·æœ¬ {i}: ç¼ºå°‘output")
                    continue
                
                # æ£€æŸ¥thinkingæ ¼å¼
                output = item.get('output', '')
                if '<thinking>' in output:
                    if '</thinking>' not in output:
                        issues.append(f"æ ·æœ¬ {i}: thinkingæ ‡ç­¾ä¸å®Œæ•´")
                        continue
                    
                    # æ£€æŸ¥thinkingå†…å®¹æ˜¯å¦ä¸ºç©º
                    thinking_match = re.search(r'<thinking>(.*?)</thinking>', output, re.DOTALL)
                    if thinking_match and not thinking_match.group(1).strip():
                        issues.append(f"æ ·æœ¬ {i}: thinkingå†…å®¹ä¸ºç©º")
                
                # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
                if len(item.get('instruction', '')) < 5:
                    issues.append(f"æ ·æœ¬ {i}: instructionè¿‡çŸ­")
                    continue
                
                if len(output) < 10:
                    issues.append(f"æ ·æœ¬ {i}: outputè¿‡çŸ­")
                    continue
                
                # æ£€æŸ¥tokenization
                test_text = f"{item.get('instruction', '')} {output}"
                try:
                    tokens = self.tokenizer(test_text, max_length=self.config.max_seq_length, truncation=True)
                    if len(tokens['input_ids']) < 10:
                        issues.append(f"æ ·æœ¬ {i}: tokenizationç»“æœè¿‡çŸ­")
                        continue
                except Exception as e:
                    issues.append(f"æ ·æœ¬ {i}: tokenizationå¤±è´¥ - {e}")
                    continue
                
                valid_samples += 1
                
            except Exception as e:
                issues.append(f"æ ·æœ¬ {i}: éªŒè¯è¿‡ç¨‹å‡ºé”™ - {e}")
        
        print(f"  âœ… æœ‰æ•ˆæ ·æœ¬: {valid_samples}/{len(self.train_dataset.data)}")
        
        if issues:
            print(f"  âš ï¸ å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for issue in issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
                print(f"    {issue}")
            if len(issues) > 10:
                print(f"    ... è¿˜æœ‰ {len(issues) - 10} ä¸ªé—®é¢˜")
            
            # ä¿å­˜é—®é¢˜æŠ¥å‘Š
            issues_file = os.path.join(self.config.output_dir, 'data_quality_issues.txt')
            with open(issues_file, 'w', encoding='utf-8') as f:
                for issue in issues:
                    f.write(f"{issue}\n")
            print(f"  ğŸ“ é—®é¢˜æŠ¥å‘Šå·²ä¿å­˜: {issues_file}")
        else:
            print("  âœ… æ‰€æœ‰æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡")
    
    def run(self):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        try:
            print("ğŸ¯ å¼€å§‹ç›´æ¥å¾®è°ƒæµç¨‹")
            print("=" * 60)
            
            # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.load_model_and_tokenizer()
            
            # 2. åˆ›å»ºæ•°æ®é›†
            self.create_dataset()
            
            # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
            self.create_dataloader()
            
            # 4. æ•°æ®è´¨é‡éªŒè¯
            self.validate_data_quality()
            
            # 5. è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
            self.setup_optimizer_and_scheduler()
            
            # 6. å¼€å§‹è®­ç»ƒ
            self.train()
            
            print("ğŸ‰ å¾®è°ƒæµç¨‹å®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False


def check_uv_environment():
    """æ£€æŸ¥uvç¯å¢ƒ"""
    try:
        import subprocess
        result = subprocess.run(['uv', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… uvç‰ˆæœ¬: {result.stdout.strip()}")
            return True
        else:
            print("âŒ uvæœªå®‰è£…æˆ–ä¸å¯ç”¨")
            return False
    except FileNotFoundError:
        print("âŒ uvæœªæ‰¾åˆ°ï¼Œè¯·å…ˆå®‰è£…uv")
        print("   å®‰è£…å‘½ä»¤: pip install uv æˆ–è®¿é—® https://docs.astral.sh/uv/getting-started/installation/")
        return False


def install_dependencies_with_uv():
    """ä½¿ç”¨uvå®‰è£…ä¾èµ–"""
    print("ğŸ”„ ä½¿ç”¨uvå®‰è£…ä¾èµ–...")
    
    required_packages = [
        "torch",
        "transformers>=4.36.0",
        "peft>=0.7.0",
        "datasets",
        "accelerate",
        "jieba",
        "opencc-python-reimplemented",
        "numpy",
        "tqdm"
    ]
    
    try:
        import subprocess
        for package in required_packages:
            print(f"  ğŸ“¦ å®‰è£… {package}...")
            result = subprocess.run(['uv', 'pip', 'install', package], 
                                  capture_output=True, text=True)
            if result.returncode != 0:
                print(f"  âŒ å®‰è£… {package} å¤±è´¥: {result.stderr}")
                return False
            else:
                print(f"  âœ… {package} å®‰è£…æˆåŠŸ")
        
        print("âœ… æ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ ä¾èµ–å®‰è£…è¿‡ç¨‹å‡ºé”™: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç›´æ¥ä½¿ç”¨å·²å®ç°æ¨¡å—è¿›è¡Œå¾®è°ƒ (uvç‰ˆæœ¬)")
    print("=" * 60)
    
    # æ£€æŸ¥uvç¯å¢ƒ
    if not check_uv_environment():
        return False
    
    # å®‰è£…ä¾èµ–
    if not install_dependencies_with_uv():
        print("âŒ ä¾èµ–å®‰è£…å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return False
    
    # æ£€æŸ¥ç¯å¢ƒ
    print("ğŸ” æ£€æŸ¥è®­ç»ƒç¯å¢ƒ...")
    if not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    else:
        print(f"âœ… æ£€æµ‹åˆ°CUDAï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
    
    # åˆ›å»ºé…ç½®
    config = DirectTrainingConfig()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(config.data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {config.data_path}")
        return False
    
    print(f"âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨: {config.data_path}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è¿è¡Œ
    trainer = DirectTrainer(config)
    success = trainer.run()
    
    if success:
        print("ğŸ‰ å¾®è°ƒæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
        print("\nğŸ“‹ ä½¿ç”¨uvè¿è¡Œå‘½ä»¤:")
        print(f"   uv run python {__file__}")
    else:
        print("âŒ å¾®è°ƒå¤±è´¥")
    
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)