"""
è®­ç»ƒæµæ°´çº¿ç¼–æ’å™¨

æœ¬æ¨¡å—å®ç°äº†ç«¯åˆ°ç«¯è®­ç»ƒæµæ°´çº¿æ§åˆ¶å™¨ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒçš„è‡ªåŠ¨åŒ–æµç¨‹
- è®­ç»ƒçŠ¶æ€ç®¡ç†å’Œæ£€æŸ¥ç‚¹æœºåˆ¶
- å¤šGPUè®­ç»ƒè°ƒåº¦å’Œç›‘æ§
- å®Œæ•´çš„è®­ç»ƒç”Ÿå‘½å‘¨æœŸç®¡ç†
"""

import os
import json
import yaml
import logging
import subprocess
import threading
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum

from src.data_models import TrainingExample, ThinkingExample
from src.config_manager import TrainingConfig, DataConfig, SystemConfig
from src.lora_config_optimizer import LoRAMemoryProfile, MultiGPULoRAConfig
from src.parallel_config import ParallelConfig, GPUTopology, ParallelStrategy
from src.gpu_utils import GPUDetector

# Import training monitor with error handling
try:
    from src.training_monitor import TrainingMonitor
except ImportError:
    TrainingMonitor = None

# Import distributed training engine with error handling  
try:
    from src.distributed_training_engine import MultiGPUProcessManager, DistributedBackendInitializer
except ImportError:
    MultiGPUProcessManager = None
    DistributedBackendInitializer = None

# Import memory manager with error handling
try:
    from src.memory_manager import MemoryManager
except ImportError:
    MemoryManager = None


class PipelineStage(Enum):
    """æµæ°´çº¿é˜¶æ®µæšä¸¾"""
    INITIALIZATION = "initialization"
    DATA_PREPARATION = "data_preparation"
    CONFIG_GENERATION = "config_generation"
    ENVIRONMENT_SETUP = "environment_setup"
    TRAINING_EXECUTION = "training_execution"
    MONITORING = "monitoring"
    CHECKPOINT_MANAGEMENT = "checkpoint_management"
    EVALUATION = "evaluation"
    COMPLETION = "completion"
    ERROR = "error"


class PipelineStatus(Enum):
    """æµæ°´çº¿çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class PipelineCheckpoint:
    """æµæ°´çº¿æ£€æŸ¥ç‚¹"""
    checkpoint_id: str
    stage: PipelineStage
    timestamp: datetime
    state_data: Dict[str, Any]
    model_path: Optional[str] = None
    config_path: Optional[str] = None
    metrics: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "checkpoint_id": self.checkpoint_id,
            "stage": self.stage.value,
            "timestamp": self.timestamp.isoformat(),
            "state_data": self.state_data,
            "model_path": self.model_path,
            "config_path": self.config_path,
            "metrics": self.metrics
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PipelineCheckpoint':
        return cls(
            checkpoint_id=data["checkpoint_id"],
            stage=PipelineStage(data["stage"]),
            timestamp=datetime.fromisoformat(data["timestamp"]),
            state_data=data["state_data"],
            model_path=data.get("model_path"),
            config_path=data.get("config_path"),
            metrics=data.get("metrics")
        )


@dataclass
class PipelineState:
    """æµæ°´çº¿çŠ¶æ€"""
    pipeline_id: str
    status: PipelineStatus = PipelineStatus.PENDING
    current_stage: PipelineStage = PipelineStage.INITIALIZATION
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    progress: float = 0.0
    error_message: Optional[str] = None
    
    # é˜¶æ®µè¿›åº¦è·Ÿè¸ª
    stage_progress: Dict[PipelineStage, float] = field(default_factory=dict)
    stage_start_times: Dict[PipelineStage, datetime] = field(default_factory=dict)
    stage_end_times: Dict[PipelineStage, datetime] = field(default_factory=dict)
    
    # æ£€æŸ¥ç‚¹ç®¡ç†
    checkpoints: List[PipelineCheckpoint] = field(default_factory=list)
    latest_checkpoint: Optional[PipelineCheckpoint] = None
    
    @property
    def runtime(self) -> Optional[timedelta]:
        """è®¡ç®—è¿è¡Œæ—¶é—´"""
        if self.start_time is None:
            return None
        end = self.end_time or datetime.now()
        return end - self.start_time
    
    @property
    def current_stage_runtime(self) -> Optional[timedelta]:
        """è®¡ç®—å½“å‰é˜¶æ®µè¿è¡Œæ—¶é—´"""
        if self.current_stage not in self.stage_start_times:
            return None
        start = self.stage_start_times[self.current_stage]
        end = self.stage_end_times.get(self.current_stage, datetime.now())
        return end - start
    
    def update_stage(self, stage: PipelineStage, progress: float = 0.0):
        """æ›´æ–°é˜¶æ®µ"""
        # ç»“æŸå½“å‰é˜¶æ®µ
        if self.current_stage in self.stage_start_times:
            self.stage_end_times[self.current_stage] = datetime.now()
            self.stage_progress[self.current_stage] = 100.0
        
        # å¼€å§‹æ–°é˜¶æ®µ
        self.current_stage = stage
        self.stage_start_times[stage] = datetime.now()
        self.stage_progress[stage] = progress
        
        # æ›´æ–°æ€»ä½“è¿›åº¦
        self._update_overall_progress()
    
    def update_stage_progress(self, stage: PipelineStage, progress: float):
        """æ›´æ–°é˜¶æ®µè¿›åº¦"""
        self.stage_progress[stage] = progress
        self._update_overall_progress()
    
    def _update_overall_progress(self):
        """æ›´æ–°æ€»ä½“è¿›åº¦"""
        stage_weights = {
            PipelineStage.INITIALIZATION: 5,
            PipelineStage.DATA_PREPARATION: 15,
            PipelineStage.CONFIG_GENERATION: 5,
            PipelineStage.ENVIRONMENT_SETUP: 10,
            PipelineStage.TRAINING_EXECUTION: 50,
            PipelineStage.MONITORING: 5,
            PipelineStage.CHECKPOINT_MANAGEMENT: 5,
            PipelineStage.EVALUATION: 5,
            PipelineStage.COMPLETION: 0
        }
        
        total_weight = sum(stage_weights.values())
        weighted_progress = 0.0
        
        for stage, weight in stage_weights.items():
            stage_progress = self.stage_progress.get(stage, 0.0)
            weighted_progress += (stage_progress / 100.0) * weight
        
        self.progress = (weighted_progress / total_weight) * 100.0
    
    def add_checkpoint(self, checkpoint: PipelineCheckpoint):
        """æ·»åŠ æ£€æŸ¥ç‚¹"""
        self.checkpoints.append(checkpoint)
        self.latest_checkpoint = checkpoint
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "pipeline_id": self.pipeline_id,
            "status": self.status.value,
            "current_stage": self.current_stage.value,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "progress": self.progress,
            "runtime_seconds": self.runtime.total_seconds() if self.runtime else None,
            "error_message": self.error_message,
            "stage_progress": {stage.value: progress for stage, progress in self.stage_progress.items()},
            "checkpoints": [cp.to_dict() for cp in self.checkpoints],
            "latest_checkpoint": self.latest_checkpoint.to_dict() if self.latest_checkpoint else None
        }


class TrainingPipelineOrchestrator:
    """è®­ç»ƒæµæ°´çº¿ç¼–æ’å™¨"""
    
    def __init__(self, 
                 pipeline_id: str,
                 output_dir: str = "pipeline_output",
                 logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒæµæ°´çº¿ç¼–æ’å™¨
        
        Args:
            pipeline_id: æµæ°´çº¿ID
            output_dir: è¾“å‡ºç›®å½•
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.pipeline_id = pipeline_id
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logger or logging.getLogger(__name__)
        
        # æµæ°´çº¿çŠ¶æ€
        self.state = PipelineState(pipeline_id=pipeline_id)
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.gpu_detector = GPUDetector()
        self.training_monitor: Optional[TrainingMonitor] = None
        self.process_manager: Optional[MultiGPUProcessManager] = None
        
        # é…ç½®å­˜å‚¨
        self.training_config: Optional[TrainingConfig] = None
        self.data_config: Optional[DataConfig] = None
        self.lora_config: Optional[Union[LoRAMemoryProfile, MultiGPULoRAConfig]] = None
        self.parallel_config: Optional[ParallelConfig] = None
        self.system_config: Optional[SystemConfig] = None
        
        # æ•°æ®å’Œæ–‡ä»¶è·¯å¾„
        self.training_data: List[Union[TrainingExample, ThinkingExample]] = []
        self.data_files: Dict[str, str] = {}
        self.config_files: Dict[str, str] = {}
        
        # å›è°ƒå‡½æ•°
        self.stage_callbacks: Dict[PipelineStage, List[Callable]] = {}
        self.progress_callbacks: List[Callable[[PipelineState], None]] = []
        
        # æ§åˆ¶æ ‡å¿—
        self.should_stop = False
        self.should_pause = False
        
        self.logger.info(f"è®­ç»ƒæµæ°´çº¿ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ: {pipeline_id}")
    
    def add_stage_callback(self, stage: PipelineStage, callback: Callable):
        """æ·»åŠ é˜¶æ®µå›è°ƒå‡½æ•°"""
        if stage not in self.stage_callbacks:
            self.stage_callbacks[stage] = []
        self.stage_callbacks[stage].append(callback)
    
    def add_progress_callback(self, callback: Callable[[PipelineState], None]):
        """æ·»åŠ è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callbacks.append(callback)
    
    def _notify_progress(self):
        """é€šçŸ¥è¿›åº¦æ›´æ–°"""
        for callback in self.progress_callbacks:
            try:
                callback(self.state)
            except Exception as e:
                self.logger.error(f"è¿›åº¦å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    def _execute_stage_callbacks(self, stage: PipelineStage):
        """æ‰§è¡Œé˜¶æ®µå›è°ƒ"""
        if stage in self.stage_callbacks:
            for callback in self.stage_callbacks[stage]:
                try:
                    callback(self.state)
                except Exception as e:
                    self.logger.error(f"é˜¶æ®µå›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    def configure_pipeline(self,
                         training_data: List[Union[TrainingExample, ThinkingExample]],
                         training_config: TrainingConfig,
                         data_config: DataConfig,
                         lora_config: Union[LoRAMemoryProfile, MultiGPULoRAConfig],
                         parallel_config: ParallelConfig,
                         system_config: Optional[SystemConfig] = None):
        """
        é…ç½®æµæ°´çº¿
        
        Args:
            training_data: è®­ç»ƒæ•°æ®
            training_config: è®­ç»ƒé…ç½®
            data_config: æ•°æ®é…ç½®
            lora_config: LoRAé…ç½®
            parallel_config: å¹¶è¡Œé…ç½®
            system_config: ç³»ç»Ÿé…ç½®
        """
        self.training_data = training_data
        self.training_config = training_config
        self.data_config = data_config
        self.lora_config = lora_config
        self.parallel_config = parallel_config
        self.system_config = system_config or SystemConfig()
        
        self.logger.info(f"æµæ°´çº¿é…ç½®å®Œæˆï¼Œè®­ç»ƒæ•°æ®: {len(training_data)}æ¡")
    
    def run_pipeline(self, resume_from_checkpoint: Optional[str] = None) -> bool:
        """
        è¿è¡Œè®­ç»ƒæµæ°´çº¿
        
        Args:
            resume_from_checkpoint: ä»æ£€æŸ¥ç‚¹æ¢å¤
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        try:
            self.state.status = PipelineStatus.RUNNING
            self.state.start_time = datetime.now()
            self._notify_progress()
            
            self.logger.info(f"å¼€å§‹æ‰§è¡Œè®­ç»ƒæµæ°´çº¿: {self.pipeline_id}")
            
            # ä»æ£€æŸ¥ç‚¹æ¢å¤
            if resume_from_checkpoint:
                if not self._resume_from_checkpoint(resume_from_checkpoint):
                    return False
            
            # æ‰§è¡Œæµæ°´çº¿é˜¶æ®µ
            pipeline_stages = [
                (PipelineStage.INITIALIZATION, self._stage_initialization),
                (PipelineStage.DATA_PREPARATION, self._stage_data_preparation),
                (PipelineStage.CONFIG_GENERATION, self._stage_config_generation),
                (PipelineStage.ENVIRONMENT_SETUP, self._stage_environment_setup),
                (PipelineStage.TRAINING_EXECUTION, self._stage_training_execution),
                (PipelineStage.EVALUATION, self._stage_evaluation),
                (PipelineStage.COMPLETION, self._stage_completion)
            ]
            
            for stage, stage_func in pipeline_stages:
                if self.should_stop:
                    self.logger.info("æµæ°´çº¿è¢«åœæ­¢")
                    self.state.status = PipelineStatus.CANCELLED
                    return False
                
                while self.should_pause:
                    self.logger.info("æµæ°´çº¿å·²æš‚åœ")
                    self.state.status = PipelineStatus.PAUSED
                    time.sleep(1)
                
                self.state.status = PipelineStatus.RUNNING
                self.state.update_stage(stage)
                self._notify_progress()
                self._execute_stage_callbacks(stage)
                
                self.logger.info(f"å¼€å§‹æ‰§è¡Œé˜¶æ®µ: {stage.value}")
                
                if not stage_func():
                    self.logger.error(f"é˜¶æ®µæ‰§è¡Œå¤±è´¥: {stage.value}")
                    self.state.status = PipelineStatus.FAILED
                    return False
                
                # åˆ›å»ºæ£€æŸ¥ç‚¹
                self._create_checkpoint(stage)
                
                self.logger.info(f"é˜¶æ®µæ‰§è¡Œå®Œæˆ: {stage.value}")
            
            self.state.status = PipelineStatus.COMPLETED
            self.state.end_time = datetime.now()
            self._notify_progress()
            
            self.logger.info(f"è®­ç»ƒæµæ°´çº¿æ‰§è¡Œå®Œæˆ: {self.pipeline_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            self.state.status = PipelineStatus.FAILED
            self.state.error_message = str(e)
            self.state.end_time = datetime.now()
            self._notify_progress()
            return False
    
    def _stage_initialization(self) -> bool:
        """åˆå§‹åŒ–é˜¶æ®µ"""
        try:
            self.logger.info("æ‰§è¡Œåˆå§‹åŒ–é˜¶æ®µ...")
            
            # éªŒè¯é…ç½®
            if not self.training_data:
                raise ValueError("è®­ç»ƒæ•°æ®ä¸ºç©º")
            
            if not all([self.training_config, self.data_config, self.lora_config, self.parallel_config]):
                raise ValueError("é…ç½®ä¸å®Œæ•´")
            
            # æ£€æµ‹GPUç¯å¢ƒ
            gpu_infos = self.gpu_detector.get_all_gpu_info()
            if not gpu_infos:
                self.logger.warning("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
            else:
                self.logger.info(f"æ£€æµ‹åˆ°{len(gpu_infos)}ä¸ªGPU")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
            directories = ["data", "configs", "checkpoints", "logs", "models"]
            for dir_name in directories:
                (self.output_dir / dir_name).mkdir(exist_ok=True)
            
            self.state.update_stage_progress(PipelineStage.INITIALIZATION, 100.0)
            return True
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–é˜¶æ®µå¤±è´¥: {e}")
            return False
    
    def _stage_data_preparation(self) -> bool:
        """æ•°æ®å‡†å¤‡é˜¶æ®µ"""
        try:
            self.logger.info("æ‰§è¡Œæ•°æ®å‡†å¤‡é˜¶æ®µ...")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            data_output_dir = self.output_dir / "data"
            dataset_name = f"{self.pipeline_id}_dataset"
            
            self.state.update_stage_progress(PipelineStage.DATA_PREPARATION, 20.0)
            
            # ç›´æ¥å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸ä¾èµ–LlamaFactory
            self.data_files = self._prepare_direct_training_data(
                self.training_data,
                str(data_output_dir),
                dataset_name
            )
            
            self.state.update_stage_progress(PipelineStage.DATA_PREPARATION, 80.0)
            
            # éªŒè¯æ•°æ®æ–‡ä»¶
            if not self._validate_direct_training_data(self.data_files):
                raise RuntimeError("è®­ç»ƒæ•°æ®éªŒè¯å¤±è´¥")
            
            self.state.update_stage_progress(PipelineStage.DATA_PREPARATION, 100.0)
            self.logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ: {self.data_files}")
            return True
            
        except Exception as e:
            self.logger.error(f"æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥: {e}")
            return False
    
    def _stage_config_generation(self) -> bool:
        """é…ç½®ç”Ÿæˆé˜¶æ®µ"""
        try:
            self.logger.info("æ‰§è¡Œé…ç½®ç”Ÿæˆé˜¶æ®µ...")
            
            config_output_dir = self.output_dir / "configs"
            dataset_name = f"{self.pipeline_id}_dataset"
            
            self.state.update_stage_progress(PipelineStage.CONFIG_GENERATION, 30.0)
            
            # ç”Ÿæˆç›´æ¥è®­ç»ƒé…ç½®
            config_file = self._create_direct_training_config(
                self.training_config,
                self.data_config,
                self.lora_config,
                self.parallel_config,
                dataset_name,
                str(config_output_dir)
            )
            
            self.config_files["direct_training_config"] = config_file
            
            self.state.update_stage_progress(PipelineStage.CONFIG_GENERATION, 70.0)
            
            # ç”Ÿæˆç›´æ¥è®­ç»ƒè„šæœ¬
            script_file = self._generate_direct_training_script(
                config_file,
                str(self.output_dir / "direct_train_script.py")
            )
            
            self.config_files["training_script"] = script_file
            
            self.state.update_stage_progress(PipelineStage.CONFIG_GENERATION, 100.0)
            self.logger.info(f"é…ç½®ç”Ÿæˆå®Œæˆ: {self.config_files}")
            return True
            
        except Exception as e:
            self.logger.error(f"é…ç½®ç”Ÿæˆé˜¶æ®µå¤±è´¥: {e}")
            return False
    
    def _stage_environment_setup(self) -> bool:
        """ç¯å¢ƒè®¾ç½®é˜¶æ®µ - é›†æˆè‡ªç ”GPUæ£€æµ‹å’Œå†…å­˜ç®¡ç†"""
        try:
            self.logger.info("æ‰§è¡Œç¯å¢ƒè®¾ç½®é˜¶æ®µ...")
            
            # ä½¿ç”¨GPUæ£€æµ‹å™¨éªŒè¯ç¯å¢ƒ
            gpu_infos = self.gpu_detector.get_all_gpu_info()
            if not gpu_infos:
                self.logger.warning("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
            else:
                self.logger.info(f"æ£€æµ‹åˆ° {len(gpu_infos)} ä¸ªGPU")
                for gpu_info in gpu_infos:
                    self.logger.info(f"GPU {gpu_info.gpu_id}: {gpu_info.name}, "
                                   f"å†…å­˜: {gpu_info.total_memory}MB")
            
            self.state.update_stage_progress(PipelineStage.ENVIRONMENT_SETUP, 30.0)
            
            # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒå˜é‡
            if self.parallel_config.world_size > 1:
                os.environ["WORLD_SIZE"] = str(self.parallel_config.world_size)
                os.environ["MASTER_ADDR"] = self.parallel_config.master_addr
                os.environ["MASTER_PORT"] = str(self.parallel_config.master_port)
                
                # è®¾ç½®NCCLç¯å¢ƒå˜é‡
                os.environ["NCCL_DEBUG"] = "INFO"
                os.environ["NCCL_SOCKET_IFNAME"] = "^docker0,lo"
                os.environ["NCCL_TIMEOUT"] = "1800"
                
                self.logger.info(f"åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒè®¾ç½®: world_size={self.parallel_config.world_size}, "
                               f"master_addr={self.parallel_config.master_addr}, "
                               f"master_port={self.parallel_config.master_port}")
            
            self.state.update_stage_progress(PipelineStage.ENVIRONMENT_SETUP, 60.0)
            
            # åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨
            if TrainingMonitor:
                try:
                    gpu_ids = list(range(min(len(gpu_infos), self.parallel_config.world_size)))
                    self.training_monitor = TrainingMonitor(
                        gpu_ids=gpu_ids,
                        log_dir=str(self.output_dir / "logs"),
                        save_interval=100
                    )
                    self.logger.info("è®­ç»ƒç›‘æ§å™¨åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"è®­ç»ƒç›‘æ§å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.training_monitor = None
            
            self.state.update_stage_progress(PipelineStage.ENVIRONMENT_SETUP, 100.0)
            self.logger.info("ç¯å¢ƒè®¾ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"ç¯å¢ƒè®¾ç½®é˜¶æ®µå¤±è´¥: {e}")
            return False
    
    def _stage_training_execution(self) -> bool:
        """è®­ç»ƒæ‰§è¡Œé˜¶æ®µ - ä½¿ç”¨è‡ªç ”åˆ†å¸ƒå¼è®­ç»ƒå¼•æ“"""
        try:
            self.logger.info("æ‰§è¡Œè®­ç»ƒé˜¶æ®µ...")
            
            # å¯åŠ¨è®­ç»ƒç›‘æ§
            if self.training_monitor:
                self.training_monitor.start_monitoring()
            
            self.state.update_stage_progress(PipelineStage.TRAINING_EXECUTION, 10.0)
            
            # ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒå¼•æ“æ‰§è¡Œè®­ç»ƒ
            success = self._execute_distributed_training()
            
            if success:
                self.state.update_stage_progress(PipelineStage.TRAINING_EXECUTION, 100.0)
                self.logger.info("åˆ†å¸ƒå¼è®­ç»ƒæ‰§è¡Œå®Œæˆ")
                return True
            else:
                self.logger.error("åˆ†å¸ƒå¼è®­ç»ƒæ‰§è¡Œå¤±è´¥")
                return False
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæ‰§è¡Œé˜¶æ®µå¤±è´¥: {e}")
            return False
        finally:
            # åœæ­¢è®­ç»ƒç›‘æ§
            if self.training_monitor:
                self.training_monitor.stop_monitoring()
    
    def _execute_distributed_training(self) -> bool:
        """æ‰§è¡Œåˆ†å¸ƒå¼è®­ç»ƒ - ä½¿ç”¨è‡ªç ”è®­ç»ƒå¼•æ“"""
        try:
            # å¯¼å…¥åˆ†å¸ƒå¼è®­ç»ƒå¼•æ“
            from src.distributed_training_engine import MultiGPUProcessManager
            from src.memory_manager import MemoryManager
            
            # æ£€æµ‹GPUæ‹“æ‰‘
            gpu_topology = self.gpu_detector.detect_gpu_topology()
            
            # åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨
            memory_manager = MemoryManager({
                "monitoring_interval": 5,
                "enable_auto_adjustment": True,
                "initial_batch_size": self.training_config.per_device_train_batch_size
            })
            
            # å¯åŠ¨å†…å­˜ç®¡ç†å™¨
            if not memory_manager.start():
                self.logger.warning("å†…å­˜ç®¡ç†å™¨å¯åŠ¨å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ")
            
            self.state.update_stage_progress(PipelineStage.TRAINING_EXECUTION, 20.0)
            
            # å¦‚æœæ˜¯å•GPUè®­ç»ƒ
            if self.parallel_config.world_size == 1:
                return self._execute_single_gpu_training(memory_manager)
            
            # å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
            process_manager = MultiGPUProcessManager(
                config=self.parallel_config,
                topology=gpu_topology,
                logger=self.logger
            )
            
            self.state.update_stage_progress(PipelineStage.TRAINING_EXECUTION, 30.0)
            
            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
            success = process_manager.spawn_training_processes(
                self._distributed_training_worker,
                self.training_data,
                self.training_config,
                self.data_config,
                self.lora_config,
                memory_manager
            )
            
            if not success:
                self.logger.error("å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒè¿›ç¨‹å¤±è´¥")
                return False
            
            self.state.update_stage_progress(PipelineStage.TRAINING_EXECUTION, 50.0)
            
            # ç­‰å¾…è®­ç»ƒå®Œæˆ
            success = process_manager.wait_for_completion(timeout=3600 * 8)  # 8å°æ—¶è¶…æ—¶
            
            if success:
                self.state.update_stage_progress(PipelineStage.TRAINING_EXECUTION, 90.0)
                self.logger.info("åˆ†å¸ƒå¼è®­ç»ƒè¿›ç¨‹å…¨éƒ¨å®Œæˆ")
            else:
                failed_processes = process_manager.get_failed_processes()
                self.logger.error(f"åˆ†å¸ƒå¼è®­ç»ƒå¤±è´¥ï¼Œå¤±è´¥è¿›ç¨‹: {failed_processes}")
            
            # æ¸…ç†èµ„æº
            process_manager.cleanup()
            memory_manager.stop()
            
            return success
            
        except Exception as e:
            self.logger.error(f"åˆ†å¸ƒå¼è®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    def _execute_single_gpu_training(self, memory_manager) -> bool:
        """æ‰§è¡Œå•GPUè®­ç»ƒ"""
        try:
            # å¯¼å…¥ç›´æ¥è®­ç»ƒæ¨¡å—
            from direct_finetuning_with_existing_modules import DirectTrainer, DirectTrainingConfig
            
            # åˆ›å»ºè®­ç»ƒé…ç½®
            direct_config = DirectTrainingConfig(
                model_name="Qwen/Qwen3-4B-Thinking-2507",
                data_path=self.data_files.get("train_file", ""),
                output_dir=self.training_config.output_dir,
                max_seq_length=self.data_config.max_samples or 2048,
                batch_size=memory_manager.get_current_batch_size(),
                gradient_accumulation_steps=self.training_config.gradient_accumulation_steps,
                learning_rate=self.training_config.learning_rate,
                num_epochs=int(self.training_config.num_train_epochs),
                warmup_ratio=self.training_config.warmup_ratio,
                save_steps=self.training_config.save_steps,
                logging_steps=self.training_config.logging_steps,
                use_gradient_checkpointing=True,
                use_fp16=self.training_config.fp16 or self.training_config.bf16
            )
            
            # è®¾ç½®LoRAé…ç½®
            if hasattr(self.lora_config, 'global_config'):
                direct_config.lora_r = self.lora_config.global_config.rank
                direct_config.lora_alpha = self.lora_config.global_config.alpha
                direct_config.target_modules = self.lora_config.global_config.target_modules
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = DirectTrainer(direct_config)
            
            # è®¾ç½®å†…å­˜ç®¡ç†å™¨å›è°ƒ
            def memory_callback(recommendations):
                for rec in recommendations:
                    if rec.strategy.value == "reduce_batch_size":
                        new_batch_size = max(1, direct_config.batch_size // 2)
                        direct_config.batch_size = new_batch_size
                        self.logger.info(f"æ ¹æ®å†…å­˜å»ºè®®è°ƒæ•´æ‰¹æ¬¡å¤§å°ä¸º: {new_batch_size}")
            
            memory_manager.add_optimization_callback(memory_callback)
            
            # è¿è¡Œè®­ç»ƒ
            trainer.run()
            
            return True
            
        except Exception as e:
            self.logger.error(f"å•GPUè®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def _distributed_training_worker(self, rank: int, config: 'ParallelConfig', 
                                   training_data, training_config, data_config, 
                                   lora_config, memory_manager):
        """åˆ†å¸ƒå¼è®­ç»ƒå·¥ä½œè¿›ç¨‹"""
        try:
            # åˆå§‹åŒ–åˆ†å¸ƒå¼åç«¯
            from src.distributed_training_engine import DistributedBackendInitializer
            
            backend_init = DistributedBackendInitializer(config, self.logger)
            if not backend_init.initialize_backend(rank, config.world_size):
                raise RuntimeError(f"Rank {rank} åˆ†å¸ƒå¼åç«¯åˆå§‹åŒ–å¤±è´¥")
            
            # å¯¼å…¥ç›´æ¥è®­ç»ƒæ¨¡å—
            from direct_finetuning_with_existing_modules import DirectTrainer, DirectTrainingConfig
            
            # åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒé…ç½®
            direct_config = DirectTrainingConfig(
                model_name="Qwen/Qwen3-4B-Thinking-2507",
                data_path=self.data_files.get("train_file", ""),
                output_dir=f"{training_config.output_dir}/rank_{rank}",
                max_seq_length=data_config.max_samples or 2048,
                batch_size=training_config.per_device_train_batch_size,
                gradient_accumulation_steps=training_config.gradient_accumulation_steps,
                learning_rate=training_config.learning_rate,
                num_epochs=int(training_config.num_train_epochs),
                warmup_ratio=training_config.warmup_ratio,
                save_steps=training_config.save_steps,
                logging_steps=training_config.logging_steps,
                use_gradient_checkpointing=True,
                use_fp16=training_config.fp16 or training_config.bf16
            )
            
            # è®¾ç½®LoRAé…ç½®
            if hasattr(lora_config, 'global_config'):
                direct_config.lora_r = lora_config.global_config.rank
                direct_config.lora_alpha = lora_config.global_config.alpha
                direct_config.target_modules = lora_config.global_config.target_modules
            
            # åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå™¨
            trainer = DirectTrainer(direct_config)
            
            # è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
            trainer.run()
            
            # æ¸…ç†åˆ†å¸ƒå¼åç«¯
            backend_init.cleanup()
            
            return True
            
        except Exception as e:
            self.logger.error(f"Rank {rank} è®­ç»ƒå·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
            return False
    
    def _stage_evaluation(self) -> bool:
        """è¯„ä¼°é˜¶æ®µ"""
        try:
            self.logger.info("æ‰§è¡Œè¯„ä¼°é˜¶æ®µ...")
            
            # ç®€åŒ–çš„è¯„ä¼°å®ç°
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨è¯„ä¼°æ¡†æ¶
            
            self.state.update_stage_progress(PipelineStage.EVALUATION, 50.0)
            
            # æ£€æŸ¥è¾“å‡ºæ¨¡å‹æ˜¯å¦å­˜åœ¨
            model_output_dir = Path(self.training_config.output_dir)
            if model_output_dir.exists():
                self.logger.info(f"æ‰¾åˆ°è®­ç»ƒè¾“å‡º: {model_output_dir}")
            else:
                self.logger.warning("æœªæ‰¾åˆ°è®­ç»ƒè¾“å‡º")
            
            self.state.update_stage_progress(PipelineStage.EVALUATION, 100.0)
            self.logger.info("è¯„ä¼°é˜¶æ®µå®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"è¯„ä¼°é˜¶æ®µå¤±è´¥: {e}")
            return False
    
    def _stage_completion(self) -> bool:
        """å®Œæˆé˜¶æ®µ"""
        try:
            self.logger.info("æ‰§è¡Œå®Œæˆé˜¶æ®µ...")
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report = self._generate_final_report()
            
            report_file = self.output_dir / "pipeline_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æµæ°´çº¿æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"å®Œæˆé˜¶æ®µå¤±è´¥: {e}")
            return False
    
    def _create_checkpoint(self, stage: PipelineStage):
        """åˆ›å»ºæ£€æŸ¥ç‚¹"""
        try:
            checkpoint_id = f"{self.pipeline_id}_{stage.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            checkpoint = PipelineCheckpoint(
                checkpoint_id=checkpoint_id,
                stage=stage,
                timestamp=datetime.now(),
                state_data={
                    "data_files": self.data_files,
                    "config_files": self.config_files,
                    "progress": self.state.progress
                }
            )
            
            self.state.add_checkpoint(checkpoint)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_file = self.output_dir / "checkpoints" / f"{checkpoint_id}.json"
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint.to_dict(), f, ensure_ascii=False, indent=2)
            
            self.logger.debug(f"æ£€æŸ¥ç‚¹å·²åˆ›å»º: {checkpoint_id}")
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def _resume_from_checkpoint(self, checkpoint_id: str) -> bool:
        """ä»æ£€æŸ¥ç‚¹æ¢å¤"""
        try:
            checkpoint_file = self.output_dir / "checkpoints" / f"{checkpoint_id}.json"
            
            if not checkpoint_file.exists():
                self.logger.error(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_file}")
                return False
            
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            checkpoint = PipelineCheckpoint.from_dict(checkpoint_data)
            
            # æ¢å¤çŠ¶æ€
            self.data_files = checkpoint.state_data.get("data_files", {})
            self.config_files = checkpoint.state_data.get("config_files", {})
            self.state.current_stage = checkpoint.stage
            self.state.progress = checkpoint.state_data.get("progress", 0.0)
            
            self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: {checkpoint_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä»æ£€æŸ¥ç‚¹æ¢å¤å¤±è´¥: {e}")
            return False
    
    def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        return {
            "pipeline_id": self.pipeline_id,
            "execution_summary": self.state.to_dict(),
            "data_files": self.data_files,
            "config_files": self.config_files,
            "training_data_count": len(self.training_data),
            "output_directory": str(self.output_dir),
            "generated_at": datetime.now().isoformat()
        }
    
    def pause_pipeline(self):
        """æš‚åœæµæ°´çº¿"""
        self.should_pause = True
        self.logger.info("æµæ°´çº¿æš‚åœè¯·æ±‚å·²å‘é€")
    
    def resume_pipeline(self):
        """æ¢å¤æµæ°´çº¿"""
        self.should_pause = False
        self.logger.info("æµæ°´çº¿æ¢å¤è¯·æ±‚å·²å‘é€")
    
    def stop_pipeline(self):
        """åœæ­¢æµæ°´çº¿"""
        self.should_stop = True
        self.logger.info("æµæ°´çº¿åœæ­¢è¯·æ±‚å·²å‘é€")
    
    def get_state(self) -> PipelineState:
        """è·å–æµæ°´çº¿çŠ¶æ€"""
        return self.state
    
    def get_progress(self) -> float:
        """è·å–è¿›åº¦ç™¾åˆ†æ¯”"""
        return self.state.progress
    
    def is_running(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
        return self.state.status == PipelineStatus.RUNNING
    
    def is_completed(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ"""
        return self.state.status == PipelineStatus.COMPLETED
    
    def has_failed(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¤±è´¥"""
        return self.state.status == PipelineStatus.FAILED
    
    def _prepare_direct_training_data(self, 
                                    training_data: List[Union[TrainingExample, ThinkingExample]],
                                    output_dir: str,
                                    dataset_name: str) -> Dict[str, str]:
        """
        ç›´æ¥å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸ä¾èµ–LlamaFactory
        """
        import json
        from pathlib import Path
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(len(training_data) * 0.9)
        train_data = training_data[:split_idx]
        val_data = training_data[split_idx:] if split_idx < len(training_data) else []
        
        # è½¬æ¢ä¸ºç›´æ¥è®­ç»ƒæ ¼å¼
        def convert_to_direct_format(examples):
            converted = []
            for example in examples:
                if isinstance(example, ThinkingExample):
                    # å¤„ç†ThinkingExample
                    output = example.final_response
                    if example.thinking_process:
                        output = f"<thinking>\n{example.thinking_process}\n</thinking>\n\n{example.final_response}"
                    
                    converted.append({
                        "instruction": example.instruction,
                        "input": "",
                        "output": output,
                        "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯†ç å­¦ä¸“å®¶ï¼Œè¯·ä»”ç»†æ€è€ƒåå›ç­”é—®é¢˜ã€‚"
                    })
                else:
                    # å¤„ç†TrainingExample
                    output = example.output
                    if example.thinking:
                        output = f"<thinking>\n{example.thinking}\n</thinking>\n\n{example.output}"
                    
                    converted.append({
                        "instruction": example.instruction,
                        "input": example.input,
                        "output": output,
                        "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯†ç å­¦ä¸“å®¶ï¼Œè¯·ä»”ç»†æ€è€ƒåå›ç­”é—®é¢˜ã€‚"
                    })
            return converted
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        train_file = output_path / f"{dataset_name}_train.json"
        with open(train_file, 'w', encoding='utf-8') as f:
            json.dump(convert_to_direct_format(train_data), f, ensure_ascii=False, indent=2)
        
        result = {"train_file": str(train_file)}
        
        # ä¿å­˜éªŒè¯æ•°æ®
        if val_data:
            val_file = output_path / f"{dataset_name}_val.json"
            with open(val_file, 'w', encoding='utf-8') as f:
                json.dump(convert_to_direct_format(val_data), f, ensure_ascii=False, indent=2)
            result["val_file"] = str(val_file)
        
        self.logger.info(f"ç›´æ¥è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {result}")
        return result
    
    def _validate_direct_training_data(self, data_files: Dict[str, str]) -> bool:
        """
        éªŒè¯ç›´æ¥è®­ç»ƒæ•°æ®
        """
        import json
        
        try:
            for file_type, file_path in data_files.items():
                if file_type in ["train_file", "val_file"]:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if not isinstance(data, list) or len(data) == 0:
                        self.logger.error(f"{file_type} æ ¼å¼é”™è¯¯æˆ–ä¸ºç©º")
                        return False
                    
                    # æ£€æŸ¥å¿…è¦å­—æ®µ
                    for i, item in enumerate(data[:5]):  # æ£€æŸ¥å‰5ä¸ªæ ·æœ¬
                        required_fields = ["instruction", "output"]
                        for field in required_fields:
                            if field not in item or not item[field]:
                                self.logger.error(f"{file_type} ç¬¬{i}ä¸ªæ ·æœ¬ç¼ºå°‘{field}å­—æ®µ")
                                return False
            
            self.logger.info("ç›´æ¥è®­ç»ƒæ•°æ®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _create_direct_training_config(self,
                                     training_config: TrainingConfig,
                                     data_config: DataConfig,
                                     lora_config: Union[LoRAMemoryProfile, MultiGPULoRAConfig],
                                     parallel_config: ParallelConfig,
                                     dataset_name: str,
                                     output_dir: str) -> str:
        """
        åˆ›å»ºç›´æ¥è®­ç»ƒé…ç½®æ–‡ä»¶
        """
        import yaml
        from pathlib import Path
        
        # æå–LoRAå‚æ•°
        if isinstance(lora_config, LoRAMemoryProfile):
            lora_rank = lora_config.rank
            lora_alpha = lora_config.alpha
            target_modules = lora_config.target_modules
        else:
            lora_rank = lora_config.global_config.rank
            lora_alpha = lora_config.global_config.alpha
            target_modules = lora_config.global_config.target_modules
        
        # ä½¿ç”¨GPUæ£€æµ‹å™¨è·å–ç¡¬ä»¶ä¿¡æ¯
        gpu_infos = self.gpu_detector.get_all_gpu_info()
        total_gpu_memory = sum(gpu.total_memory for gpu in gpu_infos) if gpu_infos else 0
        
        # æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°
        recommended_batch_size = training_config.per_device_train_batch_size
        if gpu_infos and total_gpu_memory > 0:
            # å¯¹äºQwen3-4Bæ¨¡å‹ï¼Œæ¯GB GPUå†…å­˜å¤§çº¦å¯ä»¥æ”¯æŒbatch_size=1
            max_batch_per_gpu = max(1, gpu_infos[0].total_memory // 8192)  # 8GB per batch
            recommended_batch_size = min(recommended_batch_size, max_batch_per_gpu)
        
        # åˆ›å»ºç›´æ¥è®­ç»ƒé…ç½® - é›†æˆè‡ªç ”æ¨¡å—é…ç½®
        config = {
            "model_name": "Qwen/Qwen3-4B-Thinking-2507",
            "data_path": str(self.data_files.get("train_file", "")),
            "val_data_path": str(self.data_files.get("val_file", "")),
            "output_dir": training_config.output_dir,
            "max_seq_length": data_config.max_samples or 2048,
            "batch_size": recommended_batch_size,
            "gradient_accumulation_steps": training_config.gradient_accumulation_steps,
            "learning_rate": training_config.learning_rate,
            "num_epochs": training_config.num_train_epochs,
            "warmup_ratio": training_config.warmup_ratio,
            "save_steps": training_config.save_steps,
            "logging_steps": training_config.logging_steps,
            "lora_r": lora_rank,
            "lora_alpha": lora_alpha,
            "lora_dropout": 0.1,
            "target_modules": target_modules,
            "use_gradient_checkpointing": True,
            "use_fp16": training_config.fp16 or training_config.bf16,
            "world_size": parallel_config.world_size,
            "enable_distributed": parallel_config.world_size > 1,
            # æ·»åŠ è‡ªç ”æ¨¡å—é…ç½®
            "memory_management": {
                "enable_auto_adjustment": True,
                "monitoring_interval": 5,
                "memory_threshold_high": 0.85,
                "memory_threshold_low": 0.6
            },
            "gpu_topology": {
                "num_gpus": len(gpu_infos),
                "total_memory": total_gpu_memory,
                "enable_topology_detection": True
            },
            "distributed_config": {
                "backend": parallel_config.communication_backend.value if hasattr(parallel_config, 'communication_backend') else "nccl",
                "master_addr": parallel_config.master_addr,
                "master_port": parallel_config.master_port,
                "timeout": 1800
            }
        }
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = Path(output_dir) / f"direct_training_config_{dataset_name}.yaml"
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)
        
        self.logger.info(f"ç›´æ¥è®­ç»ƒé…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: {config_path}")
        self.logger.info(f"æ¨èæ‰¹æ¬¡å¤§å°: {recommended_batch_size} (åŸºäºGPUå†…å­˜: {total_gpu_memory}MB)")
        return str(config_path)
    
    def _generate_direct_training_script(self, config_file: str, output_file: str) -> str:
        """
        ç”Ÿæˆç›´æ¥è®­ç»ƒè„šæœ¬
        """
        script_content = '''#!/usr/bin/env python3
"""
ç›´æ¥è®­ç»ƒè„šæœ¬ - ä¸ä¾èµ–LlamaFactory
è‡ªåŠ¨ç”Ÿæˆäº: {datetime.now().isoformat()}
"""

import sys
import os
sys.path.append('src')

from direct_finetuning_with_existing_modules import DirectTrainer, DirectTrainingConfig
import yaml

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    try:
        # åŠ è½½é…ç½®
        config_file = "''' + config_file + '''"
        with open(config_file, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        training_config = DirectTrainingConfig(
            model_name=config_dict.get("model_name", "Qwen/Qwen3-4B-Thinking-2507"),
            data_path=config_dict.get("data_path", ""),
            output_dir=config_dict.get("output_dir", "output"),
            max_seq_length=config_dict.get("max_seq_length", 2048),
            batch_size=config_dict.get("batch_size", 1),
            gradient_accumulation_steps=config_dict.get("gradient_accumulation_steps", 8),
            learning_rate=config_dict.get("learning_rate", 1e-4),
            num_epochs=config_dict.get("num_epochs", 3),
            warmup_ratio=config_dict.get("warmup_ratio", 0.1),
            save_steps=config_dict.get("save_steps", 500),
            logging_steps=config_dict.get("logging_steps", 10),
            lora_r=config_dict.get("lora_r", 8),
            lora_alpha=config_dict.get("lora_alpha", 16),
            lora_dropout=config_dict.get("lora_dropout", 0.1),
            target_modules=config_dict.get("target_modules", ["q_proj", "v_proj"]),
            use_gradient_checkpointing=config_dict.get("use_gradient_checkpointing", True),
            use_fp16=config_dict.get("use_fp16", True)
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = DirectTrainer(training_config)
        
        # é›†æˆè‡ªç ”æ¨¡å—
        if config_dict.get("memory_management", {}).get("enable_auto_adjustment", False):
            print("ğŸ”§ å¯ç”¨å†…å­˜ç®¡ç†å™¨...")
            from memory_manager import MemoryManager
            memory_manager = MemoryManager(config_dict["memory_management"])
            memory_manager.start()
        
        if config_dict.get("gpu_topology", {}).get("enable_topology_detection", False):
            print("ğŸ”§ å¯ç”¨GPUæ‹“æ‰‘æ£€æµ‹...")
            from gpu_utils import GPUDetector
            gpu_detector = GPUDetector()
            topology = gpu_detector.detect_gpu_topology()
            print(f"æ£€æµ‹åˆ°GPUæ‹“æ‰‘: {topology.topology_type}")
        
        # è¿è¡Œè®­ç»ƒ
        trainer.run()
        
        print("âœ… ç›´æ¥è®­ç»ƒå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()
'''
        
        # ä¿å­˜è„šæœ¬æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        import stat
        os.chmod(output_file, stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)
        
        self.logger.info(f"ç›´æ¥è®­ç»ƒè„šæœ¬å·²ç”Ÿæˆ: {output_file}")
        return output_file


def main():
    """æµ‹è¯•è®­ç»ƒæµæ°´çº¿ç¼–æ’å™¨"""
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = [
        TrainingExample(
            instruction="ä»€ä¹ˆæ˜¯å¯†ç å­¦ï¼Ÿ",
            input="",
            output="å¯†ç å­¦æ˜¯ç ”ç©¶ä¿¡æ¯å®‰å…¨çš„ç§‘å­¦ã€‚",
            thinking="<thinking>ç”¨æˆ·è¯¢é—®å¯†ç å­¦çš„åŸºæœ¬æ¦‚å¿µã€‚</thinking>",
            crypto_terms=["å¯†ç å­¦", "ä¿¡æ¯å®‰å…¨"]
        )
    ]
    
    # åˆ›å»ºé…ç½®
    training_config = TrainingConfig(num_train_epochs=1, per_device_train_batch_size=1)
    data_config = DataConfig()
    lora_config = LoRAMemoryProfile(rank=8, alpha=16, target_modules=["q_proj"])
    parallel_config = ParallelConfig(strategy=ParallelStrategy.DATA_PARALLEL)
    
    # åˆ›å»ºæµæ°´çº¿ç¼–æ’å™¨
    orchestrator = TrainingPipelineOrchestrator("test_pipeline")
    
    # é…ç½®æµæ°´çº¿
    orchestrator.configure_pipeline(
        test_data, training_config, data_config, lora_config, parallel_config
    )
    
    # æ·»åŠ è¿›åº¦å›è°ƒ
    def progress_callback(state: PipelineState):
        print(f"è¿›åº¦: {state.progress:.1f}% - é˜¶æ®µ: {state.current_stage.value}")
    
    orchestrator.add_progress_callback(progress_callback)
    
    # è¿è¡Œæµæ°´çº¿
    success = orchestrator.run_pipeline()
    
    if success:
        print("æµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
    else:
        print("æµæ°´çº¿æ‰§è¡Œå¤±è´¥ï¼")
    
    print(f"æœ€ç»ˆçŠ¶æ€: {orchestrator.get_state().to_dict()}")


if __name__ == "__main__":
    main()