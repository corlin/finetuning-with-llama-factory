#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆCLIå·¥å…·å’Œç”¨æˆ·æ¥å£

æœ¬æ¨¡å—å®ç°äº†å‘½ä»¤è¡Œè®­ç»ƒå·¥å…·ã€é…ç½®æ–‡ä»¶æ¨¡æ¿å’ŒéªŒè¯ç­‰åŸºæœ¬åŠŸèƒ½ã€‚
ä¸ä¾èµ–richåº“ï¼Œä½¿ç”¨æ ‡å‡†åº“å®ç°åŸºæœ¬çš„ç”¨æˆ·äº¤äº’ç•Œé¢ã€‚
"""

import os
import sys
import json
import yaml
import click
import logging
import threading
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime

from src.data_models import TrainingExample, ThinkingExample, DifficultyLevel
from config_manager import TrainingConfig, DataConfig, SystemConfig
from lora_config_optimizer import LoRAMemoryProfile, LoRAOptimizationStrategy
from parallel_config import ParallelConfig, ParallelStrategy
from training_pipeline import TrainingPipelineOrchestrator, PipelineStage, PipelineStatus
# ä½¿ç”¨åŸç”ŸPyTorchè®­ç»ƒå¼•æ“
from gpu_utils import GPUDetector


class ConfigTemplate:
    """é…ç½®æ¨¡æ¿ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_training_config_template() -> Dict[str, Any]:
        """ç”Ÿæˆè®­ç»ƒé…ç½®æ¨¡æ¿"""
        return {
            "model": {
                "model_name": "Qwen/Qwen3-4B-Thinking-2507",
                "model_revision": "main",
                "trust_remote_code": True
            },
            "training": {
                "num_train_epochs": 3,
                "per_device_train_batch_size": 1,
                "per_device_eval_batch_size": 1,
                "gradient_accumulation_steps": 4,
                "learning_rate": 2e-4,
                "weight_decay": 0.01,
                "warmup_ratio": 0.1,
                "lr_scheduler_type": "cosine",
                "fp16": False,
                "bf16": True,
                "save_strategy": "steps",
                "save_steps": 500,
                "eval_strategy": "steps",
                "eval_steps": 500,
                "logging_steps": 10,
                "max_seq_length": 2048,
                "seed": 42
            },
            "lora": {
                "rank": 8,
                "alpha": 16,
                "dropout": 0.1,
                "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],
                "optimization_strategy": "auto"
            },
            "data": {
                "train_split_ratio": 0.9,
                "eval_split_ratio": 0.1,
                "max_samples": None,
                "shuffle_data": True,
                "data_format": "alpaca"
            },
            "parallel": {
                "strategy": "data_parallel",
                "world_size": 1,
                "enable_distributed": False
            },
            "system": {
                "output_dir": "./output",
                "logging_dir": "./logs",
                "cache_dir": "./cache",
                "log_level": "INFO"
            }
        }
    
    @staticmethod
    def save_template(template: Dict[str, Any], output_file: str):
        """ä¿å­˜é…ç½®æ¨¡æ¿"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(template, f, default_flow_style=False, allow_unicode=True, indent=2)
        
        print(f"âœ“ é…ç½®æ¨¡æ¿å·²ä¿å­˜åˆ°: {output_path}")


class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""
    
    @staticmethod
    def validate_config_file(config_file: str) -> Tuple[bool, List[str]]:
        """éªŒè¯é…ç½®æ–‡ä»¶"""
        errors = []
        
        try:
            if not Path(config_file).exists():
                errors.append(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
                return False, errors
            
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # éªŒè¯å¿…éœ€çš„é…ç½®èŠ‚
            required_sections = ["model", "training", "lora", "data", "system"]
            for section in required_sections:
                if section not in config:
                    errors.append(f"ç¼ºå°‘é…ç½®èŠ‚: {section}")
            
            # éªŒè¯æ¨¡å‹é…ç½®
            if "model" in config:
                model_config = config["model"]
                if "model_name" not in model_config:
                    errors.append("model.model_name æ˜¯å¿…éœ€çš„")
            
            # éªŒè¯è®­ç»ƒé…ç½®
            if "training" in config:
                training_config = config["training"]
                
                # æ£€æŸ¥æ•°å€¼èŒƒå›´
                if "learning_rate" in training_config:
                    lr = training_config["learning_rate"]
                    if not isinstance(lr, (int, float)) or lr <= 0:
                        errors.append("training.learning_rate å¿…é¡»æ˜¯æ­£æ•°")
                
                if "num_train_epochs" in training_config:
                    epochs = training_config["num_train_epochs"]
                    if not isinstance(epochs, int) or epochs <= 0:
                        errors.append("training.num_train_epochs å¿…é¡»æ˜¯æ­£æ•´æ•°")
            
            # éªŒè¯LoRAé…ç½®
            if "lora" in config:
                lora_config = config["lora"]
                
                if "rank" in lora_config:
                    rank = lora_config["rank"]
                    if not isinstance(rank, int) or rank <= 0:
                        errors.append("lora.rank å¿…é¡»æ˜¯æ­£æ•´æ•°")
                
                if "alpha" in lora_config:
                    alpha = lora_config["alpha"]
                    if not isinstance(alpha, int) or alpha <= 0:
                        errors.append("lora.alpha å¿…é¡»æ˜¯æ­£æ•´æ•°")
            
            # éªŒè¯æ•°æ®é…ç½®
            if "data" in config:
                data_config = config["data"]
                
                if "train_split_ratio" in data_config:
                    ratio = data_config["train_split_ratio"]
                    if not isinstance(ratio, (int, float)) or not 0 < ratio < 1:
                        errors.append("data.train_split_ratio å¿…é¡»åœ¨0å’Œ1ä¹‹é—´")
            
            return len(errors) == 0, errors
            
        except Exception as e:
            errors.append(f"é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")
            return False, errors


class SimpleProgressDisplay:
    """ç®€å•çš„è¿›åº¦æ˜¾ç¤ºå™¨"""
    
    def __init__(self):
        self.is_running = False
        self.current_state = None
        
    def start_display(self, orchestrator: TrainingPipelineOrchestrator):
        """å¯åŠ¨è¿›åº¦æ˜¾ç¤º"""
        self.is_running = True
        
        print("=" * 60)
        print("è®­ç»ƒæµæ°´çº¿ç›‘æ§")
        print("=" * 60)
        
        while self.is_running:
            state = orchestrator.get_state()
            self.current_state = state
            
            # æ¸…å±å¹¶æ˜¾ç¤ºçŠ¶æ€
            os.system('cls' if os.name == 'nt' else 'clear')
            
            print(f"æµæ°´çº¿ID: {state.pipeline_id}")
            print(f"çŠ¶æ€: {state.status.value.upper()}")
            print(f"å½“å‰é˜¶æ®µ: {state.current_stage.value}")
            print(f"æ€»ä½“è¿›åº¦: {state.progress:.1f}%")
            
            if state.runtime:
                runtime = state.runtime.total_seconds()
                hours = int(runtime // 3600)
                minutes = int((runtime % 3600) // 60)
                seconds = int(runtime % 60)
                print(f"è¿è¡Œæ—¶é—´: {hours:02d}:{minutes:02d}:{seconds:02d}")
            
            # æ˜¾ç¤ºè¿›åº¦æ¡
            progress_bar = "â–ˆ" * int(state.progress / 5) + "â–‘" * (20 - int(state.progress / 5))
            print(f"è¿›åº¦: [{progress_bar}] {state.progress:.1f}%")
            
            # æ˜¾ç¤ºé˜¶æ®µçŠ¶æ€
            print("\né˜¶æ®µçŠ¶æ€:")
            for stage in PipelineStage:
                if stage == PipelineStage.ERROR:
                    continue
                    
                stage_progress = state.stage_progress.get(stage, 0.0)
                if stage == state.current_stage:
                    status_icon = "ğŸ”„"
                elif stage_progress >= 100.0:
                    status_icon = "âœ…"
                elif stage_progress > 0.0:
                    status_icon = "â³"
                else:
                    status_icon = "â­•"
                
                print(f"  {status_icon} {stage.value}: {stage_progress:.0f}%")
            
            if state.error_message:
                print(f"\né”™è¯¯ä¿¡æ¯: {state.error_message}")
            
            print("\næŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
            
            time.sleep(2)
    
    def stop_display(self):
        """åœæ­¢è¿›åº¦æ˜¾ç¤º"""
        self.is_running = False


# CLIå‘½ä»¤ç»„
@click.group()
@click.option('--verbose', '-v', is_flag=True, help='å¯ç”¨è¯¦ç»†è¾“å‡º')
@click.option('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
@click.pass_context
def cli(ctx, verbose, config):
    """Qwen3-4B-Thinking å¯†ç å­¦å¾®è°ƒå·¥å…·"""
    ctx.ensure_object(dict)
    ctx.obj['verbose'] = verbose
    ctx.obj['config'] = config
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


@cli.command()
@click.option('--output', '-o', default='config.yaml', help='è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„')
def init_config(output):
    """ç”Ÿæˆé…ç½®æ–‡ä»¶æ¨¡æ¿"""
    print("ğŸ”§ ç”Ÿæˆé…ç½®æ–‡ä»¶æ¨¡æ¿...")
    
    template = ConfigTemplate.generate_training_config_template()
    ConfigTemplate.save_template(template, output)
    
    print(f"\nğŸ“ é…ç½®æ–‡ä»¶æ¨¡æ¿å·²ç”Ÿæˆ: {output}")
    print("è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶åä½¿ç”¨ 'train' å‘½ä»¤å¼€å§‹è®­ç»ƒ")


@cli.command()
@click.argument('config_file')
def validate_config(config_file):
    """éªŒè¯é…ç½®æ–‡ä»¶"""
    print(f"ğŸ” éªŒè¯é…ç½®æ–‡ä»¶: {config_file}")
    
    is_valid, errors = ConfigValidator.validate_config_file(config_file)
    
    if is_valid:
        print("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
    else:
        print("âŒ é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"  â€¢ {error}")
        sys.exit(1)


@cli.command()
@click.argument('data_file')
@click.argument('config_file')
@click.option('--output-dir', '-o', default='./training_output', help='è¾“å‡ºç›®å½•')
@click.option('--pipeline-id', help='æµæ°´çº¿ID')
@click.option('--resume', help='ä»æ£€æŸ¥ç‚¹æ¢å¤')
@click.option('--dry-run', is_flag=True, help='ä»…éªŒè¯é…ç½®ï¼Œä¸æ‰§è¡Œè®­ç»ƒ')
@click.pass_context
def train(ctx, data_file, config_file, output_dir, pipeline_id, resume, dry_run):
    """å¼€å§‹è®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹...")
    
    # éªŒè¯é…ç½®æ–‡ä»¶
    is_valid, errors = ConfigValidator.validate_config_file(config_file)
    if not is_valid:
        print("âŒ é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"  â€¢ {error}")
        sys.exit(1)
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    if not Path(data_file).exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        sys.exit(1)
    
    # åŠ è½½é…ç½®
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print(f"ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®: {data_file}")
    training_data = load_training_data(data_file)
    print(f"âœ“ åŠ è½½äº† {len(training_data)} æ¡è®­ç»ƒæ•°æ®")
    
    if dry_run:
        print("ğŸ” ä»…éªŒè¯æ¨¡å¼ï¼Œä¸æ‰§è¡Œå®é™…è®­ç»ƒ")
        print("âœ… é…ç½®å’Œæ•°æ®éªŒè¯é€šè¿‡")
        return
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    training_config = create_training_config(config)
    data_config = create_data_config(config)
    lora_config = create_lora_config(config)
    parallel_config = create_parallel_config(config)
    system_config = create_system_config(config)
    
    # ç”Ÿæˆæµæ°´çº¿ID
    if not pipeline_id:
        pipeline_id = f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # åˆ›å»ºæµæ°´çº¿ç¼–æ’å™¨
    orchestrator = TrainingPipelineOrchestrator(pipeline_id, output_dir)
    orchestrator.configure_pipeline(
        training_data, training_config, data_config, lora_config, parallel_config, system_config
    )
    
    # å¯åŠ¨è¿›åº¦æ˜¾ç¤º
    progress_display = SimpleProgressDisplay()
    display_thread = threading.Thread(
        target=progress_display.start_display,
        args=(orchestrator,),
        daemon=True
    )
    display_thread.start()
    
    try:
        # è¿è¡Œè®­ç»ƒæµæ°´çº¿
        success = orchestrator.run_pipeline(resume_from_checkpoint=resume)
        
        progress_display.stop_display()
        
        if success:
            print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥ï¼")
            if orchestrator.get_state().error_message:
                print(f"é”™è¯¯ä¿¡æ¯: {orchestrator.get_state().error_message}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        orchestrator.stop_pipeline()
        progress_display.stop_display()
        sys.exit(0)


@cli.command()
@click.argument('pipeline_dir')
def status(pipeline_dir):
    """æŸ¥çœ‹è®­ç»ƒçŠ¶æ€"""
    print(f"ğŸ“Š æŸ¥çœ‹è®­ç»ƒçŠ¶æ€: {pipeline_dir}")
    
    # æŸ¥æ‰¾æœ€æ–°çš„çŠ¶æ€æ–‡ä»¶
    pipeline_path = Path(pipeline_dir)
    if not pipeline_path.exists():
        print(f"âŒ æµæ°´çº¿ç›®å½•ä¸å­˜åœ¨: {pipeline_dir}")
        sys.exit(1)
    
    # æŸ¥æ‰¾æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoints_dir = pipeline_path / "checkpoints"
    if checkpoints_dir.exists():
        checkpoint_files = list(checkpoints_dir.glob("*.json"))
        if checkpoint_files:
            # è·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹
            latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
            
            with open(latest_checkpoint, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
            print("\nè®­ç»ƒçŠ¶æ€:")
            print(f"  æ£€æŸ¥ç‚¹ID: {checkpoint_data['checkpoint_id']}")
            print(f"  é˜¶æ®µ: {checkpoint_data['stage']}")
            print(f"  æ—¶é—´: {checkpoint_data['timestamp']}")
            print(f"  è¿›åº¦: {checkpoint_data['state_data'].get('progress', 0):.1f}%")
        else:
            print("âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
    else:
        print("âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨")


@cli.command()
def list_gpus():
    """åˆ—å‡ºå¯ç”¨çš„GPU"""
    print("ğŸ” æ£€æµ‹GPUè®¾å¤‡...")
    
    detector = GPUDetector()
    gpu_infos = detector.get_all_gpu_info()
    
    if not gpu_infos:
        print("âŒ æœªæ£€æµ‹åˆ°GPUè®¾å¤‡")
        sys.exit(0)
    
    print("\nGPUè®¾å¤‡ä¿¡æ¯:")
    print("-" * 80)
    print(f"{'ID':<4} {'åç§°':<30} {'å†…å­˜':<20} {'åˆ©ç”¨ç‡':<10} {'æ¸©åº¦':<10}")
    print("-" * 80)
    
    for gpu in gpu_infos:
        memory_text = f"{gpu.used_memory}MB / {gpu.total_memory}MB"
        utilization_text = f"{gpu.utilization:.1f}%"
        temperature_text = f"{gpu.temperature:.1f}Â°C" if gpu.temperature and gpu.temperature > 0 else "N/A"
        
        print(f"{gpu.gpu_id:<4} {gpu.name:<30} {memory_text:<20} {utilization_text:<10} {temperature_text:<10}")


@cli.command()
@click.argument('data_file')
@click.option('--format', 'data_format', default='json', help='æ•°æ®æ ¼å¼ (json, jsonl)')
@click.option('--sample', type=int, help='æ˜¾ç¤ºæ ·æœ¬æ•°é‡')
def inspect_data(data_file, data_format, sample):
    """æ£€æŸ¥è®­ç»ƒæ•°æ®"""
    print(f"ğŸ” æ£€æŸ¥è®­ç»ƒæ•°æ®: {data_file}")
    
    if not Path(data_file).exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        sys.exit(1)
    
    try:
        training_data = load_training_data(data_file)
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        print("\næ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(training_data)}")
        
        # ç»Ÿè®¡éš¾åº¦çº§åˆ«
        difficulty_counts = {}
        thinking_count = 0
        crypto_terms_count = 0
        
        for example in training_data:
            # éš¾åº¦çº§åˆ«ç»Ÿè®¡
            difficulty = example.difficulty_level.name
            difficulty_counts[difficulty] = difficulty_counts.get(difficulty, 0) + 1
            
            # thinkingæ•°æ®ç»Ÿè®¡
            if example.has_thinking():
                thinking_count += 1
            
            # å¯†ç å­¦æœ¯è¯­ç»Ÿè®¡
            if example.crypto_terms:
                crypto_terms_count += len(example.crypto_terms)
        
        print(f"  åŒ…å«thinkingæ•°æ®: {thinking_count}")
        print(f"  å¯†ç å­¦æœ¯è¯­æ€»æ•°: {crypto_terms_count}")
        
        for difficulty, count in difficulty_counts.items():
            print(f"  éš¾åº¦-{difficulty}: {count}")
        
        # æ˜¾ç¤ºæ ·æœ¬
        if sample and sample > 0:
            print(f"\næ˜¾ç¤ºå‰ {min(sample, len(training_data))} ä¸ªæ ·æœ¬:")
            print("=" * 60)
            
            for i, example in enumerate(training_data[:sample]):
                print(f"\næ ·æœ¬ {i+1}:")
                print(f"  æŒ‡ä»¤: {example.instruction}")
                if example.input:
                    print(f"  è¾“å…¥: {example.input}")
                print(f"  è¾“å‡º: {example.output[:200]}...")
                if example.has_thinking():
                    print(f"  æ€è€ƒ: {example.thinking[:100]}...")
                print("-" * 40)
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ£€æŸ¥å¤±è´¥: {e}")
        sys.exit(1)


# è¾…åŠ©å‡½æ•°
def load_training_data(data_file: str) -> List[TrainingExample]:
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    data_path = Path(data_file)
    
    if data_path.suffix == '.json':
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    elif data_path.suffix == '.jsonl':
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line))
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_path.suffix}")
    
    # è½¬æ¢ä¸ºTrainingExampleå¯¹è±¡
    training_examples = []
    for item in data:
        if isinstance(item, dict):
            example = TrainingExample(
                instruction=item.get('instruction', ''),
                input=item.get('input', ''),
                output=item.get('output', ''),
                thinking=item.get('thinking'),
                crypto_terms=item.get('crypto_terms', []),
                difficulty_level=DifficultyLevel(item.get('difficulty', 2))
            )
            training_examples.append(example)
    
    return training_examples


def create_training_config(config: Dict[str, Any]) -> TrainingConfig:
    """åˆ›å»ºè®­ç»ƒé…ç½®"""
    training_section = config.get('training', {})
    
    return TrainingConfig(
        num_train_epochs=training_section.get('num_train_epochs', 3),
        per_device_train_batch_size=training_section.get('per_device_train_batch_size', 1),
        per_device_eval_batch_size=training_section.get('per_device_eval_batch_size', 1),
        gradient_accumulation_steps=training_section.get('gradient_accumulation_steps', 4),
        learning_rate=training_section.get('learning_rate', 2e-4),
        weight_decay=training_section.get('weight_decay', 0.01),
        warmup_ratio=training_section.get('warmup_ratio', 0.1),
        lr_scheduler_type=training_section.get('lr_scheduler_type', 'cosine'),
        fp16=training_section.get('fp16', False),
        bf16=training_section.get('bf16', True),
        save_strategy=training_section.get('save_strategy', 'steps'),
        save_steps=training_section.get('save_steps', 500),
        eval_strategy=training_section.get('eval_strategy', 'steps'),
        eval_steps=training_section.get('eval_steps', 500),
        logging_steps=training_section.get('logging_steps', 10),
        seed=training_section.get('seed', 42)
    )


def create_data_config(config: Dict[str, Any]) -> DataConfig:
    """åˆ›å»ºæ•°æ®é…ç½®"""
    data_section = config.get('data', {})
    
    return DataConfig(
        train_split_ratio=data_section.get('train_split_ratio', 0.9),
        eval_split_ratio=data_section.get('eval_split_ratio', 0.1),
        max_samples=data_section.get('max_samples'),
        shuffle_data=data_section.get('shuffle_data', True),
        data_format=data_section.get('data_format', 'alpaca')
    )


def create_lora_config(config: Dict[str, Any]) -> LoRAMemoryProfile:
    """åˆ›å»ºLoRAé…ç½®"""
    lora_section = config.get('lora', {})
    
    return LoRAMemoryProfile(
        rank=lora_section.get('rank', 8),
        alpha=lora_section.get('alpha', 16),
        target_modules=lora_section.get('target_modules', ['q_proj', 'k_proj', 'v_proj', 'o_proj'])
    )


def create_parallel_config(config: Dict[str, Any]) -> ParallelConfig:
    """åˆ›å»ºå¹¶è¡Œé…ç½®"""
    parallel_section = config.get('parallel', {})
    
    strategy_map = {
        'data_parallel': ParallelStrategy.DATA_PARALLEL,
        'model_parallel': ParallelStrategy.MODEL_PARALLEL,
        'pipeline_parallel': ParallelStrategy.PIPELINE_PARALLEL,
        'hybrid_parallel': ParallelStrategy.HYBRID_PARALLEL,
        'auto': ParallelStrategy.AUTO
    }
    
    strategy = strategy_map.get(parallel_section.get('strategy', 'data_parallel'), ParallelStrategy.DATA_PARALLEL)
    
    return ParallelConfig(
        strategy=strategy,
        data_parallel_size=parallel_section.get('world_size', 1),
        enable_zero_optimization=parallel_section.get('enable_distributed', False)
    )


def create_system_config(config: Dict[str, Any]) -> SystemConfig:
    """åˆ›å»ºç³»ç»Ÿé…ç½®"""
    system_section = config.get('system', {})
    
    return SystemConfig(
        cache_dir=system_section.get('cache_dir', './cache'),
        log_level=system_section.get('log_level', 'INFO')
    )


if __name__ == '__main__':
    cli()