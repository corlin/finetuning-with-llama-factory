#!/usr/bin/env python3
"""
å¹¶è¡Œç­–ç•¥è‡ªåŠ¨æ¨èæ¨¡å—
åŸºäºç¡¬ä»¶é…ç½®è‡ªåŠ¨æ¨èæœ€ä¼˜çš„å¹¶è¡Œè®­ç»ƒç­–ç•¥
æ”¯æŒæ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œé…ç½®
"""

import logging
import torch
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum

from gpu_utils import GPUDetector, GPUTopology, InterconnectType


class ParallelStrategy(Enum):
    """å¹¶è¡Œç­–ç•¥æšä¸¾"""
    DATA_PARALLEL = "data_parallel"
    MODEL_PARALLEL = "model_parallel"
    PIPELINE_PARALLEL = "pipeline_parallel"
    HYBRID_PARALLEL = "hybrid_parallel"
    SINGLE_GPU = "single_gpu"


@dataclass
class SimpleParallelConfig:
    """ç®€åŒ–çš„å¹¶è¡Œé…ç½®"""
    data_parallel: bool = False
    model_parallel: bool = False
    pipeline_parallel: bool = False
    tensor_parallel_size: int = 1
    pipeline_parallel_size: int = 1
    data_parallel_size: int = 1
    enable_zero_optimization: bool = False
    gradient_accumulation_steps: int = 1


@dataclass
class StrategyRecommendation:
    """ç­–ç•¥æ¨èç»“æœ"""
    strategy: ParallelStrategy
    config: SimpleParallelConfig
    confidence: float  # 0-1ä¹‹é—´çš„ç½®ä¿¡åº¦
    reasoning: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    expected_performance: Dict[str, float] = field(default_factory=dict)
    
    def add_reasoning(self, reason: str):
        """æ·»åŠ æ¨èç†ç”±"""
        self.reasoning.append(reason)
    
    def add_warning(self, warning: str):
        """æ·»åŠ è­¦å‘Š"""
        self.warnings.append(warning)


@dataclass
class ModelRequirements:
    """æ¨¡å‹éœ€æ±‚é…ç½®"""
    model_name: str = "Qwen3-4B-Thinking"
    model_size_gb: float = 8.0  # æ¨¡å‹å¤§å°ï¼ˆGBï¼‰
    min_memory_per_gpu: int = 8192  # æœ€å°GPUå†…å­˜éœ€æ±‚ï¼ˆMBï¼‰
    recommended_memory_per_gpu: int = 16384  # æ¨èGPUå†…å­˜ï¼ˆMBï¼‰
    max_sequence_length: int = 2048
    vocab_size: int = 151936
    hidden_size: int = 3584
    num_layers: int = 32
    num_attention_heads: int = 28
    supports_gradient_checkpointing: bool = True
    supports_mixed_precision: bool = True


class ParallelStrategyRecommender:
    """å¹¶è¡Œç­–ç•¥æ¨èå™¨"""
    
    def __init__(self, model_requirements: Optional[ModelRequirements] = None):
        self.logger = logging.getLogger(__name__)
        self.gpu_detector = GPUDetector()
        self.model_requirements = model_requirements or ModelRequirements()
        
    def recommend_strategy(self, 
                          batch_size: int = 4,
                          sequence_length: int = 2048,
                          enable_lora: bool = True,
                          lora_rank: int = 64) -> StrategyRecommendation:
        """
        æ¨èæœ€ä¼˜çš„å¹¶è¡Œè®­ç»ƒç­–ç•¥
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            sequence_length: åºåˆ—é•¿åº¦
            enable_lora: æ˜¯å¦å¯ç”¨LoRA
            lora_rank: LoRA rankå€¼
            
        Returns:
            StrategyRecommendation: æ¨èç»“æœ
        """
        # è·å–GPUæ‹“æ‰‘ä¿¡æ¯
        topology = self.gpu_detector.detect_gpu_topology()
        
        # ä¼°ç®—å†…å­˜éœ€æ±‚
        memory_requirements = self._estimate_memory_requirements(
            batch_size, sequence_length, enable_lora, lora_rank
        )
        
        # åˆ†æç¡¬ä»¶èƒ½åŠ›
        hardware_analysis = self._analyze_hardware_capabilities(topology)
        
        # ç”Ÿæˆæ¨èç­–ç•¥
        recommendation = self._generate_recommendation(
            topology, memory_requirements, hardware_analysis,
            batch_size, sequence_length, enable_lora, lora_rank
        )
        
        return recommendation
    
    def _estimate_memory_requirements(self, 
                                    batch_size: int,
                                    sequence_length: int,
                                    enable_lora: bool,
                                    lora_rank: int) -> Dict[str, float]:
        """ä¼°ç®—å†…å­˜éœ€æ±‚"""
        requirements = {}
        
        # åŸºç¡€æ¨¡å‹å†…å­˜ï¼ˆå‚æ•° + ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
        model_params_gb = self.model_requirements.model_size_gb
        optimizer_states_gb = model_params_gb * 2  # Adamä¼˜åŒ–å™¨éœ€è¦2å€å‚æ•°å†…å­˜
        
        if enable_lora:
            # LoRAå‚æ•°å†…å­˜
            lora_params_gb = (lora_rank * self.model_requirements.hidden_size * 
                            self.model_requirements.num_layers * 2) / (1024**3)
            model_params_gb += lora_params_gb
            optimizer_states_gb = lora_params_gb * 2  # åªä¼˜åŒ–LoRAå‚æ•°
        
        # æ¿€æ´»å€¼å†…å­˜ï¼ˆä¸batch_sizeå’Œsequence_lengthç›¸å…³ï¼‰
        activation_memory_gb = (batch_size * sequence_length * 
                              self.model_requirements.hidden_size * 
                              self.model_requirements.num_layers * 4) / (1024**3)
        
        # æ¢¯åº¦å†…å­˜
        gradient_memory_gb = model_params_gb
        
        # æ€»å†…å­˜éœ€æ±‚
        total_memory_gb = (model_params_gb + optimizer_states_gb + 
                          activation_memory_gb + gradient_memory_gb)
        
        requirements.update({
            "model_params_gb": model_params_gb,
            "optimizer_states_gb": optimizer_states_gb,
            "activation_memory_gb": activation_memory_gb,
            "gradient_memory_gb": gradient_memory_gb,
            "total_memory_gb": total_memory_gb,
            "per_gpu_memory_mb": total_memory_gb * 1024  # å•GPUæƒ…å†µ
        })
        
        return requirements
    
    def _analyze_hardware_capabilities(self, topology: GPUTopology) -> Dict[str, Any]:
        """åˆ†æç¡¬ä»¶èƒ½åŠ›"""
        analysis = {
            "num_gpus": topology.num_gpus,
            "total_memory_mb": 0,
            "min_memory_mb": float('inf'),
            "max_memory_mb": 0,
            "avg_memory_mb": 0,
            "has_nvlink": False,
            "has_high_bandwidth": False,
            "numa_aware": False,
            "interconnect_types": set(),
            "bandwidth_matrix": topology.bandwidth_matrix,
            "topology_score": 0.0
        }
        
        if topology.num_gpus == 0:
            return analysis
        
        # åˆ†æGPUå†…å­˜
        memory_values = []
        for gpu_info in topology.gpu_info.values():
            memory_mb = gpu_info.total_memory
            memory_values.append(memory_mb)
            analysis["total_memory_mb"] += memory_mb
            analysis["min_memory_mb"] = min(analysis["min_memory_mb"], memory_mb)
            analysis["max_memory_mb"] = max(analysis["max_memory_mb"], memory_mb)
        
        if memory_values:
            analysis["avg_memory_mb"] = sum(memory_values) / len(memory_values)
        
        # åˆ†æäº’è”èƒ½åŠ›
        for interconnect in topology.interconnects:
            analysis["interconnect_types"].add(interconnect.interconnect_type)
            
            if interconnect.interconnect_type == InterconnectType.NVLINK:
                analysis["has_nvlink"] = True
            
            if interconnect.bandwidth_gbps >= 25.0:  # é«˜å¸¦å®½é˜ˆå€¼
                analysis["has_high_bandwidth"] = True
        
        # åˆ†æNUMAæ‹“æ‰‘
        if topology.numa_topology:
            analysis["numa_aware"] = True
        
        # è®¡ç®—æ‹“æ‰‘è¯„åˆ†
        analysis["topology_score"] = self._calculate_topology_score(topology, analysis)
        
        return analysis
    
    def _calculate_topology_score(self, topology: GPUTopology, analysis: Dict[str, Any]) -> float:
        """è®¡ç®—æ‹“æ‰‘è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        score = 0.0
        
        # GPUæ•°é‡è¯„åˆ†ï¼ˆæœ€å¤š40åˆ†ï¼‰
        if topology.num_gpus == 1:
            score += 20
        elif topology.num_gpus == 2:
            score += 30
        elif topology.num_gpus <= 4:
            score += 35
        elif topology.num_gpus <= 8:
            score += 40
        else:
            score += 35  # å¤ªå¤šGPUå¯èƒ½å¸¦æ¥é€šä¿¡å¼€é”€
        
        # å†…å­˜è¯„åˆ†ï¼ˆæœ€å¤š30åˆ†ï¼‰
        if analysis["min_memory_mb"] >= self.model_requirements.recommended_memory_per_gpu:
            score += 30
        elif analysis["min_memory_mb"] >= self.model_requirements.min_memory_per_gpu:
            score += 20
        else:
            score += 10
        
        # äº’è”è¯„åˆ†ï¼ˆæœ€å¤š20åˆ†ï¼‰
        if analysis["has_nvlink"]:
            score += 20
        elif analysis["has_high_bandwidth"]:
            score += 15
        else:
            score += 5
        
        # NUMAè¯„åˆ†ï¼ˆæœ€å¤š10åˆ†ï¼‰
        if analysis["numa_aware"]:
            score += 10
        else:
            score += 5
        
        return min(score, 100.0)
    
    def _generate_recommendation(self,
                               topology: GPUTopology,
                               memory_requirements: Dict[str, float],
                               hardware_analysis: Dict[str, Any],
                               batch_size: int,
                               sequence_length: int,
                               enable_lora: bool,
                               lora_rank: int) -> StrategyRecommendation:
        """ç”Ÿæˆæ¨èç­–ç•¥"""
        
        num_gpus = topology.num_gpus
        total_memory_needed_mb = memory_requirements["per_gpu_memory_mb"]
        
        # å•GPUæƒ…å†µ
        if num_gpus <= 1:
            return self._recommend_single_gpu(
                topology, memory_requirements, hardware_analysis,
                batch_size, sequence_length, enable_lora, lora_rank
            )
        
        # å¤šGPUæƒ…å†µ
        return self._recommend_multi_gpu(
            topology, memory_requirements, hardware_analysis,
            batch_size, sequence_length, enable_lora, lora_rank
        )
    
    def _recommend_single_gpu(self,
                            topology: GPUTopology,
                            memory_requirements: Dict[str, float],
                            hardware_analysis: Dict[str, Any],
                            batch_size: int,
                            sequence_length: int,
                            enable_lora: bool,
                            lora_rank: int) -> StrategyRecommendation:
        """æ¨èå•GPUç­–ç•¥"""
        
        config = SimpleParallelConfig(
            data_parallel=False,
            model_parallel=False,
            pipeline_parallel=False,
            tensor_parallel_size=1,
            pipeline_parallel_size=1,
            data_parallel_size=1,
            enable_zero_optimization=False,
            gradient_accumulation_steps=1
        )
        
        recommendation = StrategyRecommendation(
            strategy=ParallelStrategy.SINGLE_GPU,
            config=config,
            confidence=0.9
        )
        
        # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
        if len(topology.gpu_info) > 0:
            gpu_info = list(topology.gpu_info.values())[0]
            available_memory_mb = gpu_info.total_memory
            needed_memory_mb = memory_requirements["per_gpu_memory_mb"]
            
            if available_memory_mb >= needed_memory_mb:
                recommendation.add_reasoning(f"å•GPUå†…å­˜({available_memory_mb}MB)è¶³å¤Ÿè¿è¡Œæ¨¡å‹")
                recommendation.confidence = 0.95
            else:
                recommendation.add_warning(
                    f"GPUå†…å­˜å¯èƒ½ä¸è¶³ï¼šéœ€è¦{needed_memory_mb:.0f}MBï¼Œå¯ç”¨{available_memory_mb}MB"
                )
                recommendation.add_reasoning("å»ºè®®å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦è®­ç»ƒ")
                recommendation.confidence = 0.7
                
                # è°ƒæ•´é…ç½®ä»¥èŠ‚çœå†…å­˜
                config.gradient_accumulation_steps = max(2, int(needed_memory_mb / available_memory_mb))
        
        if enable_lora:
            recommendation.add_reasoning("å¯ç”¨LoRAå¯ä»¥æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨")
        
        recommendation.expected_performance = {
            "memory_efficiency": 0.8 if enable_lora else 0.6,
            "training_speed": 1.0,
            "scalability": 0.3
        }
        
        return recommendation
    
    def _recommend_multi_gpu(self,
                           topology: GPUTopology,
                           memory_requirements: Dict[str, float],
                           hardware_analysis: Dict[str, Any],
                           batch_size: int,
                           sequence_length: int,
                           enable_lora: bool,
                           lora_rank: int) -> StrategyRecommendation:
        """æ¨èå¤šGPUç­–ç•¥"""
        
        num_gpus = topology.num_gpus
        min_memory_mb = hardware_analysis["min_memory_mb"]
        needed_memory_mb = memory_requirements["per_gpu_memory_mb"]
        has_nvlink = hardware_analysis["has_nvlink"]
        
        # åˆ¤æ–­æ˜¯å¦å¯ä»¥ä½¿ç”¨æ•°æ®å¹¶è¡Œ
        can_use_data_parallel = min_memory_mb >= needed_memory_mb
        
        if can_use_data_parallel:
            # æ¨èæ•°æ®å¹¶è¡Œ
            config = SimpleParallelConfig(
                data_parallel=True,
                model_parallel=False,
                pipeline_parallel=False,
                tensor_parallel_size=1,
                pipeline_parallel_size=1,
                data_parallel_size=num_gpus,
                enable_zero_optimization=True,
                gradient_accumulation_steps=1
            )
            
            recommendation = StrategyRecommendation(
                strategy=ParallelStrategy.DATA_PARALLEL,
                config=config,
                confidence=0.9
            )
            
            recommendation.add_reasoning(f"æ¯ä¸ªGPUå†…å­˜({min_memory_mb}MB)è¶³å¤Ÿç‹¬ç«‹è¿è¡Œæ¨¡å‹")
            recommendation.add_reasoning(f"æ•°æ®å¹¶è¡Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨{num_gpus}ä¸ªGPU")
            
            if has_nvlink:
                recommendation.add_reasoning("æ£€æµ‹åˆ°NVLinkï¼Œæ•°æ®å¹¶è¡Œé€šä¿¡æ•ˆç‡é«˜")
                recommendation.confidence = 0.95
            
            recommendation.expected_performance = {
                "memory_efficiency": 0.8,
                "training_speed": min(num_gpus * 0.85, 8.0),  # è€ƒè™‘é€šä¿¡å¼€é”€
                "scalability": 0.9
            }
            
        else:
            # æ¨èæ¨¡å‹å¹¶è¡Œæˆ–æ··åˆå¹¶è¡Œ
            if num_gpus >= 4 and has_nvlink:
                # æ¨èæ··åˆå¹¶è¡Œ
                tensor_parallel_size = min(4, num_gpus)
                data_parallel_size = num_gpus // tensor_parallel_size
                
                config = SimpleParallelConfig(
                    data_parallel=data_parallel_size > 1,
                    model_parallel=True,
                    pipeline_parallel=False,
                    tensor_parallel_size=tensor_parallel_size,
                    pipeline_parallel_size=1,
                    data_parallel_size=data_parallel_size,
                    enable_zero_optimization=True,
                    gradient_accumulation_steps=2
                )
                
                recommendation = StrategyRecommendation(
                    strategy=ParallelStrategy.HYBRID_PARALLEL,
                    config=config,
                    confidence=0.8
                )
                
                recommendation.add_reasoning("GPUå†…å­˜ä¸è¶³ä»¥æ”¯æŒçº¯æ•°æ®å¹¶è¡Œ")
                recommendation.add_reasoning(f"ä½¿ç”¨{tensor_parallel_size}è·¯å¼ é‡å¹¶è¡Œåˆ†å‰²æ¨¡å‹")
                if data_parallel_size > 1:
                    recommendation.add_reasoning(f"ç»“åˆ{data_parallel_size}è·¯æ•°æ®å¹¶è¡Œ")
                
            else:
                # æ¨èå¼ é‡å¹¶è¡Œ
                config = SimpleParallelConfig(
                    data_parallel=False,
                    model_parallel=True,
                    pipeline_parallel=False,
                    tensor_parallel_size=min(num_gpus, 4),
                    pipeline_parallel_size=1,
                    data_parallel_size=1,
                    enable_zero_optimization=True,
                    gradient_accumulation_steps=2
                )
                
                recommendation = StrategyRecommendation(
                    strategy=ParallelStrategy.MODEL_PARALLEL,
                    config=config,
                    confidence=0.75
                )
                
                recommendation.add_reasoning("GPUå†…å­˜ä¸è¶³ï¼Œéœ€è¦æ¨¡å‹å¹¶è¡Œåˆ†å‰²æ¨¡å‹")
                recommendation.add_reasoning(f"ä½¿ç”¨{config.tensor_parallel_size}è·¯å¼ é‡å¹¶è¡Œ")
            
            if not has_nvlink:
                recommendation.add_warning("ç¼ºå°‘NVLinkï¼Œæ¨¡å‹å¹¶è¡Œé€šä¿¡å¼€é”€è¾ƒå¤§")
                recommendation.confidence *= 0.9
            
            recommendation.expected_performance = {
                "memory_efficiency": 0.9,
                "training_speed": num_gpus * 0.6,  # æ¨¡å‹å¹¶è¡Œé€šä¿¡å¼€é”€è¾ƒå¤§
                "scalability": 0.7
            }
        
        return recommendation
    
    def get_optimization_suggestions(self, 
                                   recommendation: StrategyRecommendation,
                                   current_batch_size: int = 4) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºç­–ç•¥çš„å»ºè®®
        if recommendation.strategy == ParallelStrategy.SINGLE_GPU:
            suggestions.extend([
                "å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å‡å°‘å†…å­˜ä½¿ç”¨",
                "ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(FP16/BF16)",
                "è€ƒè™‘ä½¿ç”¨LoRAå¾®è°ƒå‡å°‘å‚æ•°é‡",
                "é€‚å½“å‡å°æ‰¹æ¬¡å¤§å°å¦‚æœé‡åˆ°OOM"
            ])
        
        elif recommendation.strategy == ParallelStrategy.DATA_PARALLEL:
            suggestions.extend([
                "å¯ç”¨ZeROä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡",
                "ä½¿ç”¨æ¢¯åº¦å‹ç¼©å‡å°‘é€šä¿¡å¼€é”€",
                "ç¡®ä¿æ•°æ®åŠ è½½ä¸æˆä¸ºç“¶é¢ˆ",
                "ç›‘æ§GPUåˆ©ç”¨ç‡ç¡®ä¿è´Ÿè½½å‡è¡¡"
            ])
        
        elif recommendation.strategy in [ParallelStrategy.MODEL_PARALLEL, ParallelStrategy.HYBRID_PARALLEL]:
            suggestions.extend([
                "ä¼˜åŒ–æ¨¡å‹åˆ†å‰²ç­–ç•¥å‡å°‘é€šä¿¡",
                "ä½¿ç”¨æµæ°´çº¿å¹¶è¡Œæé«˜GPUåˆ©ç”¨ç‡",
                "å¯ç”¨æ¿€æ´»å€¼æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜",
                "ç›‘æ§è·¨GPUé€šä¿¡å¸¦å®½ä½¿ç”¨"
            ])
        
        # åŸºäºè­¦å‘Šçš„å»ºè®®
        if recommendation.warnings:
            suggestions.append("æ³¨æ„è§£å†³ä»¥ä¸Šè­¦å‘Šä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        
        # åŸºäºç½®ä¿¡åº¦çš„å»ºè®®
        if recommendation.confidence < 0.8:
            suggestions.append("å»ºè®®è¿›è¡Œå°è§„æ¨¡æµ‹è¯•éªŒè¯é…ç½®æ•ˆæœ")
        
        return suggestions
    
    def compare_strategies(self, 
                         strategies: List[StrategyRecommendation]) -> StrategyRecommendation:
        """æ¯”è¾ƒå¤šä¸ªç­–ç•¥å¹¶é€‰æ‹©æœ€ä½³çš„"""
        if not strategies:
            raise ValueError("ç­–ç•¥åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if len(strategies) == 1:
            return strategies[0]
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        best_strategy = None
        best_score = -1
        
        for strategy in strategies:
            # ç»¼åˆè¯„åˆ† = ç½®ä¿¡åº¦ * 0.4 + è®­ç»ƒé€Ÿåº¦ * 0.3 + å†…å­˜æ•ˆç‡ * 0.2 + å¯æ‰©å±•æ€§ * 0.1
            performance = strategy.expected_performance
            score = (strategy.confidence * 0.4 + 
                    performance.get("training_speed", 0) / 10 * 0.3 +
                    performance.get("memory_efficiency", 0) * 0.2 +
                    performance.get("scalability", 0) * 0.1)
            
            if score > best_score:
                best_score = score
                best_strategy = strategy
        
        return best_strategy
    
    def generate_config_file(self, 
                           recommendation: StrategyRecommendation,
                           output_path: str = "parallel_config.yaml") -> str:
        """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
        import yaml
        
        config_dict = {
            "parallel_strategy": {
                "strategy_type": recommendation.strategy.value,
                "confidence": recommendation.confidence,
                "data_parallel": recommendation.config.data_parallel,
                "model_parallel": recommendation.config.model_parallel,
                "pipeline_parallel": recommendation.config.pipeline_parallel,
                "tensor_parallel_size": recommendation.config.tensor_parallel_size,
                "pipeline_parallel_size": recommendation.config.pipeline_parallel_size,
                "data_parallel_size": recommendation.config.data_parallel_size,
                "enable_zero_optimization": recommendation.config.enable_zero_optimization,
                "gradient_accumulation_steps": recommendation.config.gradient_accumulation_steps
            },
            "reasoning": recommendation.reasoning,
            "warnings": recommendation.warnings,
            "expected_performance": recommendation.expected_performance,
            "optimization_suggestions": self.get_optimization_suggestions(recommendation)
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
        
        return output_path


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•å¹¶è¡Œç­–ç•¥æ¨èåŠŸèƒ½"""
    logging.basicConfig(level=logging.INFO)
    
    recommender = ParallelStrategyRecommender()
    
    print("=== å¹¶è¡Œç­–ç•¥æ¨èæµ‹è¯• ===")
    
    # æµ‹è¯•ä¸åŒåœºæ™¯
    scenarios = [
        {"batch_size": 4, "sequence_length": 2048, "enable_lora": True, "lora_rank": 64},
        {"batch_size": 8, "sequence_length": 2048, "enable_lora": False, "lora_rank": 0},
        {"batch_size": 2, "sequence_length": 4096, "enable_lora": True, "lora_rank": 128},
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\nåœºæ™¯ {i}: {scenario}")
        recommendation = recommender.recommend_strategy(**scenario)
        
        print(f"æ¨èç­–ç•¥: {recommendation.strategy.value}")
        print(f"ç½®ä¿¡åº¦: {recommendation.confidence:.2f}")
        print(f"é…ç½®: {recommendation.config}")
        
        if recommendation.reasoning:
            print("æ¨èç†ç”±:")
            for reason in recommendation.reasoning:
                print(f"  - {reason}")
        
        if recommendation.warnings:
            print("è­¦å‘Š:")
            for warning in recommendation.warnings:
                print(f"  âš ï¸ {warning}")
        
        print(f"é¢„æœŸæ€§èƒ½: {recommendation.expected_performance}")
        
        # è·å–ä¼˜åŒ–å»ºè®®
        suggestions = recommender.get_optimization_suggestions(recommendation)
        if suggestions:
            print("ä¼˜åŒ–å»ºè®®:")
            for suggestion in suggestions:
                print(f"  ğŸ’¡ {suggestion}")


if __name__ == "__main__":
    main()