#!/usr/bin/env python3
"""
æµ‹è¯•ç›´æ¥å¾®è°ƒåŠŸèƒ½çš„ç®€åŒ–è„šæœ¬
ç”¨äºéªŒè¯å·²å®ç°æ¨¡å—çš„åŸºæœ¬åŠŸèƒ½
"""

import os
import sys
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType
import logging

# æ·»åŠ srcè·¯å¾„
sys.path.append('src')

def test_gpu_detection():
    """æµ‹è¯•GPUæ£€æµ‹åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•GPUæ£€æµ‹...")
    try:
        from gpu_utils import GPUDetector
        detector = GPUDetector()
        gpu_info = detector.detect_gpus()
        print(f"âœ… æ£€æµ‹åˆ° {len(gpu_info)} ä¸ªGPU")
        for i, gpu in enumerate(gpu_info):
            print(f"  GPU {i}: {gpu.name}, å†…å­˜: {gpu.memory_total}MB")
        return True
    except Exception as e:
        print(f"âŒ GPUæ£€æµ‹å¤±è´¥: {e}")
        return False

def test_memory_manager():
    """æµ‹è¯•å†…å­˜ç®¡ç†å™¨"""
    print("ğŸ” æµ‹è¯•å†…å­˜ç®¡ç†å™¨...")
    try:
        from memory_manager import MemoryManager
        manager = MemoryManager()
        if torch.cuda.is_available():
            snapshot = manager.get_memory_snapshot(0)
            print(f"âœ… GPUå†…å­˜å¿«ç…§: {snapshot.allocated_memory}MB / {snapshot.total_memory}MB")
        else:
            print("âœ… å†…å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼ˆCPUæ¨¡å¼ï¼‰")
        return True
    except Exception as e:
        print(f"âŒ å†…å­˜ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_chinese_processor():
    """æµ‹è¯•ä¸­æ–‡å¤„ç†å™¨"""
    print("ğŸ” æµ‹è¯•ä¸­æ–‡å¤„ç†å™¨...")
    try:
        from chinese_nlp_processor import ChineseNLPProcessor
        processor = ChineseNLPProcessor()
        
        test_text = "ä»€ä¹ˆæ˜¯å¯¹ç§°åŠ å¯†ç®—æ³•ï¼Ÿ"
        processed = processor.preprocess_text(test_text)
        print(f"âœ… ä¸­æ–‡å¤„ç†æµ‹è¯•: '{test_text}' -> '{processed}'")
        return True
    except Exception as e:
        print(f"âŒ ä¸­æ–‡å¤„ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_crypto_processor():
    """æµ‹è¯•å¯†ç å­¦æœ¯è¯­å¤„ç†å™¨"""
    print("ğŸ” æµ‹è¯•å¯†ç å­¦æœ¯è¯­å¤„ç†å™¨...")
    try:
        from crypto_term_processor import CryptoTermProcessor
        processor = CryptoTermProcessor()
        
        test_text = "AESæ˜¯ä¸€ç§å¯¹ç§°åŠ å¯†ç®—æ³•ï¼Œä½¿ç”¨ç›¸åŒçš„å¯†é’¥è¿›è¡ŒåŠ å¯†å’Œè§£å¯†ã€‚"
        terms = processor.extract_crypto_terms(test_text)
        print(f"âœ… å¯†ç å­¦æœ¯è¯­æå–: {[term.term for term in terms]}")
        return True
    except Exception as e:
        print(f"âŒ å¯†ç å­¦æœ¯è¯­å¤„ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("ğŸ” æµ‹è¯•æ•°æ®åŠ è½½...")
    try:
        data_path = "final_demo_output/data/crypto_qa_dataset_train.json"
        if not os.path.exists(data_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return False
        
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡è®­ç»ƒæ•°æ®")
        
        # æ˜¾ç¤ºç¬¬ä¸€æ¡æ•°æ®
        if data:
            sample = data[0]
            print(f"  æ ·æœ¬ç¤ºä¾‹:")
            print(f"    æŒ‡ä»¤: {sample.get('instruction', '')[:50]}...")
            print(f"    è¾“å‡º: {sample.get('output', '')[:50]}...")
        
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("ğŸ” æµ‹è¯•æ¨¡å‹åŠ è½½...")
    try:
        model_name = "Qwen/Qwen3-4B-Thinking-2507"  # ç›®æ ‡å¾®è°ƒæ¨¡å‹
        print(f"  åŠ è½½æ¨¡å‹: {model_name}")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {model.num_parameters():,}")
        
        # æµ‹è¯•LoRAé…ç½®
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=8,
            lora_alpha=16,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj"],
            bias="none"
        )
        
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        print("âœ… LoRAé…ç½®æˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_inference():
    """æµ‹è¯•ç®€å•æ¨ç†"""
    print("ğŸ” æµ‹è¯•ç®€å•æ¨ç†...")
    try:
        model_name = "Qwen/Qwen3-4B-Thinking-2507"
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        # è®¾ç½®pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # æµ‹è¯•æ¨ç†
        test_prompt = "<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯†ç å­¦ä¸“å®¶ã€‚<|im_end|>\n<|im_start|>user\nä»€ä¹ˆæ˜¯AESï¼Ÿ<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = tokenizer(test_prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=False)
        print("âœ… æ¨ç†æµ‹è¯•æˆåŠŸ")
        print(f"  è¾“å…¥: {test_prompt[:50]}...")
        print(f"  è¾“å‡º: {response[len(test_prompt):100]}...")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ æµ‹è¯•ç›´æ¥å¾®è°ƒåŠŸèƒ½")
    print("=" * 50)
    
    tests = [
        ("GPUæ£€æµ‹", test_gpu_detection),
        ("å†…å­˜ç®¡ç†å™¨", test_memory_manager),
        ("ä¸­æ–‡å¤„ç†å™¨", test_chinese_processor),
        ("å¯†ç å­¦æœ¯è¯­å¤„ç†å™¨", test_crypto_processor),
        ("æ•°æ®åŠ è½½", test_data_loading),
        ("æ¨¡å‹åŠ è½½", test_model_loading),
        ("ç®€å•æ¨ç†", test_simple_inference),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ€»ç»“
    print(f"\n{'='*20} æµ‹è¯•æ€»ç»“ {'='*20}")
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹å¾®è°ƒã€‚")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)