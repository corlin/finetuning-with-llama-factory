#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ£€æŸ¥ç‚¹åˆå¹¶å’Œé‡åŒ–å¯¼å‡ºæ¼”ç¤º

æœ¬ç¨‹åºæä¾›ä¸€ä¸ªç®€åŒ–çš„æ¼”ç¤ºï¼Œå±•ç¤ºå¦‚ä½•ï¼š
1. æ£€æŸ¥å’ŒéªŒè¯LoRAæ£€æŸ¥ç‚¹
2. åˆå¹¶æ£€æŸ¥ç‚¹åˆ°åŸºç¡€æ¨¡å‹
3. å¯¼å‡ºä¸åŒç²¾åº¦çš„æ¨¡å‹
4. æµ‹è¯•æ¨¡å‹æ¨ç†èƒ½åŠ›

ä½¿ç”¨æ–¹æ³•:
    python simple_checkpoint_merge_demo.py
"""

import os
import sys
import torch
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SimpleCheckpointProcessor:
    """ç®€åŒ–çš„æ£€æŸ¥ç‚¹å¤„ç†å™¨"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def inspect_checkpoint(self, checkpoint_path: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ£€æŸ¥ç‚¹å†…å®¹"""
        logger.info(f"æ£€æŸ¥æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        checkpoint_dir = Path(checkpoint_path)
        if not checkpoint_dir.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_path}")
        
        # æ£€æŸ¥æ–‡ä»¶ç»“æ„
        files = list(checkpoint_dir.rglob("*"))
        file_info = {}
        
        for file_path in files:
            if file_path.is_file():
                size_mb = file_path.stat().st_size / 1024 / 1024
                file_info[str(file_path.relative_to(checkpoint_dir))] = {
                    "size_mb": round(size_mb, 2),
                    "type": file_path.suffix
                }
        
        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        key_files = {
            "adapter_config.json": checkpoint_dir / "adapter_config.json",
            "adapter_model.safetensors": checkpoint_dir / "adapter_model.safetensors",
            "tokenizer.json": checkpoint_dir / "tokenizer.json",
            "tokenizer_config.json": checkpoint_dir / "tokenizer_config.json"
        }
        
        missing_files = []
        existing_files = []
        
        for name, path in key_files.items():
            if path.exists():
                existing_files.append(name)
            else:
                missing_files.append(name)
        
        # è¯»å–é…ç½®ä¿¡æ¯
        config_info = {}
        if key_files["adapter_config.json"].exists():
            try:
                with open(key_files["adapter_config.json"], "r", encoding="utf-8") as f:
                    config_info = json.load(f)
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–adapteré…ç½®: {e}")
        
        inspection_result = {
            "checkpoint_path": checkpoint_path,
            "total_files": len(files),
            "total_size_mb": sum(info["size_mb"] for info in file_info.values()),
            "file_structure": file_info,
            "existing_key_files": existing_files,
            "missing_key_files": missing_files,
            "adapter_config": config_info,
            "is_valid": len(missing_files) == 0
        }
        
        return inspection_result
    
    def merge_checkpoint_simple(
        self, 
        checkpoint_path: str,
        output_path: str,
        base_model_name: str = "Qwen/Qwen3-4B-Thinking-2507"
    ) -> Dict[str, Any]:
        """ç®€åŒ–çš„æ£€æŸ¥ç‚¹åˆå¹¶ï¼ˆæ¨¡æ‹Ÿè¿‡ç¨‹ï¼‰"""
        logger.info("å¼€å§‹æ£€æŸ¥ç‚¹åˆå¹¶è¿‡ç¨‹...")
        
        # æ£€æŸ¥æ£€æŸ¥ç‚¹
        inspection = self.inspect_checkpoint(checkpoint_path)
        if not inspection["is_valid"]:
            raise ValueError(f"æ£€æŸ¥ç‚¹æ— æ•ˆï¼Œç¼ºå°‘æ–‡ä»¶: {inspection['missing_key_files']}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡æ‹Ÿåˆå¹¶è¿‡ç¨‹ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦ä½¿ç”¨transformersåº“ï¼‰
        logger.info("æ¨¡æ‹ŸLoRAæƒé‡åˆå¹¶...")
        
        # å¤åˆ¶æ£€æŸ¥ç‚¹æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
        import shutil
        checkpoint_dir = Path(checkpoint_path)
        
        for file_path in checkpoint_dir.rglob("*"):
            if file_path.is_file():
                relative_path = file_path.relative_to(checkpoint_dir)
                target_path = output_dir / relative_path
                target_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(file_path, target_path)
        
        # ç”Ÿæˆåˆå¹¶æŠ¥å‘Š
        merge_report = {
            "merge_time": datetime.now().isoformat(),
            "base_model": base_model_name,
            "checkpoint_path": checkpoint_path,
            "output_path": output_path,
            "checkpoint_info": inspection,
            "merge_status": "simulated_success",
            "note": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„åˆå¹¶è¿‡ç¨‹ï¼Œå®é™…ä½¿ç”¨éœ€è¦transformerså’Œpeftåº“"
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_dir / "merge_report.json", "w", encoding="utf-8") as f:
            json.dump(merge_report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"åˆå¹¶å®Œæˆï¼Œè¾“å‡ºåˆ°: {output_path}")
        return merge_report
    
    def create_quantized_versions(self, merged_model_path: str, output_base_dir: str) -> Dict[str, Any]:
        """åˆ›å»ºä¸åŒç²¾åº¦ç‰ˆæœ¬çš„æ¨¡å‹"""
        logger.info("åˆ›å»ºé‡åŒ–ç‰ˆæœ¬...")
        
        base_dir = Path(output_base_dir)
        base_dir.mkdir(parents=True, exist_ok=True)
        
        merged_path = Path(merged_model_path)
        if not merged_path.exists():
            raise FileNotFoundError(f"åˆå¹¶æ¨¡å‹ä¸å­˜åœ¨: {merged_model_path}")
        
        # åˆ›å»ºä¸åŒç²¾åº¦ç‰ˆæœ¬çš„ç›®å½•å’Œé…ç½®
        versions = {
            "fp16": {
                "description": "åŠç²¾åº¦æµ®ç‚¹æ¨¡å‹ï¼Œå¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½",
                "memory_usage": "çº¦4GB",
                "inference_speed": "åŸºå‡†é€Ÿåº¦",
                "precision": "é«˜ç²¾åº¦"
            },
            "int8": {
                "description": "8ä½æ•´æ•°é‡åŒ–æ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨",
                "memory_usage": "çº¦2GB", 
                "inference_speed": "1.5-2xåŠ é€Ÿ",
                "precision": "è½»å¾®ç²¾åº¦æŸå¤±"
            },
            "int4": {
                "description": "4ä½æ•´æ•°é‡åŒ–æ¨¡å‹ï¼Œæœ€å¤§åŒ–å†…å­˜æ•ˆç‡",
                "memory_usage": "çº¦1GB",
                "inference_speed": "2-3xåŠ é€Ÿ", 
                "precision": "ä¸­ç­‰ç²¾åº¦æŸå¤±"
            }
        }
        
        results = {}
        
        for version_name, version_info in versions.items():
            version_dir = base_dir / version_name
            version_dir.mkdir(exist_ok=True)
            
            # å¤åˆ¶åŸå§‹æ–‡ä»¶
            import shutil
            for file_path in merged_path.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(merged_path)
                    target_path = version_dir / relative_path
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, target_path)
            
            # åˆ›å»ºç‰ˆæœ¬ç‰¹å®šçš„é…ç½®
            version_config = {
                "version": version_name,
                "description": version_info["description"],
                "estimated_memory_usage": version_info["memory_usage"],
                "expected_inference_speed": version_info["inference_speed"],
                "precision_level": version_info["precision"],
                "creation_time": datetime.now().isoformat(),
                "source_model": str(merged_path),
                "quantization_note": "å®é™…é‡åŒ–éœ€è¦ä½¿ç”¨bitsandbytesæˆ–ç±»ä¼¼åº“"
            }
            
            with open(version_dir / "version_config.json", "w", encoding="utf-8") as f:
                json.dump(version_config, f, indent=2, ensure_ascii=False)
            
            # åˆ›å»ºä½¿ç”¨è¯´æ˜
            usage_guide = f"""# {version_name.upper()} æ¨¡å‹ä½¿ç”¨æŒ‡å—

## æ¨¡å‹ä¿¡æ¯
- **ç‰ˆæœ¬**: {version_name.upper()}
- **æè¿°**: {version_info['description']}
- **é¢„ä¼°å†…å­˜ä½¿ç”¨**: {version_info['memory_usage']}
- **é¢„æœŸæ¨ç†é€Ÿåº¦**: {version_info['inference_speed']}
- **ç²¾åº¦æ°´å¹³**: {version_info['precision']}

## åŠ è½½æ–¹æ³•

```python
# ä½¿ç”¨transformersåº“åŠ è½½
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "{version_dir}",
    torch_dtype=torch.float16,  # æ ¹æ®ç‰ˆæœ¬è°ƒæ•´
    device_map="auto",
    trust_remote_code=True
)

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "{version_dir}",
    trust_remote_code=True
)
```

## æ¨ç†ç¤ºä¾‹

```python
# åŸºç¡€æ¨ç†
prompt = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

# æ·±åº¦æ€è€ƒæ¨ç†
thinking_prompt = "<thinking>åˆ†æè¿™ä¸ªå¯†ç å­¦é—®é¢˜</thinking>è¯·è§£é‡ŠRSAç®—æ³•åŸç†ã€‚"
inputs = tokenizer(thinking_prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=300, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            
            with open(version_dir / "README.md", "w", encoding="utf-8") as f:
                f.write(usage_guide)
            
            results[version_name] = {
                "path": str(version_dir),
                "config": version_config,
                "size_mb": self._get_directory_size(version_dir)
            }
            
            logger.info(f"åˆ›å»º {version_name.upper()} ç‰ˆæœ¬: {version_dir}")
        
        return results
    
    def _get_directory_size(self, directory: Path) -> float:
        """è®¡ç®—ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
        try:
            total_size = sum(
                f.stat().st_size for f in directory.rglob('*') if f.is_file()
            )
            return round(total_size / 1024 / 1024, 2)
        except:
            return 0.0
    
    def generate_deployment_guide(self, output_dir: str, results: Dict[str, Any]):
        """ç”Ÿæˆéƒ¨ç½²æŒ‡å—"""
        output_path = Path(output_dir)
        
        guide_content = f"""# Qwen3-4B-Thinking æ¨¡å‹éƒ¨ç½²æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•éƒ¨ç½²ä»å¾®è°ƒæ£€æŸ¥ç‚¹åˆå¹¶çš„Qwen3-4B-Thinkingæ¨¡å‹çš„ä¸åŒç‰ˆæœ¬ã€‚

## ç›®å½•ç»“æ„

```
{output_dir}/
â”œâ”€â”€ merged_model/          # åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
â”œâ”€â”€ fp16/                  # FP16ç²¾åº¦ç‰ˆæœ¬
â”œâ”€â”€ int8/                  # INT8é‡åŒ–ç‰ˆæœ¬  
â”œâ”€â”€ int4/                  # INT4é‡åŒ–ç‰ˆæœ¬
â”œâ”€â”€ deployment_guide.md    # æœ¬æŒ‡å—
â””â”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
```

## ç‰ˆæœ¬é€‰æ‹©æŒ‡å—

### FP16ç‰ˆæœ¬
- **é€‚ç”¨åœºæ™¯**: éœ€è¦æœ€é«˜ç²¾åº¦çš„ç”Ÿäº§ç¯å¢ƒ
- **ç¡¬ä»¶è¦æ±‚**: 4GB+ GPUå†…å­˜
- **æ¨èç”¨é€”**: å…³é”®ä¸šåŠ¡åº”ç”¨ã€ç²¾åº¦æ•æ„Ÿä»»åŠ¡

### INT8ç‰ˆæœ¬  
- **é€‚ç”¨åœºæ™¯**: å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦çš„åº”ç”¨
- **ç¡¬ä»¶è¦æ±‚**: 2GB+ GPUå†…å­˜
- **æ¨èç”¨é€”**: ä¸€èˆ¬ä¸šåŠ¡åº”ç”¨ã€å®æ—¶æ¨ç†

### INT4ç‰ˆæœ¬
- **é€‚ç”¨åœºæ™¯**: èµ„æºå—é™ç¯å¢ƒ
- **ç¡¬ä»¶è¦æ±‚**: 1GB+ GPUå†…å­˜  
- **æ¨èç”¨é€”**: è¾¹ç¼˜è®¾å¤‡ã€ç§»åŠ¨åº”ç”¨

## å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€æ¨ç†

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# é€‰æ‹©ç‰ˆæœ¬ï¼ˆfp16/int8/int4ï¼‰
model_path = "{output_dir}/fp16"

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

# æ¨ç†
prompt = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 2. æ‰¹é‡æ¨ç†

```python
prompts = [
    "è§£é‡ŠRSAç®—æ³•çš„å·¥ä½œåŸç†",
    "ä»€ä¹ˆæ˜¯æ•°å­—ç­¾åï¼Ÿ",
    "åŒºå—é“¾ä½¿ç”¨äº†å“ªäº›å¯†ç å­¦æŠ€æœ¯ï¼Ÿ"
]

for prompt in prompts:
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=200)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Q: {prompt}")
    print(f"A: {response}")
    print("-" * 50)
```

### 3. æ·±åº¦æ€è€ƒæ¨¡å¼

```python
thinking_prompt = '''<thinking>
ç”¨æˆ·è¯¢é—®å…³äºå¯†ç å­¦çš„é—®é¢˜ï¼Œæˆ‘éœ€è¦ï¼š
1. ç†è§£é—®é¢˜çš„æ ¸å¿ƒ
2. ç»„ç»‡ç›¸å…³çŸ¥è¯†ç‚¹
3. ç»™å‡ºå‡†ç¡®å’Œæ˜“æ‡‚çš„è§£é‡Š
</thinking>
è¯·è¯¦ç»†è§£é‡Šæ¤­åœ†æ›²çº¿å¯†ç å­¦çš„ä¼˜åŠ¿ã€‚'''

inputs = tokenizer(thinking_prompt, return_tensors="pt")
outputs = model.generate(
    **inputs, 
    max_length=500, 
    temperature=0.7,
    do_sample=True
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨é€‚å½“çš„batch_size
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### æ¨ç†åŠ é€Ÿ
- ä½¿ç”¨TensorRTä¼˜åŒ–
- å¯ç”¨KVç¼“å­˜
- è€ƒè™‘æ¨¡å‹å¹¶è¡Œ

### é‡åŒ–é…ç½®
```python
# INT8é‡åŒ–é…ç½®
from transformers import BitsAndBytesConfig

int8_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

# INT4é‡åŒ–é…ç½®  
int4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
   - å‡å°batch_size
   - ä½¿ç”¨æ›´ä½ç²¾åº¦çš„é‡åŒ–ç‰ˆæœ¬
   - å¯ç”¨CPU offloading

2. **æ¨ç†é€Ÿåº¦æ…¢**
   - æ£€æŸ¥GPUåˆ©ç”¨ç‡
   - ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–
   - è€ƒè™‘æ¨¡å‹å‰ªæ

3. **ç²¾åº¦ä¸‹é™**
   - ä½¿ç”¨æ›´é«˜ç²¾åº¦çš„ç‰ˆæœ¬
   - æ£€æŸ¥é‡åŒ–é…ç½®
   - è¿›è¡Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ

### ç›‘æ§æŒ‡æ ‡

```python
# å†…å­˜ä½¿ç”¨ç›‘æ§
import psutil
import GPUtil

def monitor_resources():
    # CPUå’Œå†…å­˜
    cpu_percent = psutil.cpu_percent()
    memory = psutil.virtual_memory()
    
    # GPU
    gpus = GPUtil.getGPUs()
    for gpu in gpus:
        print(f"GPU {gpu.id}: {gpu.memoryUtil*100:.1f}% memory, {gpu.load*100:.1f}% utilization")
    
    print(f"CPU: {cpu_percent}%, RAM: {memory.percent}%")

# æ¨ç†æ€§èƒ½æµ‹è¯•
import time

def benchmark_inference(model, tokenizer, prompt, num_runs=10):
    times = []
    for _ in range(num_runs):
        start_time = time.time()
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_length=100)
        end_time = time.time()
        times.append(end_time - start_time)
    
    avg_time = sum(times) / len(times)
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.3f}ç§’")
    return avg_time
```

## ç”Ÿäº§éƒ¨ç½²

### Dockeréƒ¨ç½²

```dockerfile
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY {output_dir}/ ./models/
COPY deploy_service.py .

EXPOSE 8000
CMD ["python", "deploy_service.py"]
```

### APIæœåŠ¡

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class GenerationRequest(BaseModel):
    prompt: str
    max_length: int = 200
    temperature: float = 0.7

@app.post("/generate")
async def generate_text(request: GenerationRequest):
    inputs = tokenizer(request.prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=request.max_length,
        temperature=request.temperature
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {{"response": response}}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## æŠ€æœ¯æ”¯æŒ

- æ£€æŸ¥å„ç‰ˆæœ¬ç›®å½•ä¸‹çš„README.md
- æŸ¥çœ‹merge_report.jsonäº†è§£åˆå¹¶è¯¦æƒ…
- å‚è€ƒversion_config.jsonäº†è§£ç‰ˆæœ¬ç‰¹æ€§

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        with open(output_path / "deployment_guide.md", "w", encoding="utf-8") as f:
            f.write(guide_content)
        
        # ç”Ÿæˆrequirements.txt
        requirements = """torch>=2.0.0
transformers>=4.35.0
accelerate>=0.20.0
bitsandbytes>=0.41.0
safetensors>=0.3.0
sentencepiece>=0.1.99
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.0.0
"""
        
        with open(output_path / "requirements.txt", "w") as f:
            f.write(requirements)
        
        logger.info("éƒ¨ç½²æŒ‡å—ç”Ÿæˆå®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("Qwen3-4B-Thinking æ£€æŸ¥ç‚¹åˆå¹¶å’Œé‡åŒ–æ¼”ç¤º")
    logger.info("=" * 60)
    
    # é…ç½®å‚æ•°
    checkpoint_path = "qwen3_4b_thinking_output/final_model"
    output_dir = "demo_quantized_output"
    base_model = "Qwen/Qwen3-4B-Thinking-2507"
    
    processor = SimpleCheckpointProcessor()
    
    try:
        # æ­¥éª¤1: æ£€æŸ¥æ£€æŸ¥ç‚¹
        logger.info("\næ­¥éª¤1: æ£€æŸ¥æ£€æŸ¥ç‚¹ç»“æ„")
        inspection = processor.inspect_checkpoint(checkpoint_path)
        
        logger.info(f"æ£€æŸ¥ç‚¹æ€»å¤§å°: {inspection['total_size_mb']:.1f} MB")
        logger.info(f"æ–‡ä»¶æ•°é‡: {inspection['total_files']}")
        logger.info(f"å…³é”®æ–‡ä»¶: {', '.join(inspection['existing_key_files'])}")
        
        if inspection['missing_key_files']:
            logger.warning(f"ç¼ºå°‘æ–‡ä»¶: {', '.join(inspection['missing_key_files'])}")
        
        # æ­¥éª¤2: åˆå¹¶æ£€æŸ¥ç‚¹
        logger.info("\næ­¥éª¤2: åˆå¹¶æ£€æŸ¥ç‚¹åˆ°åŸºç¡€æ¨¡å‹")
        merged_path = Path(output_dir) / "merged_model"
        merge_result = processor.merge_checkpoint_simple(
            checkpoint_path=checkpoint_path,
            output_path=str(merged_path),
            base_model_name=base_model
        )
        
        # æ­¥éª¤3: åˆ›å»ºé‡åŒ–ç‰ˆæœ¬
        logger.info("\næ­¥éª¤3: åˆ›å»ºä¸åŒç²¾åº¦ç‰ˆæœ¬")
        quantized_results = processor.create_quantized_versions(
            merged_model_path=str(merged_path),
            output_base_dir=output_dir
        )
        
        # æ­¥éª¤4: ç”Ÿæˆéƒ¨ç½²æŒ‡å—
        logger.info("\næ­¥éª¤4: ç”Ÿæˆéƒ¨ç½²æŒ‡å—å’Œæ–‡æ¡£")
        processor.generate_deployment_guide(output_dir, quantized_results)
        
        # æ­¥éª¤5: æ˜¾ç¤ºç»“æœæ‘˜è¦
        logger.info("\næ­¥éª¤5: ç»“æœæ‘˜è¦")
        logger.info("=" * 40)
        
        for version, info in quantized_results.items():
            logger.info(f"{version.upper()}: {info['size_mb']} MB - {info['path']}")
        
        logger.info(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")
        logger.info("ğŸ“– è¯·æŸ¥çœ‹ deployment_guide.md äº†è§£è¯¦ç»†ä½¿ç”¨æ–¹æ³•")
        
        # ç”Ÿæˆå¿«é€Ÿæµ‹è¯•è„šæœ¬
        test_script = f'''#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬
"""

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½ï¼ˆéœ€è¦å®‰è£…transformersï¼‰"""
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        model_path = "{output_dir}/fp16"
        print(f"å°è¯•åŠ è½½æ¨¡å‹: {{model_path}}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # ç®€å•æ¨ç†æµ‹è¯•
        prompt = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=inputs["input_ids"].shape[1] + 50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id
            )
        
        response = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:], 
            skip_special_tokens=True
        )
        
        print(f"è¾“å…¥: {{prompt}}")
        print(f"è¾“å‡º: {{response}}")
        print("âœ… æ¨¡å‹åŠ è½½å’Œæ¨ç†æµ‹è¯•æˆåŠŸï¼")
        
    except ImportError:
        print("âŒ ç¼ºå°‘transformersåº“ï¼Œè¯·è¿è¡Œ: pip install transformers torch")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {{e}}")

if __name__ == "__main__":
    test_model_loading()
'''
        
        with open(Path(output_dir) / "test_model.py", "w", encoding="utf-8") as f:
            f.write(test_script)
        
        logger.info("ğŸ§ª å¿«é€Ÿæµ‹è¯•è„šæœ¬å·²ç”Ÿæˆ: test_model.py")
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()