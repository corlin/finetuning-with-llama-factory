#!/usr/bin/env python3
"""
æµ‹è¯•Qwen3-4B-Thinkingæ¨¡å‹çš„ä¸“ç”¨è„šæœ¬
éªŒè¯æ¨¡å‹åŠ è½½ã€thinkingæ ¼å¼å¤„ç†å’ŒåŸºæœ¬æ¨ç†
"""

import os
import sys
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging

# æ·»åŠ srcè·¯å¾„
sys.path.append('src')

def test_model_availability():
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    print("ğŸ” æµ‹è¯•Qwen3-4B-Thinkingæ¨¡å‹å¯ç”¨æ€§...")
    
    model_name = "Qwen/Qwen3-4B-Thinking-2507"
    
    try:
        # å°è¯•åŠ è½½tokenizer
        print(f"  æ­£åœ¨åŠ è½½tokenizer: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        print("âœ… TokenizeråŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥ç‰¹æ®Štoken
        print(f"  EOS token: {tokenizer.eos_token}")
        print(f"  PAD token: {tokenizer.pad_token}")
        print(f"  è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸å¯ç”¨: {e}")
        return False

def test_thinking_data_format():
    """æµ‹è¯•thinkingæ•°æ®æ ¼å¼å¤„ç†"""
    print("\nğŸ” æµ‹è¯•thinkingæ•°æ®æ ¼å¼å¤„ç†...")
    
    try:
        # è¯»å–è®­ç»ƒæ•°æ®
        data_path = "final_demo_output/data/crypto_qa_dataset_train.json"
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # åˆ†æthinkingæ•°æ®
        thinking_samples = []
        for item in data:
            output = item.get('output', '')
            if '<thinking>' in output and '</thinking>' in output:
                thinking_samples.append(item)
        
        print(f"âœ… æ‰¾åˆ° {len(thinking_samples)} ä¸ªthinkingæ ·æœ¬")
        
        if thinking_samples:
            sample = thinking_samples[0]
            print("  æ ·æœ¬ç¤ºä¾‹:")
            print(f"    æŒ‡ä»¤: {sample['instruction'][:50]}...")
            
            output = sample['output']
            # æå–thinkingéƒ¨åˆ†
            thinking_start = output.find('<thinking>')
            thinking_end = output.find('</thinking>') + len('</thinking>')
            
            if thinking_start != -1 and thinking_end != -1:
                thinking_part = output[thinking_start:thinking_end]
                response_part = output[thinking_end:].strip()
                
                print(f"    Thinkingéƒ¨åˆ†é•¿åº¦: {len(thinking_part)} å­—ç¬¦")
                print(f"    å“åº”éƒ¨åˆ†é•¿åº¦: {len(response_part)} å­—ç¬¦")
                print(f"    Thinkingé¢„è§ˆ: {thinking_part[:100]}...")
        
        return True
    except Exception as e:
        print(f"âŒ thinkingæ•°æ®æ ¼å¼æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_memory_requirements():
    """æµ‹è¯•å†…å­˜éœ€æ±‚"""
    print("\nğŸ” æµ‹è¯•å†…å­˜éœ€æ±‚...")
    
    try:
        if not torch.cuda.is_available():
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå†…å­˜æµ‹è¯•")
            return True
        
        # æ£€æŸ¥GPUå†…å­˜
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory // 1024**2
            
            print(f"  GPU {i}: {gpu_name}")
            print(f"    æ€»å†…å­˜: {total_memory}MB")
            
            # ä¼°ç®—4Bæ¨¡å‹éœ€è¦çš„å†…å­˜
            # 4Bå‚æ•° * 2å­—èŠ‚(fp16) â‰ˆ 8GB
            # åŠ ä¸Šæ¿€æ´»å€¼ã€æ¢¯åº¦ç­‰ï¼Œå¤§çº¦éœ€è¦12-16GB
            required_memory = 12000  # MB
            
            if total_memory >= required_memory:
                print(f"    âœ… å†…å­˜å……è¶³ ({total_memory}MB >= {required_memory}MB)")
            else:
                print(f"    âš ï¸ å†…å­˜å¯èƒ½ä¸è¶³ ({total_memory}MB < {required_memory}MB)")
                print(f"    å»ºè®®: ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ›´å°æ‰¹æ¬¡å¤§å°æˆ–æ¨¡å‹å¹¶è¡Œ")
        
        return True
    except Exception as e:
        print(f"âŒ å†…å­˜éœ€æ±‚æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_tokenizer_with_thinking():
    """æµ‹è¯•tokenizerå¤„ç†thinkingæ ¼å¼"""
    print("\nğŸ” æµ‹è¯•tokenizerå¤„ç†thinkingæ ¼å¼...")
    
    try:
        model_name = "Qwen/Qwen3-4B-Thinking-2507"
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        # è®¾ç½®pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # æµ‹è¯•thinkingæ ¼å¼æ–‡æœ¬
        test_text = """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯†ç å­¦ä¸“å®¶ï¼Œè¯·ä»”ç»†æ€è€ƒåå›ç­”é—®é¢˜ã€‚<|im_end|>
<|im_start|>user
ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ<|im_end|>
<|im_start|>assistant
<thinking>
è¿™æ˜¯ä¸€ä¸ªå…³äºå¯¹ç§°åŠ å¯†ç®—æ³•çš„é—®é¢˜ã€‚AESæ˜¯é«˜çº§åŠ å¯†æ ‡å‡†ï¼Œæˆ‘éœ€è¦ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥å›ç­”ï¼š
1. AESçš„å…¨ç§°å’Œå†å²
2. AESçš„åŸºæœ¬ç‰¹å¾
3. AESçš„å·¥ä½œåŸç†
</thinking>

AESï¼ˆAdvanced Encryption Standardï¼Œé«˜çº§åŠ å¯†æ ‡å‡†ï¼‰æ˜¯ä¸€ç§å¯¹ç§°åŠ å¯†ç®—æ³•ã€‚<|im_end|>"""
        
        # åˆ†è¯æµ‹è¯•
        tokens = tokenizer(test_text, return_tensors="pt")
        
        print(f"âœ… åˆ†è¯æˆåŠŸ")
        print(f"  è¾“å…¥é•¿åº¦: {len(test_text)} å­—ç¬¦")
        print(f"  Tokenæ•°é‡: {tokens['input_ids'].shape[1]}")
        
        # æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦æ­£ç¡®å¤„ç†
        decoded = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=False)
        
        if '<thinking>' in decoded and '</thinking>' in decoded:
            print("âœ… thinkingæ ‡ç­¾ä¿æŒå®Œæ•´")
        else:
            print("âš ï¸ thinkingæ ‡ç­¾å¯èƒ½è¢«å¤„ç†")
        
        return True
    except Exception as e:
        print(f"âŒ tokenizeræµ‹è¯•å¤±è´¥: {e}")
        return False

def test_model_loading_with_optimization():
    """æµ‹è¯•ä¼˜åŒ–é…ç½®ä¸‹çš„æ¨¡å‹åŠ è½½"""
    print("\nğŸ” æµ‹è¯•ä¼˜åŒ–é…ç½®ä¸‹çš„æ¨¡å‹åŠ è½½...")
    
    try:
        model_name = "Qwen/Qwen3-4B-Thinking-2507"
        
        print("  æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        
        # ä½¿ç”¨å†…å­˜ä¼˜åŒ–é…ç½®
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # ä½¿ç”¨fp16å‡å°‘å†…å­˜
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"  å‚æ•°é‡: {model.num_parameters():,}")
        print(f"  æ¨¡å‹å¤§å°: {model.num_parameters() * 2 / 1024**3:.2f}GB (fp16)")
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        model.gradient_checkpointing_enable()
        print("âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
        
        # æ£€æŸ¥æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ
        if torch.cuda.is_available():
            device_map = {}
            for name, param in model.named_parameters():
                device = str(param.device)
                if device not in device_map:
                    device_map[device] = 0
                device_map[device] += param.numel()
            
            print("  æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ:")
            for device, param_count in device_map.items():
                print(f"    {device}: {param_count:,} å‚æ•°")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ Qwen3-4B-Thinkingæ¨¡å‹ä¸“ç”¨æµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("æ¨¡å‹å¯ç”¨æ€§", test_model_availability),
        ("Thinkingæ•°æ®æ ¼å¼", test_thinking_data_format),
        ("å†…å­˜éœ€æ±‚", test_memory_requirements),
        ("Tokenizerå¤„ç†", test_tokenizer_with_thinking),
        ("ä¼˜åŒ–æ¨¡å‹åŠ è½½", test_model_loading_with_optimization),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ€»ç»“
    print(f"\n{'='*20} æµ‹è¯•æ€»ç»“ {'='*20}")
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed >= len(results) * 0.8:  # 80%é€šè¿‡ç‡
        print("ğŸ‰ Qwen3-4B-Thinkingæ¨¡å‹æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼")
        print("\nğŸ“ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. è¿è¡Œ 'uv run python direct_finetuning_with_existing_modules.py' å¼€å§‹å¾®è°ƒ")
        print("2. ç›‘æ§GPUå†…å­˜ä½¿ç”¨ï¼Œå¿…è¦æ—¶è°ƒæ•´æ‰¹æ¬¡å¤§å°")
        print("3. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´ä¿å­˜æ£€æŸ¥ç‚¹")
        return True
    else:
        print("âš ï¸ å¤šä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)