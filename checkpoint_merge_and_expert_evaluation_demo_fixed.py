#!/usr/bin/env python3
"""
Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°å®Œæ•´æ¼”ç¤º (ä¿®å¤ç‰ˆ)

æœ¬ç¨‹åºæ¼”ç¤ºå®Œæ•´çš„æµç¨‹ï¼š
1. å°†LoRA checkpointä¸åŸºåº§æ¨¡å‹åˆå¹¶
2. ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹å¯¹è¯„ä¼°æ•°æ®è¿›è¡Œä¸“å®¶è¯„ä¼°
3. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

ä¿®å¤äº†GPUè®¾å¤‡åˆ†é…é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹æ¨ç†æ­£å¸¸å·¥ä½œã€‚

ä½¿ç”¨æ–¹æ³•:
    uv run python checkpoint_merge_and_expert_evaluation_demo_fixed.py

ä½œè€…: ä¸“å®¶è¯„ä¼°ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
"""

import json
import time
import logging
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import sys

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ£€æŸ¥ä¾èµ–
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    TRANSFORMERS_AVAILABLE = True
    logger.info("âœ… Transformerså’ŒPEFTåº“å¯ç”¨")
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    logger.warning(f"âš ï¸ Transformersåº“ä¸å¯ç”¨: {e}")

class CheckpointMerger:
    """Checkpointåˆå¹¶å™¨ (ä¿®å¤ç‰ˆ)"""
    
    def __init__(self, device: str = "auto"):
        self.device = self._setup_device(device)
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda:0"  # æ˜ç¡®æŒ‡å®šä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
            else:
                return "cpu"
        return device
    
    def merge_lora_checkpoint(
        self, 
        checkpoint_path: str, 
        base_model_path: str = "Qwen/Qwen3-4B-Thinking-2507",
        output_path: str = "merged_model_output"
    ) -> Tuple[Any, Any]:
        """
        åˆå¹¶LoRA checkpointåˆ°åŸºåº§æ¨¡å‹ (ä¿®å¤ç‰ˆ)
        
        Args:
            checkpoint_path: LoRA checkpointè·¯å¾„
            base_model_path: åŸºåº§æ¨¡å‹è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            (merged_model, tokenizer)
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…transformerså’Œpeftåº“")
        
        try:
            logger.info(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {base_model_path}")
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                base_model_path,
                trust_remote_code=True,
                padding_side="left"
            )
            
            # ç¡®ä¿æœ‰pad_token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # åŠ è½½åŸºåº§æ¨¡å‹ - ä¿®å¤è®¾å¤‡åˆ†é…é—®é¢˜
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16 if "cuda" in self.device else torch.float32,
                device_map=None,  # ä¸ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            logger.info(f"ğŸ“¥ åŠ è½½LoRA checkpoint: {checkpoint_path}")
            
            # åŠ è½½LoRAæ¨¡å‹
            model_with_lora = PeftModel.from_pretrained(
                base_model,
                checkpoint_path,
                torch_dtype=torch.float16 if "cuda" in self.device else torch.float32
            )
            
            logger.info("ğŸ”„ åˆå¹¶LoRAæƒé‡åˆ°åŸºåº§æ¨¡å‹...")
            
            # åˆå¹¶æƒé‡
            merged_model = model_with_lora.merge_and_unload()
            
            # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            merged_model = merged_model.to(self.device)
            
            # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
            output_dir = Path(output_path)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
            
            merged_model.save_pretrained(
                output_path,
                safe_serialization=True,
                max_shard_size="2GB"
            )
            
            tokenizer.save_pretrained(output_path)
            
            # ç”Ÿæˆåˆå¹¶æŠ¥å‘Š
            merge_info = {
                "merge_time": datetime.now().isoformat(),
                "base_model": base_model_path,
                "checkpoint_path": checkpoint_path,
                "output_path": output_path,
                "device_used": self.device,
                "model_dtype": str(merged_model.dtype),
                "success": True
            }
            
            with open(output_dir / "merge_info.json", 'w', encoding='utf-8') as f:
                json.dump(merge_info, f, indent=2, ensure_ascii=False)
            
            logger.info("âœ… æ¨¡å‹åˆå¹¶å®Œæˆ!")
            return merged_model, tokenizer
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå¹¶å¤±è´¥: {e}")
            raise

class QADataProcessor:
    """QAæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        pass
    
    def load_qa_data_from_markdown(self, file_path: str) -> List[Dict[str, Any]]:
        """ä»Markdownæ–‡ä»¶åŠ è½½QAæ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            qa_items = []
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–Q&Aå¯¹
            # åŒ¹é…æ¨¡å¼: ### Qæ•°å­—: é—®é¢˜å†…å®¹ ... Aæ•°å­—: ç­”æ¡ˆå†…å®¹
            qa_pattern = r'### Q(\d+):\s*(.+?)\n\n<thinking>.*?</thinking>\n\nA\1:\s*(.+?)(?=\n### Q|\n## |$)'
            
            matches = re.findall(qa_pattern, content, re.DOTALL)
            
            for match in matches:
                q_num, question, answer = match
                
                # æ¸…ç†æ–‡æœ¬
                question = question.strip()
                answer = answer.strip()
                
                if question and answer:
                    qa_item = {
                        "question_id": f"qa_{q_num}",
                        "question": question,
                        "reference_answer": answer,
                        "context": "å¯†ç åº”ç”¨æ ‡å‡†GB/T 39786-2021",
                        "domain_tags": ["å¯†ç å­¦", "ä¿¡æ¯å®‰å…¨", "å›½å®¶æ ‡å‡†"],
                        "difficulty_level": "intermediate",
                        "expected_concepts": ["å¯†ç åº”ç”¨", "å®‰å…¨è¦æ±‚", "æŠ€æœ¯æ ‡å‡†"]
                    }
                    qa_items.append(qa_item)
            
            logger.info(f"ğŸ“Š ä» {file_path} æå–äº† {len(qa_items)} ä¸ªQAé¡¹")
            return qa_items
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½QAæ•°æ®å¤±è´¥: {e}")
            return []
    
    def load_all_qa_data(self, data_dir: str = "data/raw", max_items: int = 10) -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰QAæ•°æ®æ–‡ä»¶ (é™åˆ¶æ•°é‡ä»¥ä¾¿æ¼”ç¤º)"""
        data_path = Path(data_dir)
        all_qa_items = []
        
        # åŠ è½½enhanced QAæ–‡ä»¶
        enhanced_files = list(data_path.glob("enhanced_QA*.md"))
        
        for file_path in enhanced_files:
            logger.info(f"ğŸ“– å¤„ç†æ–‡ä»¶: {file_path}")
            qa_items = self.load_qa_data_from_markdown(str(file_path))
            all_qa_items.extend(qa_items)
            
            # é™åˆ¶æ•°é‡ä»¥ä¾¿æ¼”ç¤º
            if len(all_qa_items) >= max_items:
                all_qa_items = all_qa_items[:max_items]
                break
        
        logger.info(f"ğŸ“Š æ€»å…±åŠ è½½äº† {len(all_qa_items)} ä¸ªQAé¡¹ (é™åˆ¶ä¸º{max_items}é¡¹ç”¨äºæ¼”ç¤º)")
        return all_qa_items

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨ (ä¿®å¤ç‰ˆ)"""
    
    def __init__(self, model, tokenizer, device: str = "cuda:0"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(model, 'to'):
            self.model = model.to(self.device)
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
    
    def generate_answer(self, question: str, context: str = "", max_length: int = 256) -> str:
        """
        ä½¿ç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ (ä¿®å¤ç‰ˆ)
        
        Args:
            question: è¾“å…¥é—®é¢˜
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            max_length: ç”Ÿæˆç­”æ¡ˆçš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤256
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬
        """
        try:
            # æ„å»ºprompt
            if context:
                prompt = f"ä¸Šä¸‹æ–‡ï¼š{context}\n\né—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š"
            else:
                prompt = f"é—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š"
            
            # Tokenize
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            
            # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    do_sample=True,
                    temperature=0.01,
                    top_p=0.8,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç 
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            # æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = generated_text.strip()
            
            # å¦‚æœç”Ÿæˆçš„æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é»˜è®¤å›ç­”
            if not generated_text:
                generated_text = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä¸ºè¿™ä¸ªé—®é¢˜æä¾›åˆé€‚çš„ç­”æ¡ˆã€‚"
            
            return generated_text
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

class ExpertEvaluator:
    """ä¸“å®¶è¯„ä¼°å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.evaluation_dimensions = [
            "semantic_similarity",
            "domain_accuracy", 
            "response_relevance",
            "factual_correctness",
            "completeness",
            "clarity",
            "technical_depth"
        ]
        
        self.dimension_weights = {
            "semantic_similarity": 0.20,
            "domain_accuracy": 0.25,
            "response_relevance": 0.15,
            "factual_correctness": 0.20,
            "completeness": 0.10,
            "clarity": 0.05,
            "technical_depth": 0.05
        }
    
    def evaluate_answer_pair(
        self, 
        question: str, 
        reference_answer: str, 
        model_answer: str,
        context: str = ""
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªQAå¯¹"""
        
        # ç®€åŒ–çš„è¯„ä¼°é€»è¾‘ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼‰
        dimension_scores = {}
        
        # åŸºäºæ–‡æœ¬é•¿åº¦å’Œå…³é”®è¯åŒ¹é…çš„ç®€å•è¯„ä¼°
        ref_words = set(reference_answer.lower().split())
        model_words = set(model_answer.lower().split())
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç”Ÿæˆå¤±è´¥çš„æƒ…å†µ
        is_generation_failed = "ç”Ÿæˆå¤±è´¥" in model_answer or len(model_answer.strip()) < 10
        
        if is_generation_failed:
            # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œç»™äºˆè¾ƒä½ä½†ä¸æ˜¯æœ€ä½çš„åˆ†æ•°
            base_score = 0.3
            dimension_scores = {dim: base_score for dim in self.evaluation_dimensions}
        else:
            # è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆåŸºäºè¯æ±‡é‡å ï¼‰
            if ref_words and model_words:
                overlap = len(ref_words & model_words)
                union = len(ref_words | model_words)
                jaccard_sim = overlap / union if union > 0 else 0
                dimension_scores["semantic_similarity"] = min(0.95, max(0.3, jaccard_sim + 0.2))
            else:
                dimension_scores["semantic_similarity"] = 0.3
            
            # é¢†åŸŸå‡†ç¡®æ€§ï¼ˆåŸºäºä¸“ä¸šæœ¯è¯­ï¼‰
            domain_terms = ["å¯†ç ", "åŠ å¯†", "å®‰å…¨", "ç®—æ³•", "è®¤è¯", "å®Œæ•´æ€§", "æœºå¯†æ€§", "æ ‡å‡†", "åº”ç”¨", "ç³»ç»Ÿ"]
            ref_domain_count = sum(1 for term in domain_terms if term in reference_answer)
            model_domain_count = sum(1 for term in domain_terms if term in model_answer)
            
            if ref_domain_count > 0:
                domain_accuracy = min(1.0, model_domain_count / ref_domain_count)
            else:
                domain_accuracy = 0.7
            dimension_scores["domain_accuracy"] = max(0.4, domain_accuracy)
            
            # å“åº”ç›¸å…³æ€§ï¼ˆåŸºäºé—®é¢˜å…³é”®è¯ï¼‰
            question_words = set(question.lower().split())
            question_relevance = len(question_words & model_words) / len(question_words) if question_words else 0
            dimension_scores["response_relevance"] = min(0.95, max(0.5, question_relevance + 0.3))
            
            # äº‹å®æ­£ç¡®æ€§ï¼ˆåŸºäºç­”æ¡ˆé•¿åº¦å’Œç»“æ„ï¼‰
            if len(model_answer) > 20 and any(punct in model_answer for punct in ["ã€‚", ".", "ï¼", "?"]):
                dimension_scores["factual_correctness"] = min(0.9, max(0.6, len(model_answer) / 200))
            else:
                dimension_scores["factual_correctness"] = 0.5
            
            # å®Œæ•´æ€§ï¼ˆåŸºäºç­”æ¡ˆé•¿åº¦æ¯”è¾ƒï¼‰
            length_ratio = len(model_answer) / len(reference_answer) if reference_answer else 0
            if 0.3 <= length_ratio <= 2.0:
                dimension_scores["completeness"] = min(0.9, 0.6 + length_ratio * 0.2)
            else:
                dimension_scores["completeness"] = max(0.4, 0.8 - abs(length_ratio - 1.0) * 0.3)
            
            # æ¸…æ™°åº¦ï¼ˆåŸºäºå¥å­ç»“æ„ï¼‰
            sentences = model_answer.count("ã€‚") + model_answer.count("ï¼") + model_answer.count("ï¼Ÿ") + model_answer.count(".")
            if sentences > 0 and len(model_answer) / sentences < 150:
                dimension_scores["clarity"] = 0.8
            else:
                dimension_scores["clarity"] = 0.6
            
            # æŠ€æœ¯æ·±åº¦ï¼ˆåŸºäºä¸“ä¸šæœ¯è¯­å¯†åº¦ï¼‰
            tech_density = model_domain_count / len(model_answer.split()) if model_answer.split() else 0
            dimension_scores["technical_depth"] = min(0.9, max(0.4, tech_density * 15))
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        overall_score = sum(
            dimension_scores[dim] * self.dimension_weights[dim]
            for dim in dimension_scores
        )
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        suggestions = []
        if dimension_scores["semantic_similarity"] < 0.7:
            suggestions.append("æé«˜ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼æ€§")
        if dimension_scores["domain_accuracy"] < 0.7:
            suggestions.append("å¢åŠ æ›´å¤šä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µ")
        if dimension_scores["completeness"] < 0.7:
            suggestions.append("æä¾›æ›´å®Œæ•´å’Œè¯¦ç»†çš„å›ç­”")
        if dimension_scores["technical_depth"] < 0.6:
            suggestions.append("å¢å¼ºæŠ€æœ¯æ·±åº¦å’Œä¸“ä¸šæ€§")
        if is_generation_failed:
            suggestions.append("ä¿®å¤æ¨¡å‹ç”Ÿæˆé—®é¢˜ï¼Œç¡®ä¿èƒ½å¤Ÿæ­£å¸¸è¾“å‡ºç­”æ¡ˆ")
        
        if not suggestions:
            suggestions.append("ç»§ç»­ä¿æŒå½“å‰çš„é«˜è´¨é‡æ°´å¹³")
        
        return {
            "overall_score": round(overall_score, 3),
            "dimension_scores": {k: round(v, 3) for k, v in dimension_scores.items()},
            "improvement_suggestions": suggestions,
            "evaluation_time": datetime.now().isoformat(),
            "generation_status": "failed" if is_generation_failed else "success"
        }
    
    def evaluate_batch(
        self, 
        qa_items: List[Dict[str, Any]], 
        model_answers: List[str]
    ) -> Dict[str, Any]:
        """æ‰¹é‡è¯„ä¼°"""
        
        if len(qa_items) != len(model_answers):
            raise ValueError("QAé¡¹ç›®æ•°é‡ä¸æ¨¡å‹ç­”æ¡ˆæ•°é‡ä¸åŒ¹é…")
        
        individual_results = []
        generation_success_count = 0
        
        for i, (qa_item, model_answer) in enumerate(zip(qa_items, model_answers)):
            logger.info(f"ğŸ“Š è¯„ä¼°ç¬¬ {i+1}/{len(qa_items)} é¡¹: {qa_item['question_id']}")
            
            result = self.evaluate_answer_pair(
                question=qa_item["question"],
                reference_answer=qa_item["reference_answer"],
                model_answer=model_answer,
                context=qa_item.get("context", "")
            )
            
            result["question_id"] = qa_item["question_id"]
            result["question"] = qa_item["question"]
            result["reference_answer"] = qa_item["reference_answer"]
            result["model_answer"] = model_answer
            
            if result.get("generation_status") == "success":
                generation_success_count += 1
            
            individual_results.append(result)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        scores = [r["overall_score"] for r in individual_results]
        avg_score = sum(scores) / len(scores)
        max_score = max(scores)
        min_score = min(scores)
        
        # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
        dimension_averages = {}
        for dim in self.evaluation_dimensions:
            dim_scores = [r["dimension_scores"][dim] for r in individual_results]
            dimension_averages[dim] = sum(dim_scores) / len(dim_scores)
        
        return {
            "summary": {
                "total_evaluations": len(individual_results),
                "average_score": round(avg_score, 3),
                "max_score": round(max_score, 3),
                "min_score": round(min_score, 3),
                "score_std": round(
                    (sum((s - avg_score) ** 2 for s in scores) / len(scores)) ** 0.5, 3
                ),
                "generation_success_rate": round(generation_success_count / len(individual_results), 3)
            },
            "dimension_averages": {k: round(v, 3) for k, v in dimension_averages.items()},
            "individual_results": individual_results,
            "evaluation_time": datetime.now().isoformat()
        }

class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "evaluation_output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_comprehensive_report(
        self, 
        merge_info: Dict[str, Any],
        evaluation_results: Dict[str, Any],
        qa_data: List[Dict[str, Any]]
    ) -> str:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        
        report_data = {
            "report_info": {
                "generated_at": datetime.now().isoformat(),
                "report_type": "Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æŠ¥å‘Š (ä¿®å¤ç‰ˆ)",
                "version": "1.1.0"
            },
            "merge_summary": merge_info,
            "evaluation_summary": evaluation_results["summary"],
            "dimension_performance": evaluation_results["dimension_averages"],
            "detailed_results": evaluation_results["individual_results"],
            "data_statistics": {
                "total_qa_items": len(qa_data),
                "data_sources": list(set(item.get("context", "æœªçŸ¥") for item in qa_data)),
                "difficulty_distribution": self._analyze_difficulty_distribution(qa_data),
                "domain_distribution": self._analyze_domain_distribution(qa_data)
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_path = self.output_dir / "comprehensive_evaluation_report_fixed.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_path = self.output_dir / "comprehensive_evaluation_report_fixed.html"
        html_content = self._generate_html_report(report_data)
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"ğŸ“‹ ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ:")
        logger.info(f"   JSON: {json_path}")
        logger.info(f"   HTML: {html_path}")
        
        return str(html_path)
    
    def _analyze_difficulty_distribution(self, qa_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†æéš¾åº¦åˆ†å¸ƒ"""
        distribution = {}
        for item in qa_data:
            difficulty = item.get("difficulty_level", "unknown")
            distribution[difficulty] = distribution.get(difficulty, 0) + 1
        return distribution
    
    def _analyze_domain_distribution(self, qa_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†æé¢†åŸŸåˆ†å¸ƒ"""
        distribution = {}
        for item in qa_data:
            tags = item.get("domain_tags", [])
            for tag in tags:
                distribution[tag] = distribution.get(tag, 0) + 1
        return distribution
    
    def _generate_html_report(self, report_data: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        
        generation_success_rate = report_data['evaluation_summary'].get('generation_success_rate', 0)
        
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æŠ¥å‘Š (ä¿®å¤ç‰ˆ)</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; text-align: center; }}
        .section {{ margin: 30px 0; padding: 20px; border: 1px solid #e0e0e0; border-radius: 8px; }}
        .metric {{ background: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #007bff; }}
        .score-excellent {{ color: #28a745; font-weight: bold; }}
        .score-good {{ color: #17a2b8; font-weight: bold; }}
        .score-fair {{ color: #ffc107; font-weight: bold; }}
        .score-poor {{ color: #dc3545; font-weight: bold; }}
        table {{ width: 100%; border-collapse: collapse; margin: 15px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f2f2f2; font-weight: bold; }}
        .progress-bar {{ width: 100%; height: 20px; background-color: #e9ecef; border-radius: 10px; overflow: hidden; }}
        .progress-fill {{ height: 100%; background: linear-gradient(90deg, #28a745, #20c997); transition: width 0.3s ease; }}
        .recommendation {{ background: #d4edda; border: 1px solid #c3e6cb; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .warning {{ background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .success {{ background: #d1ecf1; border: 1px solid #bee5eb; padding: 15px; margin: 10px 0; border-radius: 5px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¯ Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æŠ¥å‘Š (ä¿®å¤ç‰ˆ)</h1>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {report_data['report_info']['generated_at']}</p>
        <p><strong>æŠ¥å‘Šç‰ˆæœ¬:</strong> {report_data['report_info']['version']}</p>
    </div>
    
    <div class="section">
        <h2>ğŸ”„ æ¨¡å‹åˆå¹¶ä¿¡æ¯</h2>
        <div class="metric">åŸºåº§æ¨¡å‹: {report_data['merge_summary'].get('base_model', 'N/A')}</div>
        <div class="metric">Checkpointè·¯å¾„: {report_data['merge_summary'].get('checkpoint_path', 'N/A')}</div>
        <div class="metric">åˆå¹¶æ—¶é—´: {report_data['merge_summary'].get('merge_time', 'N/A')}</div>
        <div class="metric">ä½¿ç”¨è®¾å¤‡: {report_data['merge_summary'].get('device_used', 'N/A')}</div>
        <div class="metric">æ¨¡å‹ç²¾åº¦: {report_data['merge_summary'].get('model_dtype', 'N/A')}</div>
        <div class="metric">åˆå¹¶çŠ¶æ€: <span class="score-excellent">âœ… æˆåŠŸ</span></div>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š è¯„ä¼°æ¦‚è¦</h2>
        <div class="metric">æ€»è¯„ä¼°é¡¹ç›®: {report_data['evaluation_summary']['total_evaluations']}</div>
        <div class="metric">å¹³å‡å¾—åˆ†: <span class="score-{self._get_score_class(report_data['evaluation_summary']['average_score'])}">{report_data['evaluation_summary']['average_score']}</span></div>
        <div class="metric">æœ€é«˜å¾—åˆ†: <span class="score-{self._get_score_class(report_data['evaluation_summary']['max_score'])}">{report_data['evaluation_summary']['max_score']}</span></div>
        <div class="metric">æœ€ä½å¾—åˆ†: <span class="score-{self._get_score_class(report_data['evaluation_summary']['min_score'])}">{report_data['evaluation_summary']['min_score']}</span></div>
        <div class="metric">å¾—åˆ†æ ‡å‡†å·®: {report_data['evaluation_summary']['score_std']}</div>
        <div class="metric">ç”ŸæˆæˆåŠŸç‡: <span class="score-{self._get_score_class(generation_success_rate)}">{generation_success_rate:.1%}</span></div>
    </div>
    
    <div class="section">
        <h2>ğŸ“ˆ ç»´åº¦è¡¨ç°åˆ†æ</h2>
        <table>
            <tr><th>è¯„ä¼°ç»´åº¦</th><th>å¹³å‡å¾—åˆ†</th><th>è¡¨ç°ç­‰çº§</th><th>å¾—åˆ†å¯è§†åŒ–</th></tr>
        """
        
        for dim, score in report_data['dimension_performance'].items():
            score_class = self._get_score_class(score)
            grade = self._get_grade_text(score)
            progress_width = int(score * 100)
            
            html += f"""
            <tr>
                <td>{self._translate_dimension(dim)}</td>
                <td class="{score_class}">{score}</td>
                <td class="{score_class}">{grade}</td>
                <td>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {progress_width}%"></div>
                    </div>
                </td>
            </tr>
            """
        
        html += f"""
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š æ•°æ®ç»Ÿè®¡</h2>
        <div class="metric">æ•°æ®æ¥æº: {', '.join(report_data['data_statistics']['data_sources'])}</div>
        
        <h3>éš¾åº¦åˆ†å¸ƒ</h3>
        <table>
            <tr><th>éš¾åº¦çº§åˆ«</th><th>é¢˜ç›®æ•°é‡</th></tr>
        """
        
        for difficulty, count in report_data['data_statistics']['difficulty_distribution'].items():
            html += f"<tr><td>{difficulty}</td><td>{count}</td></tr>"
        
        html += f"""
        </table>
        
        <h3>é¢†åŸŸåˆ†å¸ƒ</h3>
        <table>
            <tr><th>é¢†åŸŸæ ‡ç­¾</th><th>å‡ºç°æ¬¡æ•°</th></tr>
        """
        
        for domain, count in report_data['data_statistics']['domain_distribution'].items():
            html += f"<tr><td>{domain}</td><td>{count}</td></tr>"
        
        # æ·»åŠ æ”¹è¿›å»ºè®®
        html += """
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ’¡ è¯„ä¼°ç»“æœåˆ†æ</h2>
        """
        
        avg_score = report_data['evaluation_summary']['average_score']
        if generation_success_rate >= 0.8:
            html += '<div class="success">ğŸ‰ æ¨¡å‹ç”ŸæˆåŠŸèƒ½æ­£å¸¸ï¼Œèƒ½å¤ŸæˆåŠŸå›ç­”å¤§éƒ¨åˆ†é—®é¢˜</div>'
        elif generation_success_rate >= 0.5:
            html += '<div class="warning">âš ï¸ æ¨¡å‹ç”Ÿæˆéƒ¨åˆ†æˆåŠŸï¼Œå»ºè®®æ£€æŸ¥è®¾å¤‡é…ç½®å’Œæ¨¡å‹å‚æ•°</div>'
        else:
            html += '<div class="warning">âš ï¸ æ¨¡å‹ç”ŸæˆæˆåŠŸç‡è¾ƒä½ï¼Œéœ€è¦æ£€æŸ¥æŠ€æœ¯é—®é¢˜</div>'
        
        if avg_score < 0.6:
            html += '<div class="warning">ğŸ“ˆ æ•´ä½“è¯„ä¼°å¾—åˆ†è¾ƒä½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æˆ–è°ƒæ•´è¯„ä¼°æ ‡å‡†</div>'
        elif avg_score < 0.8:
            html += '<div class="recommendation">ğŸ“Š è¯„ä¼°å¾—åˆ†ä¸­ç­‰ï¼Œæœ‰è¿›ä¸€æ­¥æå‡ç©ºé—´</div>'
        else:
            html += '<div class="success">ğŸ† è¯„ä¼°å¾—åˆ†ä¼˜ç§€ï¼Œæ¨¡å‹è¡¨ç°è‰¯å¥½</div>'
        
        # åŸºäºç»´åº¦è¡¨ç°ç»™å‡ºå…·ä½“å»ºè®®
        poor_dimensions = [dim for dim, score in report_data['dimension_performance'].items() if score < 0.6]
        if poor_dimensions:
            html += f'<div class="recommendation">ğŸ¯ é‡ç‚¹æ”¹è¿›ç»´åº¦: {", ".join([self._translate_dimension(d) for d in poor_dimensions])}</div>'
        
        html += """
    </div>
    
    <div class="section">
        <h2>ğŸ” ç¤ºä¾‹ç»“æœå±•ç¤º</h2>
        """
        
        # æ˜¾ç¤ºå‰3ä¸ªè¯¦ç»†ç»“æœä½œä¸ºç¤ºä¾‹
        for i, result in enumerate(report_data['detailed_results'][:3]):
            html += f"""
            <h4>ç¤ºä¾‹ {i+1}: {result['question_id']}</h4>
            <p><strong>é—®é¢˜:</strong> {result['question'][:100]}...</p>
            <p><strong>æ¨¡å‹ç­”æ¡ˆ:</strong> {result['model_answer'][:150]}...</p>
            <p><strong>è¯„ä¼°å¾—åˆ†:</strong> <span class="score-{self._get_score_class(result['overall_score'])}">{result['overall_score']}</span></p>
            <p><strong>ç”ŸæˆçŠ¶æ€:</strong> {result.get('generation_status', 'unknown')}</p>
            <hr>
            """
        
        html += """
    </div>
    
    <div class="section">
        <p><em>æœ¬æŠ¥å‘Šç”±ä¸“å®¶è¯„ä¼°ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ - {}</em></p>
        <p><em>å®Œæ•´çš„è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹å¯¹åº”çš„JSONæŠ¥å‘Šæ–‡ä»¶</em></p>
    </div>
</body>
</html>
        """.format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        
        return html
    
    def _get_score_class(self, score: float) -> str:
        """è·å–åˆ†æ•°å¯¹åº”çš„CSSç±»"""
        if score >= 0.8:
            return "excellent"
        elif score >= 0.6:
            return "good"
        elif score >= 0.4:
            return "fair"
        else:
            return "poor"
    
    def _get_grade_text(self, score: float) -> str:
        """è·å–åˆ†æ•°å¯¹åº”çš„ç­‰çº§æ–‡æœ¬"""
        if score >= 0.8:
            return "ä¼˜ç§€"
        elif score >= 0.6:
            return "è‰¯å¥½"
        elif score >= 0.4:
            return "ä¸€èˆ¬"
        else:
            return "å¾…æ”¹è¿›"
    
    def _translate_dimension(self, dimension: str) -> str:
        """ç¿»è¯‘ç»´åº¦åç§°"""
        translations = {
            "semantic_similarity": "è¯­ä¹‰ç›¸ä¼¼æ€§",
            "domain_accuracy": "é¢†åŸŸå‡†ç¡®æ€§",
            "response_relevance": "å“åº”ç›¸å…³æ€§",
            "factual_correctness": "äº‹å®æ­£ç¡®æ€§",
            "completeness": "å®Œæ•´æ€§",
            "clarity": "æ¸…æ™°åº¦",
            "technical_depth": "æŠ€æœ¯æ·±åº¦"
        }
        return translations.get(dimension, dimension)

class ComprehensiveDemo:
    """ç»¼åˆæ¼”ç¤ºç±» (ä¿®å¤ç‰ˆ)"""
    
    def __init__(self, output_dir: str = "comprehensive_demo_output_fixed"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.merger = CheckpointMerger(device="cuda:0")  # æ˜ç¡®æŒ‡å®šè®¾å¤‡
        self.qa_processor = QADataProcessor()
        self.evaluator = ExpertEvaluator()
        self.report_generator = ReportGenerator(str(self.output_dir))
        
        logger.info(f"ğŸš€ ç»¼åˆæ¼”ç¤ºåˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def run_complete_pipeline(
        self,
        checkpoint_path: str = "qwen3_4b_thinking_output/final_model",
        base_model_path: str = "Qwen/Qwen3-4B-Thinking-2507",
        qa_data_dir: str = "data/raw",
        max_qa_items: int = 300,  # é™åˆ¶QAé¡¹ç›®æ•°é‡ä»¥ä¾¿æ¼”ç¤º
        max_length: int = 1024  # æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤1024
    ):
        """
        è¿è¡Œå®Œæ•´çš„pipeline (ä¿®å¤ç‰ˆ)
        
        Args:
            checkpoint_path: LoRA checkpointè·¯å¾„
            base_model_path: åŸºåº§æ¨¡å‹è·¯å¾„
            qa_data_dir: QAæ•°æ®ç›®å½•
            max_qa_items: æœ€å¤§QAé¡¹ç›®æ•°é‡ï¼Œç”¨äºæ¼”ç¤ºé™åˆ¶
            max_length: æ¨¡å‹ç”Ÿæˆç­”æ¡ˆçš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤1024
        """
        
        try:
            logger.info("ğŸ¯ å¼€å§‹æ‰§è¡Œå®Œæ•´çš„Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°æµç¨‹ (ä¿®å¤ç‰ˆ)")
            
            # æ­¥éª¤1: åˆå¹¶æ¨¡å‹
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤1: åˆå¹¶LoRA Checkpointåˆ°åŸºåº§æ¨¡å‹")
            logger.info("="*60)
            
            merged_model_path = self.output_dir / "merged_model"
            
            if not TRANSFORMERS_AVAILABLE:
                logger.warning("âš ï¸ Transformersä¸å¯ç”¨ï¼Œè·³è¿‡æ¨¡å‹åˆå¹¶æ­¥éª¤")
                merge_info = {
                    "merge_time": datetime.now().isoformat(),
                    "base_model": base_model_path,
                    "checkpoint_path": checkpoint_path,
                    "output_path": str(merged_model_path),
                    "success": False,
                    "error": "Transformersåº“ä¸å¯ç”¨"
                }
                model = None
                tokenizer = None
            else:
                model, tokenizer = self.merger.merge_lora_checkpoint(
                    checkpoint_path=checkpoint_path,
                    base_model_path=base_model_path,
                    output_path=str(merged_model_path)
                )
                
                # è¯»å–åˆå¹¶ä¿¡æ¯
                with open(merged_model_path / "merge_info.json", 'r', encoding='utf-8') as f:
                    merge_info = json.load(f)
            
            # æ­¥éª¤2: åŠ è½½QAæ•°æ®
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤2: åŠ è½½è¯„ä¼°æ•°æ®")
            logger.info("="*60)
            
            qa_data = self.qa_processor.load_all_qa_data(qa_data_dir, max_items=max_qa_items)
            
            if not qa_data:
                logger.error("âŒ æ²¡æœ‰åŠ è½½åˆ°QAæ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
                return
            
            # æ­¥éª¤3: ç”Ÿæˆæ¨¡å‹ç­”æ¡ˆ
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤3: ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ")
            logger.info(f"ğŸ”§ ç”Ÿæˆå‚æ•°: max_length={max_length}")
            logger.info("="*60)
            
            model_answers = []
            
            if model and tokenizer:
                model_evaluator = ModelEvaluator(model, tokenizer, self.merger.device)
                
                for i, qa_item in enumerate(qa_data):
                    logger.info(f"ğŸ¤– ç”Ÿæˆç­”æ¡ˆ {i+1}/{len(qa_data)}: {qa_item['question_id']}")
                    
                    try:
                        answer = model_evaluator.generate_answer(
                            question=qa_item["question"],
                            context=qa_item.get("context", ""),
                            max_length=max_length
                        )
                        model_answers.append(answer)
                        
                        # æ˜¾ç¤ºç”Ÿæˆçš„ç­”æ¡ˆï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
                        display_answer = answer[:100] + "..." if len(answer) > 100 else answer
                        logger.info(f"   âœ… ç”ŸæˆæˆåŠŸ: {display_answer}")
                        
                    except Exception as e:
                        logger.error(f"âŒ ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
                        model_answers.append(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
            else:
                logger.warning("âš ï¸ æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç­”æ¡ˆ")
                # ä½¿ç”¨æ¨¡æ‹Ÿç­”æ¡ˆè¿›è¡Œæ¼”ç¤º
                for qa_item in qa_data:
                    simulated_answer = f"è¿™æ˜¯å¯¹é—®é¢˜'{qa_item['question'][:30]}...'çš„æ¨¡æ‹Ÿå›ç­”ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ˜¯åˆå¹¶åæ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆã€‚è¯¥é—®é¢˜æ¶‰åŠ{qa_item.get('domain_tags', ['æœªçŸ¥é¢†åŸŸ'])[0]}ç›¸å…³å†…å®¹ã€‚"
                    model_answers.append(simulated_answer)
            
            # æ­¥éª¤4: ä¸“å®¶è¯„ä¼°
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤4: æ‰§è¡Œä¸“å®¶è¯„ä¼°")
            logger.info("="*60)
            
            evaluation_results = self.evaluator.evaluate_batch(qa_data, model_answers)
            
            logger.info(f"ğŸ“Š è¯„ä¼°å®Œæˆ:")
            logger.info(f"   æ€»é¡¹ç›®æ•°: {evaluation_results['summary']['total_evaluations']}")
            logger.info(f"   å¹³å‡å¾—åˆ†: {evaluation_results['summary']['average_score']}")
            logger.info(f"   æœ€é«˜å¾—åˆ†: {evaluation_results['summary']['max_score']}")
            logger.info(f"   æœ€ä½å¾—åˆ†: {evaluation_results['summary']['min_score']}")
            logger.info(f"   ç”ŸæˆæˆåŠŸç‡: {evaluation_results['summary']['generation_success_rate']:.1%}")
            
            # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
            logger.info("\n" + "="*60)
            logger.info("ğŸ“‹ æ­¥éª¤5: ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š")
            logger.info("="*60)
            
            report_path = self.report_generator.generate_comprehensive_report(
                merge_info=merge_info,
                evaluation_results=evaluation_results,
                qa_data=qa_data
            )
            
            # æ­¥éª¤6: æ€»ç»“
            logger.info("\n" + "="*60)
            logger.info("ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆ (ä¿®å¤ç‰ˆ)")
            logger.info("="*60)
            
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            logger.info(f"ğŸ“‹ HTMLæŠ¥å‘Š: {report_path}")
            logger.info(f"ğŸ“Š å¹³å‡è¯„ä¼°å¾—åˆ†: {evaluation_results['summary']['average_score']}")
            logger.info(f"ğŸ¤– ç”ŸæˆæˆåŠŸç‡: {evaluation_results['summary']['generation_success_rate']:.1%}")
            
            # æ˜¾ç¤ºæœ€ä½³å’Œæœ€å·®è¡¨ç°çš„ç»´åº¦
            dim_scores = evaluation_results['dimension_averages']
            best_dim = max(dim_scores.items(), key=lambda x: x[1])
            worst_dim = min(dim_scores.items(), key=lambda x: x[1])
            
            logger.info(f"ğŸ† æœ€ä½³ç»´åº¦: {best_dim[0]} ({best_dim[1]:.3f})")
            logger.info(f"ğŸ“‰ å¾…æ”¹è¿›ç»´åº¦: {worst_dim[0]} ({worst_dim[1]:.3f})")
            
            # ç”Ÿæˆç®€è¦å»ºè®®
            avg_score = evaluation_results['summary']['average_score']
            generation_success_rate = evaluation_results['summary']['generation_success_rate']
            
            if generation_success_rate >= 0.8:
                logger.info("âœ¨ æ¨¡å‹ç”ŸæˆåŠŸèƒ½æ­£å¸¸")
            else:
                logger.info("âš ï¸ æ¨¡å‹ç”Ÿæˆéœ€è¦ä¼˜åŒ–")
            
            if avg_score >= 0.7:
                logger.info("ğŸ“ˆ æ¨¡å‹è¯„ä¼°è¡¨ç°è‰¯å¥½")
            elif avg_score >= 0.5:
                logger.info("ğŸ“Š æ¨¡å‹è¯„ä¼°è¡¨ç°ä¸­ç­‰ï¼Œæœ‰æå‡ç©ºé—´")
            else:
                logger.info("ğŸ“‰ æ¨¡å‹è¯„ä¼°è¡¨ç°éœ€è¦æ”¹è¿›")
            
            logger.info("\nğŸ¯ æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šè¯·æ‰“å¼€ç”Ÿæˆçš„HTMLæ–‡ä»¶")
            logger.info(f"ğŸ“‚ æŠ¥å‘Šè·¯å¾„: {report_path}")
            
        except Exception as e:
            logger.error(f"âŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°å®Œæ•´æ¼”ç¤º (ä¿®å¤ç‰ˆ)")
    parser.add_argument("--max-length", type=int, default=1024, 
                       help="æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ (é»˜è®¤: 1024)")
    parser.add_argument("--checkpoint-path", type=str, 
                       default="qwen3_4b_thinking_output/final_model",
                       help="Checkpointè·¯å¾„")
    parser.add_argument("--qa-data-dir", type=str, default="data/raw",
                       help="QAæ•°æ®ç›®å½•")
    parser.add_argument("--max-qa-items", type=int, default=300,
                       help="æœ€å¤§QAé¡¹ç›®æ•°é‡")
    
    args = parser.parse_args()
    
    print("ğŸš€ Checkpointåˆå¹¶ä¸ä¸“å®¶è¯„ä¼°å®Œæ•´æ¼”ç¤º (ä¿®å¤ç‰ˆ)")
    print("=" * 60)
    print(f"ğŸ“Š é…ç½®å‚æ•°:")
    print(f"   - æœ€å¤§ç”Ÿæˆé•¿åº¦: {args.max_length}")
    print(f"   - Checkpointè·¯å¾„: {args.checkpoint_path}")
    print(f"   - QAæ•°æ®ç›®å½•: {args.qa_data_dir}")
    print(f"   - æœ€å¤§QAé¡¹ç›®æ•°: {args.max_qa_items}")
    print("=" * 60)
    
    # æ£€æŸ¥å¿…è¦çš„è·¯å¾„
    if not Path(args.checkpoint_path).exists():
        logger.error(f"âŒ Checkpointè·¯å¾„ä¸å­˜åœ¨: {args.checkpoint_path}")
        return
    
    if not Path(args.qa_data_dir).exists():
        logger.error(f"âŒ QAæ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.qa_data_dir}")
        return
    
    try:
        # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
        demo = ComprehensiveDemo()
        demo.run_complete_pipeline(
            checkpoint_path=args.checkpoint_path,
            qa_data_dir=args.qa_data_dir,
            max_qa_items=args.max_qa_items,
            max_length=args.max_length
        )
        
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()