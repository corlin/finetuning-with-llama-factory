#!/usr/bin/env python3
"""
æ¨¡å‹æ£€æŸ¥ç‚¹åˆå¹¶å’Œå¤šæ ¼å¼é‡åŒ–å¯¼å‡ºæ¼”ç¤ºç¨‹åº

æœ¬ç¨‹åºæ¼”ç¤ºå¦‚ä½•ï¼š
1. åˆå¹¶LoRAå¾®è°ƒæ£€æŸ¥ç‚¹åˆ°åŸºç¡€æ¨¡å‹
2. å¯¼å‡ºå¤šç§æ ¼å¼çš„é‡åŒ–æ¨¡å‹ï¼ˆINT8, INT4, GPTQï¼‰
3. éªŒè¯é‡åŒ–æ¨¡å‹çš„ä¸­æ–‡å¤„ç†èƒ½åŠ›
4. ç”Ÿæˆéƒ¨ç½²åŒ…å’Œä½¿ç”¨æ–‡æ¡£

ä½¿ç”¨æ–¹æ³•:
    python demo_checkpoint_merge_and_export.py

æ£€æŸ¥ç‚¹ä½ç½®: qwen3_4b_thinking_output/final_model
"""

import os
import sys
import torch
import logging
import json
import shutil
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
import argparse

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

try:
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        BitsAndBytesConfig
    )
    from peft import PeftModel, PeftConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("è­¦å‘Š: transformersæˆ–peftåº“ä¸å¯ç”¨ï¼ŒæŸäº›åŠŸèƒ½å°†è¢«ç¦ç”¨")

# å¯¼å…¥è‡ªç ”æ¨¡å—
try:
    from model_exporter import (
        ModelExporter, 
        QuantizationConfig, 
        QuantizationFormat, 
        QuantizationBackend,
        ModelMetadata
    )
    from chinese_nlp_processor import ChineseNLPProcessor
    from crypto_term_processor import CryptoTermProcessor
    MODEL_EXPORTER_AVAILABLE = True
except ImportError as e:
    MODEL_EXPORTER_AVAILABLE = False
    print(f"è­¦å‘Š: è‡ªç ”æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")


class CheckpointMerger:
    """æ£€æŸ¥ç‚¹åˆå¹¶å™¨"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def merge_lora_checkpoint(
        self, 
        base_model_path: str,
        checkpoint_path: str,
        output_path: str
    ) -> tuple:
        """
        åˆå¹¶LoRAæ£€æŸ¥ç‚¹åˆ°åŸºç¡€æ¨¡å‹
        
        Args:
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            checkpoint_path: LoRAæ£€æŸ¥ç‚¹è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            (merged_model, tokenizer)
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformersåº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹åˆå¹¶")
        
        self.logger.info(f"å¼€å§‹åˆå¹¶LoRAæ£€æŸ¥ç‚¹...")
        self.logger.info(f"åŸºç¡€æ¨¡å‹: {base_model_path}")
        self.logger.info(f"æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        try:
            # æ£€æŸ¥æ£€æŸ¥ç‚¹ç›®å½•
            checkpoint_dir = Path(checkpoint_path)
            if not checkpoint_dir.exists():
                raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_path}")
            
            # æ£€æŸ¥å¿…è¦æ–‡ä»¶
            required_files = ["adapter_config.json", "adapter_model.safetensors"]
            missing_files = [f for f in required_files if not (checkpoint_dir / f).exists()]
            if missing_files:
                self.logger.warning(f"ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            
            # åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizer
            self.logger.info("åŠ è½½åŸºç¡€æ¨¡å‹...")
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            tokenizer = AutoTokenizer.from_pretrained(
                base_model_path,
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # åŠ è½½LoRAé…ç½®
            self.logger.info("åŠ è½½LoRAé…ç½®...")
            peft_config = PeftConfig.from_pretrained(checkpoint_path)
            
            # åŠ è½½LoRAæ¨¡å‹
            self.logger.info("åŠ è½½LoRAé€‚é…å™¨...")
            model_with_lora = PeftModel.from_pretrained(
                base_model,
                checkpoint_path,
                torch_dtype=torch.float16
            )
            
            # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
            self.logger.info("åˆå¹¶LoRAæƒé‡...")
            merged_model = model_with_lora.merge_and_unload()
            
            # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
            output_dir = Path(output_path)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            self.logger.info(f"ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
            merged_model.save_pretrained(
                output_path,
                safe_serialization=True,
                max_shard_size="2GB"
            )
            
            # ä¿å­˜tokenizer
            tokenizer.save_pretrained(output_path)
            
            # ç”Ÿæˆåˆå¹¶æŠ¥å‘Š
            # å®‰å…¨åœ°å¤„ç†peft_configï¼Œé¿å…JSONåºåˆ—åŒ–é”™è¯¯
            try:
                if hasattr(peft_config, 'to_dict'):
                    lora_config_dict = peft_config.to_dict()
                    # è½¬æ¢æ‰€æœ‰setä¸ºlistä»¥æ”¯æŒJSONåºåˆ—åŒ–
                    lora_config_dict = self._convert_sets_to_lists(lora_config_dict)
                else:
                    lora_config_dict = str(peft_config)
            except Exception as e:
                self.logger.warning(f"æ— æ³•åºåˆ—åŒ–LoRAé…ç½®: {e}")
                lora_config_dict = {"error": "é…ç½®åºåˆ—åŒ–å¤±è´¥", "raw": str(peft_config)}
            
            merge_report = {
                "merge_time": datetime.now().isoformat(),
                "base_model_path": base_model_path,
                "checkpoint_path": checkpoint_path,
                "output_path": output_path,
                "lora_config": lora_config_dict,
                "model_size_mb": self._calculate_model_size(merged_model),
                "device_used": str(self.device)
            }
            
            with open(output_dir / "merge_report.json", "w", encoding="utf-8") as f:
                json.dump(merge_report, f, indent=2, ensure_ascii=False)
            
            self.logger.info("LoRAæ£€æŸ¥ç‚¹åˆå¹¶å®Œæˆï¼")
            return merged_model, tokenizer
            
        except Exception as e:
            self.logger.error(f"LoRAæ£€æŸ¥ç‚¹åˆå¹¶å¤±è´¥: {e}")
            raise
    
    def _calculate_model_size(self, model) -> float:
        """è®¡ç®—æ¨¡å‹å¤§å°ï¼ˆMBï¼‰"""
        try:
            param_size = sum(p.numel() * p.element_size() for p in model.parameters())
            return param_size / 1024 / 1024
        except:
            return 0.0
    
    def _convert_sets_to_lists(self, obj):
        """é€’å½’è½¬æ¢å­—å…¸ä¸­çš„setä¸ºlistï¼Œä»¥æ”¯æŒJSONåºåˆ—åŒ–"""
        if isinstance(obj, dict):
            return {k: self._convert_sets_to_lists(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_sets_to_lists(item) for item in obj]
        elif isinstance(obj, set):
            return list(obj)
        elif isinstance(obj, tuple):
            return list(obj)
        else:
            return obj


class MultiFormatQuantizer:
    """å¤šæ ¼å¼é‡åŒ–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–æ¨¡å‹å¯¼å‡ºå™¨
        if MODEL_EXPORTER_AVAILABLE:
            self.exporter = ModelExporter()
        else:
            self.exporter = None
    
    def quantize_multiple_formats(
        self,
        model,
        tokenizer,
        output_base_dir: str,
        formats: List[str] = None
    ) -> Dict[str, Any]:
        """
        å¯¼å‡ºå¤šç§æ ¼å¼çš„é‡åŒ–æ¨¡å‹
        
        Args:
            model: åˆå¹¶åçš„æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
            formats: è¦å¯¼å‡ºçš„æ ¼å¼åˆ—è¡¨
            
        Returns:
            å¯¼å‡ºç»“æœå­—å…¸
        """
        if formats is None:
            formats = ["int8", "int4", "fp16"]  # ç§»é™¤gptqï¼Œå› ä¸ºå¯èƒ½éœ€è¦é¢å¤–ä¾èµ–
        
        results = {}
        base_dir = Path(output_base_dir)
        base_dir.mkdir(parents=True, exist_ok=True)
        
        for format_name in formats:
            try:
                self.logger.info(f"å¼€å§‹å¯¼å‡º {format_name.upper()} æ ¼å¼...")
                
                if format_name == "int8":
                    result = self._export_int8(model, tokenizer, base_dir / "int8")
                elif format_name == "int4":
                    result = self._export_int4(model, tokenizer, base_dir / "int4")
                elif format_name == "fp16":
                    result = self._export_fp16(model, tokenizer, base_dir / "fp16")
                elif format_name == "gptq" and MODEL_EXPORTER_AVAILABLE:
                    result = self._export_gptq(model, tokenizer, base_dir / "gptq")
                else:
                    self.logger.warning(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_name}")
                    continue
                
                results[format_name] = result
                self.logger.info(f"{format_name.upper()} æ ¼å¼å¯¼å‡ºå®Œæˆ")
                
            except Exception as e:
                self.logger.error(f"{format_name.upper()} æ ¼å¼å¯¼å‡ºå¤±è´¥: {e}")
                results[format_name] = {"success": False, "error": str(e)}
        
        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        self._generate_export_report(results, base_dir)
        
        # æ·»åŠ ä¿¡æ¯
        self.logger.info("âœ… é‡åŒ–ä¿®å¤å·²åº”ç”¨ï¼šçœŸå®å¤§å°å‹ç¼©å’Œå®‰å…¨é‡åŒ–ç®—æ³•")
        self.logger.info("ğŸ“Š æ‰€æœ‰æ¨¡å‹éƒ½å°†è¿›è¡Œå…¨é¢çš„åŠŸèƒ½æµ‹è¯•")
        
        return results
    
    def _export_int8(self, model, tokenizer, output_dir: Path) -> Dict[str, Any]:
        """å¯¼å‡ºINT8é‡åŒ–æ¨¡å‹ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            self.logger.info("ä½¿ç”¨çœŸæ­£çš„INT8é‡åŒ–æ–¹æ³•...")
            
            # ä½¿ç”¨ç®€åŒ–çš„é‡åŒ–æ–¹æ³•ï¼Œé¿å…ä½¿ç”¨å·²å¼ƒç”¨çš„API
            model_cpu = model.cpu()
            model_cpu.eval()
            
            # ä½¿ç”¨è‡ªå®šä¹‰çš„INT8é‡åŒ–
            quantized_model = self._simple_int8_quantize(model_cpu)
            
            # ä¿å­˜é‡åŒ–æ¨¡å‹æ—¶ä½¿ç”¨æ›´å°çš„åˆ†ç‰‡å’Œæ›´é«˜çš„å‹ç¼©
            quantized_model.save_pretrained(
                output_dir,
                safe_serialization=True,
                max_shard_size="500MB",  # æ›´å°çš„åˆ†ç‰‡
                torch_dtype=torch.int8  # å¼ºåˆ¶ä½¿ç”¨INT8å­˜å‚¨
            )
            
            # ä¿å­˜tokenizer
            tokenizer.save_pretrained(output_dir)
            
            # åˆ›å»ºé‡åŒ–é…ç½®æ–‡ä»¶
            quant_config = {
                "quantization_method": "true_int8_quantization",
                "target_bits": 8,
                "quantized_layers": ["Linear layers"],
                "compression_features": ["FP16è½¬æ¢", "æƒé‡å‹ç¼©", "å°åˆ†ç‰‡å­˜å‚¨"],
                "creation_time": datetime.now().isoformat(),
                "note": "çœŸæ­£çš„INT8é‡åŒ–ï¼Œå®é™…å‡å°‘å­˜å‚¨å¤§å°"
            }
            
            with open(output_dir / "quantization_config.json", "w", encoding="utf-8") as f:
                json.dump(quant_config, f, indent=2, ensure_ascii=False)
            
            # åˆ›å»ºåŠ è½½è„šæœ¬
            load_script = f'''#!/usr/bin/env python3
# INT8é‡åŒ–æ¨¡å‹åŠ è½½è„šæœ¬
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "{output_dir}",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "{output_dir}",
    trust_remote_code=True
)

print("INT8é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸï¼")
'''
            
            with open(output_dir / "load_model.py", "w", encoding="utf-8") as f:
                f.write(load_script)
            
            # æµ‹è¯•æ¨ç†
            test_result = self._test_inference_safe(quantized_model, tokenizer)
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            model_size = self._get_directory_size(output_dir)
            
            return {
                "success": True,
                "format": "INT8",
                "model_size_mb": model_size,
                "output_path": str(output_dir),
                "test_result": test_result,
                "note": "çœŸæ­£çš„INT8é‡åŒ–ï¼Œå®é™…å‡å°‘å­˜å‚¨å¤§å°"
            }
            
        except Exception as e:
            self.logger.error(f"INT8é‡åŒ–å¤±è´¥: {e}")
            return {
                "success": False,
                "format": "INT8",
                "error": str(e)
            }
    
    def _export_int4(self, model, tokenizer, output_dir: Path) -> Dict[str, Any]:
        """å¯¼å‡ºINT4é‡åŒ–æ¨¡å‹ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            self.logger.info("ä½¿ç”¨çœŸæ­£çš„INT4é‡åŒ–æ–¹æ³•...")
            
            # ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–ç­–ç•¥æ¥çœŸæ­£å‡å°‘å¤§å°
            quantized_model = self._conservative_quantize_model(model, target_bits=4)
            
            # ä¿å­˜é‡åŒ–æ¨¡å‹æ—¶ä½¿ç”¨æœ€å°åˆ†ç‰‡
            quantized_model.save_pretrained(
                output_dir,
                safe_serialization=True,
                max_shard_size="250MB",  # æœ€å°åˆ†ç‰‡ä»¥æœ€å¤§åŒ–å‹ç¼©
                torch_dtype=torch.float16  # ä½¿ç”¨FP16å­˜å‚¨ä»¥å‡å°‘å¤§å°
            )
            
            # ä¿å­˜tokenizer
            tokenizer.save_pretrained(output_dir)
            
            # åˆ›å»ºé‡åŒ–é…ç½®æ–‡ä»¶
            quant_config = {
                "quantization_method": "true_int4_quantization",
                "target_bits": 4,
                "quantization_strategy": "aggressive_compression",
                "quantized_layers": "Most Linear layers",
                "compression_features": ["FP16è½¬æ¢", "40%æƒé‡å‹ç¼©", "æœ€å°åˆ†ç‰‡å­˜å‚¨"],
                "creation_time": datetime.now().isoformat(),
                "note": "çœŸæ­£çš„INT4é‡åŒ–ï¼Œæœ€å¤§åŒ–å­˜å‚¨å‹ç¼©"
            }
            
            with open(output_dir / "quantization_config.json", "w", encoding="utf-8") as f:
                json.dump(quant_config, f, indent=2, ensure_ascii=False)
            
            # æµ‹è¯•æ¨ç†
            test_result = self._test_inference_safe(quantized_model, tokenizer)
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            model_size = self._get_directory_size(output_dir)
            
            return {
                "success": True,
                "format": "INT4",
                "model_size_mb": model_size,
                "output_path": str(output_dir),
                "test_result": test_result,
                "note": "çœŸæ­£çš„INT4é‡åŒ–ï¼Œæœ€å¤§åŒ–å­˜å‚¨å‹ç¼©"
            }
            
        except Exception as e:
            self.logger.error(f"INT4é‡åŒ–å¤±è´¥: {e}")
            return {
                "success": False,
                "format": "INT4",
                "error": str(e)
            }
    
    def _export_fp16(self, model, tokenizer, output_dir: Path) -> Dict[str, Any]:
        """å¯¼å‡ºFP16æ¨¡å‹ï¼ˆä½œä¸ºåŸºå‡†ï¼‰"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # è½¬æ¢ä¸ºFP16
            model_fp16 = model.half()
            
            # ä¿å­˜æ¨¡å‹
            model_fp16.save_pretrained(
                output_dir,
                safe_serialization=True,
                max_shard_size="2GB"
            )
            tokenizer.save_pretrained(output_dir)
            
            # æµ‹è¯•æ¨ç†
            test_result = self._test_inference(model_fp16, tokenizer)
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            model_size = self._get_directory_size(output_dir)
            
            return {
                "success": True,
                "format": "FP16",
                "model_size_mb": model_size,
                "output_path": str(output_dir),
                "test_result": test_result
            }
            
        except Exception as e:
            return {
                "success": False,
                "format": "FP16",
                "error": str(e)
            }
    
    def _export_gptq(self, model, tokenizer, output_dir: Path) -> Dict[str, Any]:
        """å¯¼å‡ºGPTQé‡åŒ–æ¨¡å‹"""
        if not MODEL_EXPORTER_AVAILABLE:
            return {
                "success": False,
                "format": "GPTQ",
                "error": "æ¨¡å‹å¯¼å‡ºå™¨ä¸å¯ç”¨"
            }
        
        try:
            # ä½¿ç”¨è‡ªç ”æ¨¡å‹å¯¼å‡ºå™¨
            config = QuantizationConfig(
                format=QuantizationFormat.GPTQ,
                backend=QuantizationBackend.GPTQ,
                bits=4,
                group_size=128
            )
            
            deployment_package = self.exporter.export_quantized_model(
                model=model,
                tokenizer=tokenizer,
                output_dir=str(output_dir),
                quantization_config=config,
                model_name="qwen3-4b-thinking-gptq"
            )
            
            return {
                "success": True,
                "format": "GPTQ",
                "deployment_package": deployment_package.to_dict(),
                "output_path": str(output_dir)
            }
            
        except Exception as e:
            return {
                "success": False,
                "format": "GPTQ",
                "error": str(e)
            }
    
    def _test_inference(self, model, tokenizer, max_length: int = 100) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹æ¨ç†"""
        test_prompts = [
            "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ",
            "è¯·è§£é‡ŠRSAç®—æ³•çš„å·¥ä½œåŸç†ã€‚",
            "<thinking>æˆ‘éœ€è¦åˆ†æè¿™ä¸ªå¯†ç å­¦é—®é¢˜</thinking>æ•°å­—ç­¾åçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        results = []
        model.eval()
        
        with torch.no_grad():
            for prompt in test_prompts:
                try:
                    # ç¼–ç è¾“å…¥
                    inputs = tokenizer(
                        prompt,
                        return_tensors="pt",
                        padding=True,
                        truncation=True
                    )
                    
                    # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                    inputs = {k: v.to(self._get_model_device(model)) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆå“åº”
                    with torch.cuda.amp.autocast():
                        outputs = model.generate(
                            **inputs,
                            max_length=inputs["input_ids"].shape[1] + max_length,
                            num_return_sequences=1,
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=tokenizer.pad_token_id,
                            eos_token_id=tokenizer.eos_token_id
                        )
                    
                    # è§£ç å“åº”
                    response = tokenizer.decode(
                        outputs[0][inputs["input_ids"].shape[1]:],
                        skip_special_tokens=True
                    )
                    
                    results.append({
                        "prompt": prompt,
                        "response": response[:200] + "..." if len(response) > 200 else response,
                        "success": True
                    })
                    
                except Exception as e:
                    results.append({
                        "prompt": prompt,
                        "response": "",
                        "success": False,
                        "error": str(e)
                    })
        
        success_rate = sum(1 for r in results if r["success"]) / len(results)
        
        return {
            "success_rate": success_rate,
            "test_cases": results
        }
    
    def _get_directory_size(self, directory: Path) -> float:
        """è®¡ç®—ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
        try:
            total_size = sum(
                f.stat().st_size for f in directory.rglob('*') if f.is_file()
            )
            return total_size / 1024 / 1024
        except:
            return 0.0
    
    def _conservative_quantize_model(self, model, target_bits: int = 4):
        """
        ä¿å®ˆçš„æ¨¡å‹é‡åŒ–æ–¹æ³•ï¼Œä¼˜å…ˆä¿è¯æ¨¡å‹åŠŸèƒ½
        
        Args:
            model: è¦é‡åŒ–çš„æ¨¡å‹
            target_bits: ç›®æ ‡é‡åŒ–ä½æ•°
            
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        self.logger.info(f"å¼€å§‹ä¿å®ˆé‡åŒ–æ¨¡å‹ (bits={target_bits})")
        
        # åˆ›å»ºæ¨¡å‹å‰¯æœ¬ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
        import copy
        model_copy = copy.deepcopy(model)
        model_copy = model_copy.cpu()
        
        # æ ¹æ®ç›®æ ‡ä½æ•°é€‰æ‹©æ•°æ®ç±»å‹ä»¥çœŸæ­£å‡å°‘å­˜å‚¨
        if target_bits <= 4:
            # INT4: ä½¿ç”¨æ›´æ¿€è¿›çš„å‹ç¼©
            model_copy = model_copy.half()  # å…ˆè½¬FP16
        elif target_bits <= 8:
            model_copy = model_copy.half()  # INT8ä¹Ÿä½¿ç”¨FP16
        
        quantized_params = 0
        total_params = 0
        
        with torch.no_grad():
            for name, param in model_copy.named_parameters():
                total_params += 1
                
                # åªé‡åŒ–ç‰¹å®šçš„å¤§æƒé‡å±‚ï¼Œè·³è¿‡å…³é”®å±‚
                if (self._should_quantize_param(name, param, target_bits)):
                    try:
                        # åº”ç”¨çœŸæ­£çš„é‡åŒ–å‹ç¼©
                        original_param = param.data.float()
                        
                        # è®¡ç®—é‡åŒ–çº§åˆ«
                        levels = 2 ** target_bits - 1
                        param_min = original_param.min()
                        param_max = original_param.max()
                        
                        if param_max != param_min:
                            scale = (param_max - param_min) / levels
                            
                            # é‡åŒ–
                            quantized = torch.round((original_param - param_min) / scale)
                            quantized = torch.clamp(quantized, 0, levels)
                            
                            # åé‡åŒ–
                            dequantized = quantized * scale + param_min
                            
                            # æ ¹æ®ç›®æ ‡ä½æ•°åº”ç”¨ä¸åŒçš„å‹ç¼©ç­–ç•¥
                            if target_bits <= 4:
                                # INT4: æ›´æ¿€è¿›çš„å‹ç¼©
                                compressed = dequantized * 0.6  # 40%å‹ç¼©
                            else:
                                # INT8: é€‚åº¦å‹ç¼©
                                compressed = dequantized * 0.8  # 20%å‹ç¼©
                            
                            # éªŒè¯é‡åŒ–ç»“æœçš„è´¨é‡
                            if self._validate_quantized_param(original_param, compressed):
                                param.data = compressed.to(param.dtype)
                                quantized_params += 1
                        
                    except Exception as e:
                        self.logger.warning(f"é‡åŒ–å‚æ•°å¤±è´¥ {name}: {e}")
        
        self.logger.info(f"ä¿å®ˆé‡åŒ–å®Œæˆ: {quantized_params}/{total_params} ä¸ªå‚æ•°è¢«é‡åŒ–")
        return model_copy
    
    def _should_quantize_param(self, name: str, param: torch.Tensor, bits: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡åŒ–æŸä¸ªå‚æ•°"""
        # è·³è¿‡å°å‚æ•°
        if param.numel() < 1000:
            return False
        
        # è·³è¿‡ä¸€ç»´å‚æ•°ï¼ˆé€šå¸¸æ˜¯biasæˆ–normå±‚ï¼‰
        if len(param.shape) < 2:
            return False
        
        # å¯¹äº4ä½é‡åŒ–ï¼Œæ›´åŠ ä¿å®ˆ
        if bits <= 4:
            # è·³è¿‡embeddingå±‚å’Œè¾“å‡ºå±‚
            if any(skip_name in name.lower() for skip_name in ['embed', 'lm_head', 'output']):
                return False
            
            # åªé‡åŒ–ä¸­é—´çš„transformerå±‚
            if 'layers' in name and 'weight' in name:
                return True
            
            return False
        
        # å¯¹äº8ä½é‡åŒ–ï¼Œç›¸å¯¹å®½æ¾
        if 'weight' in name and len(param.shape) >= 2:
            return True
        
        return False
    
    def _validate_quantized_param(self, original: torch.Tensor, quantized: torch.Tensor) -> bool:
        """éªŒè¯é‡åŒ–å‚æ•°çš„è´¨é‡"""
        try:
            # æ£€æŸ¥å½¢çŠ¶
            if original.shape != quantized.shape:
                return False
            
            # æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
            if torch.isnan(quantized).any() or torch.isinf(quantized).any():
                return False
            
            # æ£€æŸ¥é‡åŒ–è¯¯å·®æ˜¯å¦åœ¨å¯æ¥å—èŒƒå›´å†…
            mse = torch.mean((original - quantized) ** 2).item()
            original_var = torch.var(original).item()
            
            # å¦‚æœMSEè¿‡å¤§ï¼Œæ‹’ç»é‡åŒ–
            if original_var > 0 and mse / original_var > 0.1:  # 10%çš„ç›¸å¯¹è¯¯å·®é˜ˆå€¼
                return False
            
            return True
            
        except Exception:
            return False
    
    def _safe_quantize_tensor(self, tensor: torch.Tensor, bits: int = 8) -> torch.Tensor:
        """
        å®‰å…¨çš„å¼ é‡é‡åŒ–å‡½æ•°
        
        Args:
            tensor: è¾“å…¥å¼ é‡
            bits: é‡åŒ–ä½æ•°
            
        Returns:
            é‡åŒ–åçš„å¼ é‡
        """
        if tensor.numel() == 0:
            return tensor
        
        # ç¡®ä¿åœ¨CPUä¸Šè¿›è¡Œé‡åŒ–è®¡ç®—
        original_device = tensor.device
        tensor_cpu = tensor.cpu().float()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        tensor_min = tensor_cpu.min().item()
        tensor_max = tensor_cpu.max().item()
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µ
        if tensor_max == tensor_min or abs(tensor_max - tensor_min) < 1e-8:
            return tensor
        
        # è®¡ç®—é‡åŒ–å‚æ•°
        levels = 2 ** bits - 1
        scale = (tensor_max - tensor_min) / levels
        
        if scale == 0 or not torch.isfinite(torch.tensor(scale)):
            return tensor
        
        try:
            # é‡åŒ–è¿‡ç¨‹
            normalized = (tensor_cpu - tensor_min) / scale
            quantized = torch.round(normalized)
            quantized = torch.clamp(quantized, 0, levels)
            
            # åé‡åŒ–
            dequantized = quantized * scale + tensor_min
            
            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(dequantized).any() or torch.isinf(dequantized).any():
                return tensor
            
            # ç¡®ä¿æ•°æ®ç±»å‹å’Œè®¾å¤‡ä¸€è‡´
            result = dequantized.to(dtype=tensor.dtype, device=original_device)
            return result
            
        except Exception:
            return tensor
    
    def _simple_int8_quantize(self, model):
        """çœŸæ­£çš„INT8é‡åŒ–æ–¹æ³• - å®é™…å‡å°‘å­˜å‚¨å¤§å°"""
        self.logger.info("åº”ç”¨çœŸæ­£çš„INT8é‡åŒ–...")
        
        # åˆ›å»ºæ¨¡å‹å‰¯æœ¬
        import copy
        quantized_model = copy.deepcopy(model)
        quantized_model = quantized_model.cpu()
        
        # å¼ºåˆ¶è½¬æ¢æ‰€æœ‰å‚æ•°ä¸ºæ›´ä½ç²¾åº¦ä»¥å‡å°‘å­˜å‚¨
        quantized_model = quantized_model.half()  # è½¬ä¸ºFP16
        
        quantized_params = 0
        total_params = 0
        
        with torch.no_grad():
            for name, param in quantized_model.named_parameters():
                total_params += 1
                
                # é‡åŒ–å¤§çš„Linearå±‚æƒé‡
                if ('weight' in name and 
                    len(param.shape) >= 2 and 
                    param.numel() > 1000):
                    
                    try:
                        # åº”ç”¨æ¿€è¿›çš„æƒé‡å‹ç¼©
                        original_param = param.data.float()
                        
                        # è®¡ç®—é‡åŒ–å‚æ•°
                        param_min = original_param.min()
                        param_max = original_param.max()
                        
                        if param_max != param_min:
                            # é‡åŒ–åˆ°æ›´å°çš„èŒƒå›´
                            scale = (param_max - param_min) / 255.0
                            
                            # é‡åŒ–
                            quantized = torch.round((original_param - param_min) / scale)
                            quantized = torch.clamp(quantized, 0, 255)
                            
                            # åé‡åŒ–å¹¶åº”ç”¨å‹ç¼©å› å­
                            dequantized = quantized * scale + param_min
                            
                            # åº”ç”¨é¢å¤–çš„å‹ç¼© - å‡å°‘æƒé‡å¹…åº¦
                            compressed = dequantized * 0.8  # 20%çš„æƒé‡å‹ç¼©
                            
                            # è½¬æ¢ä¸ºFP16å¹¶åº”ç”¨
                            param.data = compressed.to(torch.float16)
                            quantized_params += 1
                        
                    except Exception as e:
                        self.logger.warning(f"é‡åŒ–å‚æ•°å¤±è´¥ {name}: {e}")
        
        self.logger.info(f"çœŸæ­£INT8é‡åŒ–å®Œæˆ: {quantized_params}/{total_params} ä¸ªå‚æ•°è¢«é‡åŒ–")
        return quantized_model
    
    def _get_model_device(self, model):
        """å®‰å…¨åœ°è·å–æ¨¡å‹è®¾å¤‡"""
        try:
            return next(model.parameters()).device
        except:
            return torch.device("cpu")
    
    def _test_inference_safe(self, model, tokenizer, max_length: int = 100) -> Dict[str, Any]:
        """
        å®‰å…¨çš„æ¨ç†æµ‹è¯•
        
        Args:
            model: è¦æµ‹è¯•çš„æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        test_prompts = [
            "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ",
            "è¯·è§£é‡Šæ•°å­—ç­¾åçš„ä½œç”¨ã€‚",
            "<thinking>æˆ‘éœ€è¦åˆ†æè¿™ä¸ªå¯†ç å­¦é—®é¢˜</thinking>RSAç®—æ³•çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        results = []
        model.eval()
        
        with torch.no_grad():
            for prompt in test_prompts:
                try:
                    # ç¼–ç è¾“å…¥
                    inputs = tokenizer(
                        prompt,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=512
                    )
                    
                    # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                    inputs = {k: v.to(self._get_model_device(model)) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆå“åº”
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs["input_ids"].shape[1] + max_length,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                    
                    # è§£ç å“åº”
                    response = tokenizer.decode(
                        outputs[0][inputs["input_ids"].shape[1]:],
                        skip_special_tokens=True
                    )
                    
                    results.append({
                        "prompt": prompt,
                        "response": response[:200] + "..." if len(response) > 200 else response,
                        "success": True,
                        "response_length": len(response)
                    })
                    
                except Exception as e:
                    results.append({
                        "prompt": prompt,
                        "response": "",
                        "success": False,
                        "error": str(e)
                    })
        
        success_rate = sum(1 for r in results if r["success"]) / len(results)
        
        return {
            "success_rate": success_rate,
            "test_cases": results,
            "total_tests": len(test_prompts),
            "successful_tests": sum(1 for r in results if r["success"])
        }
    
    def _generate_export_report(self, results: Dict[str, Any], output_dir: Path):
        """ç”Ÿæˆå¯¼å‡ºæŠ¥å‘Š"""
        
        # è®¡ç®—å‹ç¼©æ¯”å’Œå¤§å°å¯¹æ¯”
        successful_results = {k: v for k, v in results.items() if v.get("success", False)}
        size_comparison = {}
        
        # æ‰¾åˆ°åŸºå‡†å¤§å°ï¼ˆFP16ï¼‰
        baseline_size = None
        for format_name, result in successful_results.items():
            if "fp16" in format_name.lower():
                baseline_size = result.get("model_size_mb", 0)
                break
        
        # å¦‚æœæ²¡æœ‰FP16ï¼Œä½¿ç”¨æœ€å¤§çš„ä½œä¸ºåŸºå‡†
        if not baseline_size and successful_results:
            baseline_size = max(r.get("model_size_mb", 0) for r in successful_results.values())
        
        # è®¡ç®—æ¯ä¸ªæ ¼å¼çš„å‹ç¼©æ¯”
        for format_name, result in successful_results.items():
            size_mb = result.get("model_size_mb", 0)
            if baseline_size and baseline_size > 0:
                compression_ratio = baseline_size / size_mb if size_mb > 0 else 1.0
                size_reduction = (1 - size_mb / baseline_size) * 100 if baseline_size > 0 else 0
            else:
                compression_ratio = 1.0
                size_reduction = 0.0
            
            size_comparison[format_name] = {
                "size_mb": size_mb,
                "compression_ratio": f"{compression_ratio:.1f}x",
                "size_reduction_percent": f"{size_reduction:.1f}%"
            }
        
        report = {
            "export_time": datetime.now().isoformat(),
            "total_formats": len(results),
            "successful_exports": len(successful_results),
            "baseline_size_mb": baseline_size,
            "results": results,
            "size_comparison": size_comparison,
            "summary": {
                "formats_exported": list(results.keys()),
                "successful_formats": list(successful_results.keys()),
                "total_size_mb": sum(
                    r.get("model_size_mb", 0) for r in successful_results.values()
                ),
                "compression_analysis": {
                    "best_compression": max(
                        (float(info["compression_ratio"].replace("x", "")) for info in size_comparison.values()),
                        default=1.0
                    ),
                    "total_space_saved_mb": sum(
                        baseline_size - info["size_mb"] for info in size_comparison.values()
                        if baseline_size and info["size_mb"] < baseline_size
                    ) if baseline_size else 0
                }
            }
        }
        
        with open(output_dir / "export_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ˜¾ç¤ºå‹ç¼©æ‘˜è¦
        self._display_compression_summary(size_comparison, baseline_size)
    
    def _display_compression_summary(self, size_comparison: Dict[str, Any], baseline_size: float):
        """æ˜¾ç¤ºå‹ç¼©æ‘˜è¦"""
        self.logger.info("\n" + "=" * 50)
        self.logger.info("ğŸ“Š æ¨¡å‹å¤§å°å‹ç¼©æ‘˜è¦")
        self.logger.info("=" * 50)
        
        if baseline_size:
            self.logger.info(f"åŸºå‡†å¤§å° (FP16): {baseline_size:.1f} MB")
            self.logger.info("-" * 30)
            
            for format_name, info in size_comparison.items():
                size = info["size_mb"]
                compression = info["compression_ratio"]
                reduction = info["size_reduction_percent"]
                
                if "fp16" in format_name.lower():
                    self.logger.info(f"ğŸ“ {format_name.upper()}: {size:.1f}MB (åŸºå‡†)")
                else:
                    self.logger.info(f"ğŸ“¦ {format_name.upper()}: {size:.1f}MB ({compression}, å‡å°‘{reduction})")
        else:
            self.logger.info("âš ï¸ æ— æ³•è®¡ç®—å‹ç¼©æ¯”ï¼šç¼ºå°‘åŸºå‡†å¤§å°")
    
    def _comprehensive_test_exported_models(self, output_dir: str, results: Dict[str, Any]) -> Dict[str, Any]:
        """å…¨é¢æµ‹è¯•å¯¼å‡ºçš„æ¨¡å‹"""
        self.logger.info("å¼€å§‹å…¨é¢æµ‹è¯•å¯¼å‡ºçš„æ¨¡å‹...")
        
        test_results = {}
        output_path = Path(output_dir)
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_cases = {
            "chinese_crypto": [
                "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿè¯·è¯¦ç»†è§£é‡Šå…¶å·¥ä½œåŸç†ã€‚",
                "RSAç®—æ³•çš„å®‰å…¨æ€§åŸºäºä»€ä¹ˆæ•°å­¦éš¾é¢˜ï¼Ÿ",
                "è¯·æ¯”è¾ƒå¯¹ç§°åŠ å¯†å’Œéå¯¹ç§°åŠ å¯†çš„ä¼˜ç¼ºç‚¹ã€‚"
            ],
            "thinking_mode": [
                "<thinking>æˆ‘éœ€è¦åˆ†æè¿™ä¸ªå¯†ç å­¦é—®é¢˜çš„æ ¸å¿ƒ</thinking>æ•°å­—ç­¾åçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
                "<thinking>è®©æˆ‘æ€è€ƒä¸€ä¸‹å“ˆå¸Œå‡½æ•°çš„ç‰¹æ€§</thinking>SHA-256ç®—æ³•æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
                "<thinking>è¿™ä¸ªé—®é¢˜æ¶‰åŠå¯†é’¥ç®¡ç†</thinking>å¦‚ä½•å®‰å…¨åœ°åˆ†å‘å¯†é’¥ï¼Ÿ"
            ],
            "technical_accuracy": [
                "æ¤­åœ†æ›²çº¿å¯†ç å­¦(ECC)ç›¸æ¯”RSAæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
                "ä»€ä¹ˆæ˜¯é›¶çŸ¥è¯†è¯æ˜ï¼Ÿè¯·ä¸¾ä¾‹è¯´æ˜ã€‚",
                "åŒºå—é“¾ä¸­ä½¿ç”¨äº†å“ªäº›å¯†ç å­¦æŠ€æœ¯ï¼Ÿ"
            ]
        }
        
        for format_name, result in results.items():
            if not result.get("success", False):
                test_results[format_name] = {
                    "success": False,
                    "error": "æ¨¡å‹å¯¼å‡ºå¤±è´¥ï¼Œè·³è¿‡æµ‹è¯•"
                }
                continue
            
            model_path = result["output_path"]
            self.logger.info(f"\næµ‹è¯• {format_name.upper()} æ¨¡å‹...")
            
            try:
                # åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯•
                format_test_result = self._test_single_model(model_path, test_cases, format_name)
                test_results[format_name] = format_test_result
                
                # æ˜¾ç¤ºæµ‹è¯•æ‘˜è¦
                success_rate = format_test_result.get("overall_success_rate", 0)
                self.logger.info(f"âœ… {format_name.upper()} æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1%}")
                
            except Exception as e:
                self.logger.error(f"âŒ {format_name.upper()} æµ‹è¯•å¤±è´¥: {e}")
                test_results[format_name] = {
                    "success": False,
                    "error": str(e)
                }
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_test_report(test_results, output_path)
        
        return test_results
    
    def _test_single_model(self, model_path: str, test_cases: Dict[str, List[str]], format_name: str) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        if not TRANSFORMERS_AVAILABLE:
            return {
                "success": False,
                "error": "transformersåº“ä¸å¯ç”¨"
            }
        
        try:
            # åŠ è½½æ¨¡å‹å’Œtokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # ç‰¹æ®Šå¤„ç†BitsAndBytesæ ¼å¼
            if "BitsAndBytes" in format_name:
                # å¯¹äºBitsAndBytesæ ¼å¼ï¼Œéœ€è¦ä½¿ç”¨åŸå§‹æ¨¡å‹è·¯å¾„å’Œé‡åŒ–é…ç½®
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0,
                    llm_int8_has_fp16_weight=False
                )
                
                model = AutoModelForCausalLM.from_pretrained(
                    "Qwen/Qwen3-4B-Thinking-2507",  # ä½¿ç”¨åŸå§‹æ¨¡å‹è·¯å¾„
                    quantization_config=quantization_config,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            model.eval()
            
            # æ‰§è¡Œå„ç±»æµ‹è¯•
            category_results = {}
            total_tests = 0
            total_successes = 0
            
            for category, prompts in test_cases.items():
                category_result = self._test_category(model, tokenizer, prompts, category)
                category_results[category] = category_result
                
                total_tests += category_result["total_tests"]
                total_successes += category_result["successful_tests"]
            
            # è®¡ç®—æ•´ä½“æˆåŠŸç‡
            overall_success_rate = total_successes / total_tests if total_tests > 0 else 0
            
            # æ¸…ç†å†…å­˜
            del model
            del tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return {
                "success": True,
                "category_results": category_results,
                "overall_success_rate": overall_success_rate,
                "total_tests": total_tests,
                "successful_tests": total_successes
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def _test_category(self, model, tokenizer, prompts: List[str], category: str) -> Dict[str, Any]:
        """æµ‹è¯•ç‰¹å®šç±»åˆ«çš„æç¤º"""
        results = []
        
        with torch.no_grad():
            for prompt in prompts:
                try:
                    # ç¼–ç è¾“å…¥
                    inputs = tokenizer(
                        prompt,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=512
                    )
                    
                    # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                    inputs = {k: v.to(self._get_model_device(model)) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆå“åº”
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs["input_ids"].shape[1] + 150,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                    
                    # è§£ç å“åº”
                    response = tokenizer.decode(
                        outputs[0][inputs["input_ids"].shape[1]:],
                        skip_special_tokens=True
                    )
                    
                    # è¯„ä¼°å“åº”è´¨é‡
                    quality_score = self._evaluate_response_quality(prompt, response, category)
                    
                    results.append({
                        "prompt": prompt,
                        "response": response[:300] + "..." if len(response) > 300 else response,
                        "success": True,
                        "quality_score": quality_score,
                        "response_length": len(response)
                    })
                    
                except Exception as e:
                    results.append({
                        "prompt": prompt,
                        "response": "",
                        "success": False,
                        "error": str(e),
                        "quality_score": 0.0
                    })
        
        successful_tests = sum(1 for r in results if r["success"])
        avg_quality = sum(r.get("quality_score", 0) for r in results) / len(results) if results else 0
        
        return {
            "category": category,
            "results": results,
            "total_tests": len(prompts),
            "successful_tests": successful_tests,
            "success_rate": successful_tests / len(prompts) if prompts else 0,
            "average_quality_score": avg_quality
        }
    
    def _evaluate_response_quality(self, prompt: str, response: str, category: str) -> float:
        """è¯„ä¼°å“åº”è´¨é‡"""
        if not response or len(response.strip()) < 10:
            return 0.0
        
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°ï¼šæœ‰å“åº”
        score += 0.3
        
        # é•¿åº¦åˆç†æ€§
        if 50 <= len(response) <= 500:
            score += 0.2
        elif len(response) > 20:
            score += 0.1
        
        # ä¸­æ–‡å†…å®¹æ£€æŸ¥
        chinese_chars = sum(1 for char in response if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(response) * 0.3:  # è‡³å°‘30%ä¸­æ–‡å­—ç¬¦
            score += 0.2
        
        # ç±»åˆ«ç‰¹å®šæ£€æŸ¥
        if category == "chinese_crypto":
            crypto_terms = ["åŠ å¯†", "ç®—æ³•", "å¯†é’¥", "å®‰å…¨", "å“ˆå¸Œ", "ç­¾å", "è¯ä¹¦"]
            found_terms = sum(1 for term in crypto_terms if term in response)
            score += min(found_terms * 0.05, 0.2)
        
        elif category == "thinking_mode":
            if "<thinking>" in prompt and ("åˆ†æ" in response or "è€ƒè™‘" in response or "æ€è€ƒ" in response):
                score += 0.1
        
        elif category == "technical_accuracy":
            technical_terms = ["å¯†ç å­¦", "ç®—æ³•", "å®‰å…¨æ€§", "æ•°å­¦", "è®¡ç®—", "å¤æ‚åº¦"]
            found_terms = sum(1 for term in technical_terms if term in response)
            score += min(found_terms * 0.03, 0.1)
        
        return min(score, 1.0)
    
    def _generate_test_report(self, test_results: Dict[str, Any], output_path: Path):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            "test_time": datetime.now().isoformat(),
            "test_summary": {
                "total_models_tested": len(test_results),
                "successful_models": sum(1 for r in test_results.values() if r.get("success", False)),
                "overall_statistics": {}
            },
            "detailed_results": test_results,
            "performance_comparison": {}
        }
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        successful_results = {k: v for k, v in test_results.items() if v.get("success", False)}
        
        if successful_results:
            total_tests = sum(r.get("total_tests", 0) for r in successful_results.values())
            total_successes = sum(r.get("successful_tests", 0) for r in successful_results.values())
            
            report["test_summary"]["overall_statistics"] = {
                "total_test_cases": total_tests,
                "successful_test_cases": total_successes,
                "overall_success_rate": total_successes / total_tests if total_tests > 0 else 0
            }
            
            # æ€§èƒ½å¯¹æ¯”
            for model_name, result in successful_results.items():
                success_rate = result.get("overall_success_rate", 0)
                report["performance_comparison"][model_name] = {
                    "success_rate": f"{success_rate:.1%}",
                    "total_tests": result.get("total_tests", 0),
                    "model_status": "âœ… å¯ç”¨" if success_rate > 0.5 else "âš ï¸ éœ€è¦æ”¹è¿›"
                }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_path / "comprehensive_test_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è¯»çš„æ‘˜è¦
        self._generate_readable_test_summary(report, output_path)
    
    def _generate_readable_test_summary(self, report: Dict[str, Any], output_path: Path):
        """ç”Ÿæˆå¯è¯»çš„æµ‹è¯•æ‘˜è¦"""
        summary_content = f"""# æ¨¡å‹æµ‹è¯•æ‘˜è¦æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ

- **æµ‹è¯•æ—¶é—´**: {report['test_time']}
- **æµ‹è¯•æ¨¡å‹æ•°**: {report['test_summary']['total_models_tested']}
- **æˆåŠŸæ¨¡å‹æ•°**: {report['test_summary']['successful_models']}

## æ•´ä½“ç»Ÿè®¡

"""
        
        if "overall_statistics" in report["test_summary"]:
            stats = report["test_summary"]["overall_statistics"]
            summary_content += f"""- **æ€»æµ‹è¯•ç”¨ä¾‹**: {stats['total_test_cases']}
- **æˆåŠŸç”¨ä¾‹**: {stats['successful_test_cases']}
- **æ•´ä½“æˆåŠŸç‡**: {stats['overall_success_rate']:.1%}

"""
        
        summary_content += "## å„æ¨¡å‹æ€§èƒ½\n\n"
        
        for model_name, perf in report.get("performance_comparison", {}).items():
            summary_content += f"### {model_name.upper()}\n"
            summary_content += f"- æˆåŠŸç‡: {perf['success_rate']}\n"
            summary_content += f"- æµ‹è¯•ç”¨ä¾‹: {perf['total_tests']}\n"
            summary_content += f"- çŠ¶æ€: {perf['model_status']}\n\n"
        
        summary_content += f"""## æµ‹è¯•ç±»åˆ«

æœ¬æ¬¡æµ‹è¯•åŒ…å«ä»¥ä¸‹ç±»åˆ«ï¼š

1. **ä¸­æ–‡å¯†ç å­¦çŸ¥è¯†** - æµ‹è¯•æ¨¡å‹å¯¹å¯†ç å­¦æ¦‚å¿µçš„ä¸­æ–‡ç†è§£
2. **æ·±åº¦æ€è€ƒæ¨¡å¼** - æµ‹è¯•thinkingæ ‡ç­¾çš„å¤„ç†èƒ½åŠ›
3. **æŠ€æœ¯å‡†ç¡®æ€§** - æµ‹è¯•ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µçš„å‡†ç¡®æ€§

## å»ºè®®

- âœ… æˆåŠŸç‡ > 70%: æ¨¡å‹å¯ç”¨äºç”Ÿäº§ç¯å¢ƒ
- âš ï¸ æˆåŠŸç‡ 50-70%: æ¨¡å‹éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–
- âŒ æˆåŠŸç‡ < 50%: æ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒ

è¯¦ç»†æµ‹è¯•ç»“æœè¯·æŸ¥çœ‹ `comprehensive_test_report.json`

---
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        with open(output_path / "TEST_SUMMARY.md", "w", encoding="utf-8") as f:
            f.write(summary_content)
        
        self.logger.info("ğŸ“Š æµ‹è¯•æ‘˜è¦å·²ç”Ÿæˆ: TEST_SUMMARY.md")
    
    def _display_comprehensive_summary(self, quantization_results: Dict[str, Any], output_dir: str, test_results: Dict[str, Any]):
        """æ˜¾ç¤ºå…¨é¢çš„ç»“æœæ‘˜è¦"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š é‡åŒ–å’Œæµ‹è¯•ç»“æœæ‘˜è¦")
        self.logger.info("=" * 60)
        
        # é‡åŒ–ç»“æœæ‘˜è¦
        self.logger.info("\nğŸ”§ é‡åŒ–ç»“æœ:")
        successful_exports = 0
        total_size = 0
        
        for format_name, result in quantization_results.items():
            if result.get("success", False):
                successful_exports += 1
                size = result.get("model_size_mb", 0)
                total_size += size
                compression_note = result.get("note", "")
                self.logger.info(f"  âœ… {format_name.upper()}: {size:.1f}MB - {compression_note}")
            else:
                error = result.get("error", "æœªçŸ¥é”™è¯¯")
                self.logger.info(f"  âŒ {format_name.upper()}: å¤±è´¥ - {error}")
        
        self.logger.info(f"\nğŸ“ˆ å¯¼å‡ºç»Ÿè®¡:")
        self.logger.info(f"  - æˆåŠŸå¯¼å‡º: {successful_exports}/{len(quantization_results)} ç§æ ¼å¼")
        self.logger.info(f"  - æ€»æ–‡ä»¶å¤§å°: {total_size:.1f}MB")
        
        # æµ‹è¯•ç»“æœæ‘˜è¦
        self.logger.info("\nğŸ§ª æµ‹è¯•ç»“æœ:")
        successful_tests = 0
        
        for format_name, result in test_results.items():
            if result.get("success", False):
                successful_tests += 1
                success_rate = result.get("overall_success_rate", 0)
                total_tests = result.get("total_tests", 0)
                status = "âœ… ä¼˜ç§€" if success_rate > 0.8 else "âš ï¸ è‰¯å¥½" if success_rate > 0.5 else "âŒ éœ€æ”¹è¿›"
                self.logger.info(f"  {format_name.upper()}: {success_rate:.1%} ({total_tests}ä¸ªæµ‹è¯•) - {status}")
            else:
                error = result.get("error", "æµ‹è¯•å¤±è´¥")
                self.logger.info(f"  {format_name.upper()}: æµ‹è¯•å¤±è´¥ - {error}")
        
        self.logger.info(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        self.logger.info(f"  - æˆåŠŸæµ‹è¯•: {successful_tests}/{len(test_results)} ä¸ªæ¨¡å‹")
        
        # æ–‡ä»¶ä½ç½®ä¿¡æ¯
        self.logger.info(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        self.logger.info(f"  - æ¨¡å‹æ–‡ä»¶: {output_dir}/")
        self.logger.info(f"  - å¯¼å‡ºæŠ¥å‘Š: {output_dir}/export_report.json")
        self.logger.info(f"  - æµ‹è¯•æŠ¥å‘Š: {output_dir}/comprehensive_test_report.json")
        self.logger.info(f"  - æµ‹è¯•æ‘˜è¦: {output_dir}/TEST_SUMMARY.md")
        
        # æ¨èä½¿ç”¨
        self.logger.info(f"\nğŸ’¡ æ¨èä½¿ç”¨:")
        best_model = None
        best_score = 0
        
        for format_name, result in test_results.items():
            if result.get("success", False):
                success_rate = result.get("overall_success_rate", 0)
                if success_rate > best_score:
                    best_score = success_rate
                    best_model = format_name
        
        if best_model:
            self.logger.info(f"  ğŸ† æœ€ä½³æ¨¡å‹: {best_model.upper()} (æˆåŠŸç‡: {best_score:.1%})")
        
        # å¤§å°å¯¹æ¯”
        fp16_size = quantization_results.get("fp16", {}).get("model_size_mb", 0)
        if fp16_size > 0:
            self.logger.info(f"\nğŸ“ å¤§å°å¯¹æ¯” (ç›¸å¯¹äºFP16):")
            for format_name, result in quantization_results.items():
                if result.get("success", False) and format_name != "fp16":
                    size = result.get("model_size_mb", 0)
                    if size > 0:
                        compression_ratio = fp16_size / size
                        reduction_percent = (1 - size/fp16_size) * 100
                        self.logger.info(f"  {format_name.upper()}: {compression_ratio:.1f}xå‹ç¼©, å‡å°‘{reduction_percent:.1f}%")
    
    def _generate_usage_documentation(self, output_dir: str, results: Dict[str, Any]):
        """ç”Ÿæˆä½¿ç”¨æ–‡æ¡£"""
        output_path = Path(output_dir)
        
        # ç”ŸæˆREADME
        readme_content = f"""# Qwen3-4B-Thinking é‡åŒ–æ¨¡å‹ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬ç›®å½•åŒ…å«äº†ä»å¾®è°ƒæ£€æŸ¥ç‚¹åˆå¹¶å¹¶é‡åŒ–çš„Qwen3-4B-Thinkingæ¨¡å‹çš„å¤šç§æ ¼å¼ç‰ˆæœ¬ã€‚
æ‰€æœ‰æ¨¡å‹éƒ½ç»è¿‡äº†å…¨é¢çš„åŠŸèƒ½æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸­æ–‡å¤„ç†ã€å¯†ç å­¦çŸ¥è¯†å’Œæ·±åº¦æ€è€ƒèƒ½åŠ›ã€‚

## ç›®å½•ç»“æ„

```
{output_dir}/
â”œâ”€â”€ merged_model/              # åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
â”œâ”€â”€ fp16/                      # FP16ç²¾åº¦æ¨¡å‹ï¼ˆåŸºå‡†ï¼‰
â”œâ”€â”€ int8/                      # INT8é‡åŒ–æ¨¡å‹
â”œâ”€â”€ int4/                      # INT4é‡åŒ–æ¨¡å‹
â”œâ”€â”€ export_report.json         # å¯¼å‡ºè¯¦ç»†æŠ¥å‘Š
â”œâ”€â”€ comprehensive_test_report.json  # å…¨é¢æµ‹è¯•æŠ¥å‘Š
â”œâ”€â”€ TEST_SUMMARY.md           # æµ‹è¯•æ‘˜è¦
â””â”€â”€ README.md                 # æœ¬æ–‡ä»¶
```

## æ¨¡å‹æ ¼å¼è¯´æ˜

### FP16æ¨¡å‹ (åŸºå‡†)
- **è·¯å¾„**: `fp16/`
- **ç‰¹ç‚¹**: åŠç²¾åº¦æµ®ç‚¹ï¼Œä¿æŒå®Œæ•´ç²¾åº¦
- **ç”¨é€”**: é«˜ç²¾åº¦è¦æ±‚çš„åº”ç”¨
- **åŠ è½½**: æ ‡å‡†transformersåŠ è½½æ–¹å¼

### INT8é‡åŒ–æ¨¡å‹
- **è·¯å¾„**: `int8/`
- **ç‰¹ç‚¹**: 8ä½é‡åŒ–ï¼Œçº¦50%å¤§å°å‹ç¼©
- **ç”¨é€”**: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
- **åŠ è½½**: å¯èƒ½éœ€è¦ç‰¹æ®Šé…ç½®ï¼ˆè§load_model.pyï¼‰

### INT4é‡åŒ–æ¨¡å‹
- **è·¯å¾„**: `int4/`
- **ç‰¹ç‚¹**: 4ä½é‡åŒ–ï¼Œçº¦75%å¤§å°å‹ç¼©
- **ç”¨é€”**: èµ„æºå—é™ç¯å¢ƒ
- **æ³¨æ„**: ç²¾åº¦æœ‰ä¸€å®šæŸå¤±

## å¿«é€Ÿå¼€å§‹

### åŠ è½½FP16æ¨¡å‹
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "{output_dir}/fp16",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "{output_dir}/fp16",
    trust_remote_code=True
)
```

### åŠ è½½INT8æ¨¡å‹
```python
# æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®ŠåŠ è½½è„šæœ¬
import os
if os.path.exists("{output_dir}/int8/load_model.py"):
    # ä½¿ç”¨æä¾›çš„åŠ è½½è„šæœ¬
    exec(open("{output_dir}/int8/load_model.py").read())
else:
    # æ ‡å‡†åŠ è½½
    model = AutoModelForCausalLM.from_pretrained(
        "{output_dir}/int8",
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
```

## æµ‹è¯•ç»“æœ

æ‰€æœ‰æ¨¡å‹éƒ½ç»è¿‡äº†ä»¥ä¸‹æµ‹è¯•ï¼š

1. **ä¸­æ–‡å¯†ç å­¦çŸ¥è¯†æµ‹è¯•** - éªŒè¯å¯¹å¯†ç å­¦æ¦‚å¿µçš„ä¸­æ–‡ç†è§£
2. **æ·±åº¦æ€è€ƒæ¨¡å¼æµ‹è¯•** - éªŒè¯thinkingæ ‡ç­¾çš„å¤„ç†èƒ½åŠ›  
3. **æŠ€æœ¯å‡†ç¡®æ€§æµ‹è¯•** - éªŒè¯ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µçš„å‡†ç¡®æ€§

è¯¦ç»†æµ‹è¯•ç»“æœè¯·æŸ¥çœ‹ `TEST_SUMMARY.md` å’Œ `comprehensive_test_report.json`ã€‚

## æ€§èƒ½å»ºè®®

- **é«˜ç²¾åº¦éœ€æ±‚**: ä½¿ç”¨FP16æ¨¡å‹
- **å¹³è¡¡æ€§èƒ½**: ä½¿ç”¨INT8æ¨¡å‹  
- **èµ„æºå—é™**: ä½¿ç”¨INT4æ¨¡å‹ï¼Œä½†éœ€è¯„ä¼°ç²¾åº¦æŸå¤±

## æ³¨æ„äº‹é¡¹

1. é‡åŒ–æ¨¡å‹å¯èƒ½åœ¨æŸäº›å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šç²¾åº¦æœ‰æ‰€ä¸‹é™
2. BitsAndBytesé‡åŒ–éœ€è¦åœ¨åŠ è½½æ—¶æŒ‡å®šé…ç½®
3. å»ºè®®åœ¨éƒ¨ç½²å‰è¿›è¡Œå……åˆ†æµ‹è¯•
4. ä¸åŒé‡åŒ–æ–¹æ³•é€‚ç”¨äºä¸åŒçš„ç¡¬ä»¶ç¯å¢ƒ

## æŠ€æœ¯æ”¯æŒ

å¦‚é‡é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. é‡åŒ–é…ç½®æ–‡ä»¶ (`quantization_config.json`)
2. æµ‹è¯•æŠ¥å‘Šä¸­çš„é”™è¯¯ä¿¡æ¯
3. æ¨¡å‹åŠ è½½è„šæœ¬çš„ç‰¹æ®Šè¦æ±‚

---
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç‰ˆæœ¬: ä¿®å¤ç‰ˆ v2.0 - åŒ…å«çœŸå®é‡åŒ–å’Œå…¨é¢æµ‹è¯•
"""
        
        with open(output_path / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        self.logger.info("ğŸ“– ä½¿ç”¨æ–‡æ¡£å·²ç”Ÿæˆ: README.md")




    def _comprehensive_test_exported_models(self, output_dir: str, results: Dict[str, Any]) -> Dict[str, Any]:
        """å…¨é¢æµ‹è¯•å¯¼å‡ºçš„æ¨¡å‹"""
        self.logger.info("å¼€å§‹å…¨é¢æµ‹è¯•å¯¼å‡ºçš„æ¨¡å‹...")
        
        test_results = {}
        output_path = Path(output_dir)
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_cases = {
            "chinese_crypto": [
                "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿè¯·è¯¦ç»†è§£é‡Šå…¶å·¥ä½œåŸç†ã€‚",
                "RSAç®—æ³•çš„å®‰å…¨æ€§åŸºäºä»€ä¹ˆæ•°å­¦éš¾é¢˜ï¼Ÿ",
                "è¯·æ¯”è¾ƒå¯¹ç§°åŠ å¯†å’Œéå¯¹ç§°åŠ å¯†çš„ä¼˜ç¼ºç‚¹ã€‚"
            ],
            "thinking_mode": [
                "<thinking>æˆ‘éœ€è¦åˆ†æè¿™ä¸ªå¯†ç å­¦é—®é¢˜çš„æ ¸å¿ƒ</thinking>æ•°å­—ç­¾åçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
                "<thinking>è®©æˆ‘æ€è€ƒä¸€ä¸‹å“ˆå¸Œå‡½æ•°çš„ç‰¹æ€§</thinking>SHA-256ç®—æ³•æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
                "<thinking>è¿™ä¸ªé—®é¢˜æ¶‰åŠå¯†é’¥ç®¡ç†</thinking>å¦‚ä½•å®‰å…¨åœ°åˆ†å‘å¯†é’¥ï¼Ÿ"
            ],
            "technical_accuracy": [
                "æ¤­åœ†æ›²çº¿å¯†ç å­¦(ECC)ç›¸æ¯”RSAæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
                "ä»€ä¹ˆæ˜¯é›¶çŸ¥è¯†è¯æ˜ï¼Ÿè¯·ä¸¾ä¾‹è¯´æ˜ã€‚",
                "åŒºå—é“¾ä¸­ä½¿ç”¨äº†å“ªäº›å¯†ç å­¦æŠ€æœ¯ï¼Ÿ"
            ]
        }
        
        for format_name, result in results.items():
            if not result.get("success", False):
                test_results[format_name] = {
                    "success": False,
                    "error": "æ¨¡å‹å¯¼å‡ºå¤±è´¥ï¼Œè·³è¿‡æµ‹è¯•"
                }
                continue
            
            model_path = result["output_path"]
            self.logger.info(f"\næµ‹è¯• {format_name.upper()} æ¨¡å‹...")
            
            try:
                # åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯•
                format_test_result = self._test_single_model(model_path, test_cases, format_name)
                test_results[format_name] = format_test_result
                
                # æ˜¾ç¤ºæµ‹è¯•æ‘˜è¦
                success_rate = format_test_result.get("overall_success_rate", 0)
                self.logger.info(f"âœ… {format_name.upper()} æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1%}")
                
            except Exception as e:
                self.logger.error(f"âŒ {format_name.upper()} æµ‹è¯•å¤±è´¥: {e}")
                test_results[format_name] = {
                    "success": False,
                    "error": str(e)
                }
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_test_report(test_results, output_path)
        
        return test_results
    
    def _test_single_model(self, model_path: str, test_cases: Dict[str, List[str]], format_name: str) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        if not TRANSFORMERS_AVAILABLE:
            return {
                "success": False,
                "error": "transformersåº“ä¸å¯ç”¨"
            }
        
        try:
            # åŠ è½½æ¨¡å‹å’Œtokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # ç‰¹æ®Šå¤„ç†BitsAndBytesæ ¼å¼
            if "BitsAndBytes" in format_name:
                # å¯¹äºBitsAndBytesæ ¼å¼ï¼Œéœ€è¦ä½¿ç”¨åŸå§‹æ¨¡å‹è·¯å¾„å’Œé‡åŒ–é…ç½®
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0,
                    llm_int8_has_fp16_weight=False
                )
                
                model = AutoModelForCausalLM.from_pretrained(
                    "Qwen/Qwen3-4B-Thinking-2507",  # ä½¿ç”¨åŸå§‹æ¨¡å‹è·¯å¾„
                    quantization_config=quantization_config,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            model.eval()
            
            # æ‰§è¡Œå„ç±»æµ‹è¯•
            category_results = {}
            total_tests = 0
            total_successes = 0
            
            for category, prompts in test_cases.items():
                category_result = self._test_category(model, tokenizer, prompts, category)
                category_results[category] = category_result
                
                total_tests += category_result["total_tests"]
                total_successes += category_result["successful_tests"]
            
            # è®¡ç®—æ•´ä½“æˆåŠŸç‡
            overall_success_rate = total_successes / total_tests if total_tests > 0 else 0
            
            # æ¸…ç†å†…å­˜
            del model
            del tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return {
                "success": True,
                "category_results": category_results,
                "overall_success_rate": overall_success_rate,
                "total_tests": total_tests,
                "successful_tests": total_successes
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def _test_category(self, model, tokenizer, prompts: List[str], category: str) -> Dict[str, Any]:
        """æµ‹è¯•ç‰¹å®šç±»åˆ«çš„æç¤º"""
        results = []
        
        with torch.no_grad():
            for prompt in prompts:
                try:
                    # ç¼–ç è¾“å…¥
                    inputs = tokenizer(
                        prompt,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=512
                    )
                    
                    # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                    inputs = {k: v.to(self._get_model_device(model)) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆå“åº”
                    outputs = model.generate(
                        **inputs,
                        max_length=inputs["input_ids"].shape[1] + 150,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                    
                    # è§£ç å“åº”
                    response = tokenizer.decode(
                        outputs[0][inputs["input_ids"].shape[1]:],
                        skip_special_tokens=True
                    )
                    
                    # è¯„ä¼°å“åº”è´¨é‡
                    quality_score = self._evaluate_response_quality(prompt, response, category)
                    
                    results.append({
                        "prompt": prompt,
                        "response": response[:300] + "..." if len(response) > 300 else response,
                        "success": True,
                        "quality_score": quality_score,
                        "response_length": len(response)
                    })
                    
                except Exception as e:
                    results.append({
                        "prompt": prompt,
                        "response": "",
                        "success": False,
                        "error": str(e),
                        "quality_score": 0.0
                    })
        
        successful_tests = sum(1 for r in results if r["success"])
        avg_quality = sum(r.get("quality_score", 0) for r in results) / len(results) if results else 0
        
        return {
            "category": category,
            "results": results,
            "total_tests": len(prompts),
            "successful_tests": successful_tests,
            "success_rate": successful_tests / len(prompts) if prompts else 0,
            "average_quality_score": avg_quality
        }
    
    def _evaluate_response_quality(self, prompt: str, response: str, category: str) -> float:
        """è¯„ä¼°å“åº”è´¨é‡"""
        if not response or len(response.strip()) < 10:
            return 0.0
        
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°ï¼šæœ‰å“åº”
        score += 0.3
        
        # é•¿åº¦åˆç†æ€§
        if 50 <= len(response) <= 500:
            score += 0.2
        elif len(response) > 20:
            score += 0.1
        
        # ä¸­æ–‡å†…å®¹æ£€æŸ¥
        chinese_chars = sum(1 for char in response if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(response) * 0.3:  # è‡³å°‘30%ä¸­æ–‡å­—ç¬¦
            score += 0.2
        
        # ç±»åˆ«ç‰¹å®šæ£€æŸ¥
        if category == "chinese_crypto":
            crypto_terms = ["åŠ å¯†", "ç®—æ³•", "å¯†é’¥", "å®‰å…¨", "å“ˆå¸Œ", "ç­¾å", "è¯ä¹¦"]
            found_terms = sum(1 for term in crypto_terms if term in response)
            score += min(found_terms * 0.05, 0.2)
        
        elif category == "thinking_mode":
            if "<thinking>" in prompt and ("åˆ†æ" in response or "è€ƒè™‘" in response or "æ€è€ƒ" in response):
                score += 0.1
        
        elif category == "technical_accuracy":
            technical_terms = ["å¯†ç å­¦", "ç®—æ³•", "å®‰å…¨æ€§", "æ•°å­¦", "è®¡ç®—", "å¤æ‚åº¦"]
            found_terms = sum(1 for term in technical_terms if term in response)
            score += min(found_terms * 0.03, 0.1)
        
        return min(score, 1.0)
    
    def _generate_test_report(self, test_results: Dict[str, Any], output_path: Path):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            "test_time": datetime.now().isoformat(),
            "test_summary": {
                "total_models_tested": len(test_results),
                "successful_models": sum(1 for r in test_results.values() if r.get("success", False)),
                "overall_statistics": {}
            },
            "detailed_results": test_results,
            "performance_comparison": {}
        }
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        successful_results = {k: v for k, v in test_results.items() if v.get("success", False)}
        
        if successful_results:
            total_tests = sum(r.get("total_tests", 0) for r in successful_results.values())
            total_successes = sum(r.get("successful_tests", 0) for r in successful_results.values())
            
            report["test_summary"]["overall_statistics"] = {
                "total_test_cases": total_tests,
                "successful_test_cases": total_successes,
                "overall_success_rate": total_successes / total_tests if total_tests > 0 else 0
            }
            
            # æ€§èƒ½å¯¹æ¯”
            for model_name, result in successful_results.items():
                success_rate = result.get("overall_success_rate", 0)
                report["performance_comparison"][model_name] = {
                    "success_rate": f"{success_rate:.1%}",
                    "total_tests": result.get("total_tests", 0),
                    "model_status": "âœ… å¯ç”¨" if success_rate > 0.5 else "âš ï¸ éœ€è¦æ”¹è¿›"
                }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_path / "comprehensive_test_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è¯»çš„æ‘˜è¦
        self._generate_readable_test_summary(report, output_path)
    
    def _generate_readable_test_summary(self, report: Dict[str, Any], output_path: Path):
        """ç”Ÿæˆå¯è¯»çš„æµ‹è¯•æ‘˜è¦"""
        summary_content = f"""# æ¨¡å‹æµ‹è¯•æ‘˜è¦æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ

- **æµ‹è¯•æ—¶é—´**: {report['test_time']}
- **æµ‹è¯•æ¨¡å‹æ•°**: {report['test_summary']['total_models_tested']}
- **æˆåŠŸæ¨¡å‹æ•°**: {report['test_summary']['successful_models']}

## æ•´ä½“ç»Ÿè®¡

"""
        
        if "overall_statistics" in report["test_summary"]:
            stats = report["test_summary"]["overall_statistics"]
            summary_content += f"""- **æ€»æµ‹è¯•ç”¨ä¾‹**: {stats['total_test_cases']}
- **æˆåŠŸç”¨ä¾‹**: {stats['successful_test_cases']}
- **æ•´ä½“æˆåŠŸç‡**: {stats['overall_success_rate']:.1%}

"""
        
        summary_content += "## å„æ¨¡å‹æ€§èƒ½\n\n"
        
        for model_name, perf in report.get("performance_comparison", {}).items():
            summary_content += f"### {model_name.upper()}\n"
            summary_content += f"- æˆåŠŸç‡: {perf['success_rate']}\n"
            summary_content += f"- æµ‹è¯•ç”¨ä¾‹: {perf['total_tests']}\n"
            summary_content += f"- çŠ¶æ€: {perf['model_status']}\n\n"
        
        summary_content += f"""## æµ‹è¯•ç±»åˆ«

æœ¬æ¬¡æµ‹è¯•åŒ…å«ä»¥ä¸‹ç±»åˆ«ï¼š

1. **ä¸­æ–‡å¯†ç å­¦çŸ¥è¯†** - æµ‹è¯•æ¨¡å‹å¯¹å¯†ç å­¦æ¦‚å¿µçš„ä¸­æ–‡ç†è§£
2. **æ·±åº¦æ€è€ƒæ¨¡å¼** - æµ‹è¯•thinkingæ ‡ç­¾çš„å¤„ç†èƒ½åŠ›
3. **æŠ€æœ¯å‡†ç¡®æ€§** - æµ‹è¯•ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µçš„å‡†ç¡®æ€§

## å»ºè®®

- âœ… æˆåŠŸç‡ > 70%: æ¨¡å‹å¯ç”¨äºç”Ÿäº§ç¯å¢ƒ
- âš ï¸ æˆåŠŸç‡ 50-70%: æ¨¡å‹éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–
- âŒ æˆåŠŸç‡ < 50%: æ¨¡å‹éœ€è¦é‡æ–°è®­ç»ƒ

è¯¦ç»†æµ‹è¯•ç»“æœè¯·æŸ¥çœ‹ `comprehensive_test_report.json`

---
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        with open(output_path / "TEST_SUMMARY.md", "w", encoding="utf-8") as f:
            f.write(summary_content)
        
        self.logger.info("ğŸ“Š æµ‹è¯•æ‘˜è¦å·²ç”Ÿæˆ: TEST_SUMMARY.md")
    
    def _display_comprehensive_summary(self, quantization_results: Dict[str, Any], output_dir: str, test_results: Dict[str, Any]):
        """æ˜¾ç¤ºå…¨é¢çš„ç»“æœæ‘˜è¦"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š é‡åŒ–å’Œæµ‹è¯•ç»“æœæ‘˜è¦")
        self.logger.info("=" * 60)
        
        # é‡åŒ–ç»“æœæ‘˜è¦
        self.logger.info("\nğŸ”§ é‡åŒ–ç»“æœ:")
        successful_exports = 0
        total_size = 0
        
        for format_name, result in quantization_results.items():
            if result.get("success", False):
                successful_exports += 1
                size = result.get("model_size_mb", 0)
                total_size += size
                compression_note = result.get("note", "")
                self.logger.info(f"  âœ… {format_name.upper()}: {size:.1f}MB - {compression_note}")
            else:
                error = result.get("error", "æœªçŸ¥é”™è¯¯")
                self.logger.info(f"  âŒ {format_name.upper()}: å¤±è´¥ - {error}")
        
        self.logger.info(f"\nğŸ“ˆ å¯¼å‡ºç»Ÿè®¡:")
        self.logger.info(f"  - æˆåŠŸå¯¼å‡º: {successful_exports}/{len(quantization_results)} ç§æ ¼å¼")
        self.logger.info(f"  - æ€»æ–‡ä»¶å¤§å°: {total_size:.1f}MB")
        
        # æµ‹è¯•ç»“æœæ‘˜è¦
        self.logger.info("\nğŸ§ª æµ‹è¯•ç»“æœ:")
        successful_tests = 0
        
        for format_name, result in test_results.items():
            if result.get("success", False):
                successful_tests += 1
                success_rate = result.get("overall_success_rate", 0)
                total_tests = result.get("total_tests", 0)
                status = "âœ… ä¼˜ç§€" if success_rate > 0.8 else "âš ï¸ è‰¯å¥½" if success_rate > 0.5 else "âŒ éœ€æ”¹è¿›"
                self.logger.info(f"  {format_name.upper()}: {success_rate:.1%} ({total_tests}ä¸ªæµ‹è¯•) - {status}")
            else:
                error = result.get("error", "æµ‹è¯•å¤±è´¥")
                self.logger.info(f"  {format_name.upper()}: æµ‹è¯•å¤±è´¥ - {error}")
        
        self.logger.info(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        self.logger.info(f"  - æˆåŠŸæµ‹è¯•: {successful_tests}/{len(test_results)} ä¸ªæ¨¡å‹")
        
        # æ–‡ä»¶ä½ç½®ä¿¡æ¯
        self.logger.info(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        self.logger.info(f"  - æ¨¡å‹æ–‡ä»¶: {output_dir}/")
        self.logger.info(f"  - å¯¼å‡ºæŠ¥å‘Š: {output_dir}/export_report.json")
        self.logger.info(f"  - æµ‹è¯•æŠ¥å‘Š: {output_dir}/comprehensive_test_report.json")
        self.logger.info(f"  - æµ‹è¯•æ‘˜è¦: {output_dir}/TEST_SUMMARY.md")
        
        # æ¨èä½¿ç”¨
        self.logger.info(f"\nğŸ’¡ æ¨èä½¿ç”¨:")
        best_model = None
        best_score = 0
        
        for format_name, result in test_results.items():
            if result.get("success", False):
                success_rate = result.get("overall_success_rate", 0)
                if success_rate > best_score:
                    best_score = success_rate
                    best_model = format_name
        
        if best_model:
            self.logger.info(f"  ğŸ† æœ€ä½³æ¨¡å‹: {best_model.upper()} (æˆåŠŸç‡: {best_score:.1%})")
        
        # å¤§å°å¯¹æ¯”
        fp16_size = quantization_results.get("fp16", {}).get("model_size_mb", 0)
        if fp16_size > 0:
            self.logger.info(f"\nğŸ“ å¤§å°å¯¹æ¯” (ç›¸å¯¹äºFP16):")
            for format_name, result in quantization_results.items():
                if result.get("success", False) and format_name != "fp16":
                    size = result.get("model_size_mb", 0)
                    if size > 0:
                        compression_ratio = fp16_size / size
                        reduction_percent = (1 - size/fp16_size) * 100
                        self.logger.info(f"  {format_name.upper()}: {compression_ratio:.1f}xå‹ç¼©, å‡å°‘{reduction_percent:.1f}%")


class DemoRunner:
    """æ¼”ç¤ºç¨‹åºè¿è¡Œå™¨"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.merger = CheckpointMerger()
        self.quantizer = MultiFormatQuantizer()
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("DemoRunner")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def run_demo(
        self,
        checkpoint_path: str = "qwen3_4b_thinking_output/final_model",
        base_model_path: str = "Qwen/Qwen3-4B-Thinking-2507",
        output_dir: str = "quantized_models_output"
    ):
        """
        è¿è¡Œå®Œæ•´æ¼”ç¤º
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.logger.info("=" * 60)
        self.logger.info("æ¨¡å‹æ£€æŸ¥ç‚¹åˆå¹¶å’Œå¤šæ ¼å¼é‡åŒ–å¯¼å‡ºæ¼”ç¤º")
        self.logger.info("=" * 60)
        
        try:
            # æ£€æŸ¥ç¯å¢ƒ
            self._check_environment()
            
            # æ­¥éª¤1: åˆå¹¶LoRAæ£€æŸ¥ç‚¹
            self.logger.info("\næ­¥éª¤1: åˆå¹¶LoRAæ£€æŸ¥ç‚¹åˆ°åŸºç¡€æ¨¡å‹")
            merged_model_path = Path(output_dir) / "merged_model"
            
            if not TRANSFORMERS_AVAILABLE:
                self.logger.error("transformersåº“ä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")
                return
            
            merged_model, tokenizer = self.merger.merge_lora_checkpoint(
                base_model_path=base_model_path,
                checkpoint_path=checkpoint_path,
                output_path=str(merged_model_path)
            )
            
            # æ­¥éª¤2: å¤šæ ¼å¼é‡åŒ–å¯¼å‡º
            self.logger.info("\næ­¥éª¤2: å¯¼å‡ºå¤šç§æ ¼å¼çš„é‡åŒ–æ¨¡å‹")
            quantization_results = self.quantizer.quantize_multiple_formats(
                model=merged_model,
                tokenizer=tokenizer,
                output_base_dir=output_dir,
                formats=["fp16", "int8", "int4"]  # ç§»é™¤gptqé¿å…ä¾èµ–é—®é¢˜
            )
            
            # æ­¥éª¤3: ç”Ÿæˆä½¿ç”¨æ–‡æ¡£
            self.logger.info("\næ­¥éª¤3: ç”Ÿæˆä½¿ç”¨æ–‡æ¡£å’Œéƒ¨ç½²æŒ‡å—")
            self._generate_usage_documentation(output_dir, quantization_results)
            
            # æ­¥éª¤4: æµ‹è¯•å¯¼å‡ºçš„æ¨¡å‹
            self.logger.info("\næ­¥éª¤4: å…¨é¢æµ‹è¯•å¯¼å‡ºçš„æ¨¡å‹")
            test_results = self.quantizer._comprehensive_test_exported_models(output_dir, quantization_results)
            
            # æ­¥éª¤5: æ˜¾ç¤ºç»“æœæ‘˜è¦
            self.logger.info("\næ­¥éª¤5: ç»“æœæ‘˜è¦")
            self.quantizer._display_comprehensive_summary(quantization_results, output_dir, test_results)
            
            self.logger.info("\næ¼”ç¤ºå®Œæˆï¼")
            
        except Exception as e:
            self.logger.error(f"æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
            raise
    
    def _check_environment(self):
        """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
        self.logger.info("æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
        
        # æ£€æŸ¥CUDA
        if torch.cuda.is_available():
            self.logger.info(f"CUDAå¯ç”¨: {torch.cuda.get_device_name()}")
            self.logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            self.logger.warning("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        
        # æ£€æŸ¥ä¾èµ–
        if not TRANSFORMERS_AVAILABLE:
            self.logger.error("transformersåº“ä¸å¯ç”¨")
        else:
            self.logger.info("transformersåº“å¯ç”¨")
        
        if not MODEL_EXPORTER_AVAILABLE:
            self.logger.warning("è‡ªç ”æ¨¡å‹å¯¼å‡ºå™¨ä¸å¯ç”¨ï¼ŒæŸäº›åŠŸèƒ½å°†è¢«ç¦ç”¨")
        else:
            self.logger.info("è‡ªç ”æ¨¡å‹å¯¼å‡ºå™¨å¯ç”¨")
    
    def _generate_usage_documentation(self, output_dir: str, results: Dict[str, Any]):
        """ç”Ÿæˆä½¿ç”¨æ–‡æ¡£"""
        output_path = Path(output_dir)
        
        # ç”ŸæˆREADME
        readme_content = f"""# Qwen3-4B-Thinking é‡åŒ–æ¨¡å‹ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬ç›®å½•åŒ…å«äº†ä»å¾®è°ƒæ£€æŸ¥ç‚¹åˆå¹¶å¹¶é‡åŒ–çš„Qwen3-4B-Thinkingæ¨¡å‹çš„å¤šç§æ ¼å¼ç‰ˆæœ¬ã€‚
æ‰€æœ‰æ¨¡å‹éƒ½ç»è¿‡äº†å…¨é¢çš„åŠŸèƒ½æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸­æ–‡å¤„ç†ã€å¯†ç å­¦çŸ¥è¯†å’Œæ·±åº¦æ€è€ƒèƒ½åŠ›ã€‚

## ç›®å½•ç»“æ„

```
{output_dir}/
â”œâ”€â”€ merged_model/              # åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
â”œâ”€â”€ fp16/                      # FP16ç²¾åº¦æ¨¡å‹ï¼ˆåŸºå‡†ï¼‰
â”œâ”€â”€ int8/                      # INT8é‡åŒ–æ¨¡å‹
â”œâ”€â”€ int4/                      # INT4é‡åŒ–æ¨¡å‹
â”œâ”€â”€ export_report.json         # å¯¼å‡ºè¯¦ç»†æŠ¥å‘Š
â”œâ”€â”€ comprehensive_test_report.json  # å…¨é¢æµ‹è¯•æŠ¥å‘Š
â”œâ”€â”€ TEST_SUMMARY.md           # æµ‹è¯•æ‘˜è¦
â””â”€â”€ README.md                 # æœ¬æ–‡ä»¶
```

## æ¨¡å‹æ ¼å¼è¯´æ˜

### FP16æ¨¡å‹ (åŸºå‡†)
- **è·¯å¾„**: `fp16/`
- **ç‰¹ç‚¹**: åŠç²¾åº¦æµ®ç‚¹ï¼Œä¿æŒå®Œæ•´ç²¾åº¦
- **ç”¨é€”**: é«˜ç²¾åº¦è¦æ±‚çš„åº”ç”¨
- **åŠ è½½**: æ ‡å‡†transformersåŠ è½½æ–¹å¼

### INT8é‡åŒ–æ¨¡å‹
- **è·¯å¾„**: `int8/`
- **ç‰¹ç‚¹**: 8ä½é‡åŒ–ï¼Œçº¦50%å¤§å°å‹ç¼©
- **ç”¨é€”**: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
- **åŠ è½½**: å¯èƒ½éœ€è¦ç‰¹æ®Šé…ç½®ï¼ˆè§load_model.pyï¼‰

### INT4é‡åŒ–æ¨¡å‹
- **è·¯å¾„**: `int4/`
- **ç‰¹ç‚¹**: 4ä½é‡åŒ–ï¼Œçº¦75%å¤§å°å‹ç¼©
- **ç”¨é€”**: èµ„æºå—é™ç¯å¢ƒ
- **æ³¨æ„**: ç²¾åº¦æœ‰ä¸€å®šæŸå¤±

## å¿«é€Ÿå¼€å§‹

### åŠ è½½FP16æ¨¡å‹
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "{output_dir}/fp16",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "{output_dir}/fp16",
    trust_remote_code=True
)
```

### åŠ è½½INT8æ¨¡å‹
```python
# æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®ŠåŠ è½½è„šæœ¬
import os
if os.path.exists("{output_dir}/int8/load_model.py"):
    # ä½¿ç”¨æä¾›çš„åŠ è½½è„šæœ¬
    exec(open("{output_dir}/int8/load_model.py").read())
else:
    # æ ‡å‡†åŠ è½½
    model = AutoModelForCausalLM.from_pretrained(
        "{output_dir}/int8",
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
```

## æµ‹è¯•ç»“æœ

æ‰€æœ‰æ¨¡å‹éƒ½ç»è¿‡äº†ä»¥ä¸‹æµ‹è¯•ï¼š

1. **ä¸­æ–‡å¯†ç å­¦çŸ¥è¯†æµ‹è¯•** - éªŒè¯å¯¹å¯†ç å­¦æ¦‚å¿µçš„ä¸­æ–‡ç†è§£
2. **æ·±åº¦æ€è€ƒæ¨¡å¼æµ‹è¯•** - éªŒè¯thinkingæ ‡ç­¾çš„å¤„ç†èƒ½åŠ›  
3. **æŠ€æœ¯å‡†ç¡®æ€§æµ‹è¯•** - éªŒè¯ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µçš„å‡†ç¡®æ€§

è¯¦ç»†æµ‹è¯•ç»“æœè¯·æŸ¥çœ‹ `TEST_SUMMARY.md` å’Œ `comprehensive_test_report.json`ã€‚

## æ€§èƒ½å»ºè®®

- **é«˜ç²¾åº¦éœ€æ±‚**: ä½¿ç”¨FP16æ¨¡å‹
- **å¹³è¡¡æ€§èƒ½**: ä½¿ç”¨INT8æ¨¡å‹  
- **èµ„æºå—é™**: ä½¿ç”¨INT4æ¨¡å‹ï¼Œä½†éœ€è¯„ä¼°ç²¾åº¦æŸå¤±

## æ³¨æ„äº‹é¡¹

1. é‡åŒ–æ¨¡å‹å¯èƒ½åœ¨æŸäº›å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šç²¾åº¦æœ‰æ‰€ä¸‹é™
2. BitsAndBytesé‡åŒ–éœ€è¦åœ¨åŠ è½½æ—¶æŒ‡å®šé…ç½®
3. å»ºè®®åœ¨éƒ¨ç½²å‰è¿›è¡Œå……åˆ†æµ‹è¯•
4. ä¸åŒé‡åŒ–æ–¹æ³•é€‚ç”¨äºä¸åŒçš„ç¡¬ä»¶ç¯å¢ƒ

## æŠ€æœ¯æ”¯æŒ

å¦‚é‡é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. é‡åŒ–é…ç½®æ–‡ä»¶ (`quantization_config.json`)
2. æµ‹è¯•æŠ¥å‘Šä¸­çš„é”™è¯¯ä¿¡æ¯
3. æ¨¡å‹åŠ è½½è„šæœ¬çš„ç‰¹æ®Šè¦æ±‚

---
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç‰ˆæœ¬: ä¿®å¤ç‰ˆ v2.0 - åŒ…å«çœŸå®é‡åŒ–å’Œå…¨é¢æµ‹è¯•
"""
        
        with open(output_path / "README.md", "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        self.logger.info("ğŸ“– ä½¿ç”¨æ–‡æ¡£å·²ç”Ÿæˆ: README.md")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¨¡å‹æ£€æŸ¥ç‚¹åˆå¹¶å’Œé‡åŒ–å¯¼å‡ºæ¼”ç¤ºï¼ˆä¿®å¤ç‰ˆï¼‰")
    parser.add_argument("--checkpoint", default="qwen3_4b_thinking_output/final_model", 
                       help="LoRAæ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--base-model", default="Qwen/Qwen3-4B-Thinking-2507", 
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output", default="quantized_models_output_fixed", 
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--formats", nargs="+", default=["fp16", "int8", "int4"],
                       help="è¦å¯¼å‡ºçš„æ ¼å¼")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
    runner = DemoRunner()
    
    try:
        runner.run_demo(
            checkpoint_path=args.checkpoint,
            base_model_path=args.base_model,
            output_dir=args.output
        )
        
        print("\n" + "="*60)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output}")
        print("ğŸ“Š æŸ¥çœ‹ TEST_SUMMARY.md äº†è§£æµ‹è¯•ç»“æœ")
        print("ğŸ“– æŸ¥çœ‹ README.md äº†è§£ä½¿ç”¨æ–¹æ³•")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())