"""
æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå›å½’æµ‹è¯•æ¨¡å—

æœ¬æ¨¡å—å®ç°äº†ç³»ç»Ÿæ€§èƒ½çš„åŸºå‡†æµ‹è¯•å’Œå›å½’æ£€æµ‹ï¼ŒåŒ…æ‹¬ï¼š
- è®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•
- å†…å­˜ä½¿ç”¨æ•ˆç‡æµ‹è¯•
- GPUåˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•
- æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•
- å¤šGPUé€šä¿¡æ•ˆç‡æµ‹è¯•
- æ€§èƒ½å›å½’æ£€æµ‹å’ŒæŠ¥å‘Š
"""

import os
import sys
import json
import time
import logging
import statistics
import threading
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import unittest

import torch
import torch.distributed as dist
import psutil
import numpy as np

# å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append(str(Path(__file__).parent.parent))

from src.gpu_utils import GPUDetector, GPUInfo
from src.memory_manager import MemoryManager, MemorySnapshot
from src.training_monitor import TrainingMonitor
from src.distributed_training_engine import MultiGPUProcessManager
from src.chinese_nlp_processor import ChineseNLPProcessor
from src.crypto_term_processor import CryptoTermProcessor
from src.thinking_generator import ThinkingDataGenerator
from src.dataset_splitter import DatasetSplitter
from src.data_models import TrainingExample, ThinkingExample


@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†"""
    benchmark_name: str
    metric_name: str
    value: float
    unit: str
    timestamp: datetime
    environment_info: Dict[str, Any]
    test_config: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "benchmark_name": self.benchmark_name,
            "metric_name": self.metric_name,
            "value": self.value,
            "unit": self.unit,
            "timestamp": self.timestamp.isoformat(),
            "environment_info": self.environment_info,
            "test_config": self.test_config
        }


@dataclass
class PerformanceRegression:
    """æ€§èƒ½å›å½’"""
    metric_name: str
    current_value: float
    baseline_value: float
    change_percent: float
    threshold_percent: float
    is_regression: bool
    severity: str  # "minor", "major", "critical"
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "metric_name": self.metric_name,
            "current_value": self.current_value,
            "baseline_value": self.baseline_value,
            "change_percent": self.change_percent,
            "threshold_percent": self.threshold_percent,
            "is_regression": self.is_regression,
            "severity": self.severity
        }


class TrainingPerformanceBenchmark:
    """è®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, output_dir: str = "benchmark_output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.gpu_detector = GPUDetector()
        self.chinese_processor = ChineseNLPProcessor()
        self.crypto_processor = CryptoTermProcessor()
        self.thinking_generator = ThinkingDataGenerator()
        
        # åŸºå‡†é…ç½® - æœ€å°æ ·æœ¬æ•°é‡ä»¥åŠ å¿«æµ‹è¯•é€Ÿåº¦
        self.benchmark_configs = {
            "small_batch": {"batch_size": 1, "sequence_length": 512, "samples": 5},
            "medium_batch": {"batch_size": 2, "sequence_length": 1024, "samples": 5},
            "large_batch": {"batch_size": 4, "sequence_length": 2048, "samples": 5},
            "thinking_data": {"batch_size": 1, "sequence_length": 1024, "samples": 5, "enable_thinking": True},
            "chinese_crypto": {"batch_size": 2, "sequence_length": 1024, "samples": 5, "enable_chinese": True}
        }
    
    def run_data_processing_benchmark(self) -> Dict[str, PerformanceBenchmark]:
        """è¿è¡Œæ•°æ®å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        self.logger.info("å¼€å§‹æ•°æ®å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•")
        benchmarks = {}
        
        for config_name, config in self.benchmark_configs.items():
            self.logger.info(f"æµ‹è¯•é…ç½®: {config_name}")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = self._generate_benchmark_data(config)
            
            # æµ‹è¯•æ•°æ®é¢„å¤„ç†æ€§èƒ½
            start_time = time.time()
            processed_data = self._process_benchmark_data(test_data, config)
            processing_time = time.time() - start_time
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            samples_per_second = len(test_data) / processing_time if processing_time > 0 else 0
            tokens_per_second = (len(test_data) * config["sequence_length"]) / processing_time if processing_time > 0 else 0
            
            # åˆ›å»ºåŸºå‡†
            benchmarks[f"{config_name}_samples_per_sec"] = PerformanceBenchmark(
                benchmark_name="data_processing",
                metric_name=f"{config_name}_samples_per_second",
                value=samples_per_second,
                unit="samples/sec",
                timestamp=datetime.now(),
                environment_info=self._get_environment_info(),
                test_config=config
            )
            
            benchmarks[f"{config_name}_tokens_per_sec"] = PerformanceBenchmark(
                benchmark_name="data_processing",
                metric_name=f"{config_name}_tokens_per_second",
                value=tokens_per_second,
                unit="tokens/sec",
                timestamp=datetime.now(),
                environment_info=self._get_environment_info(),
                test_config=config
            )
            
            self.logger.info(f"{config_name}: {samples_per_second:.2f} samples/sec, {tokens_per_second:.2f} tokens/sec")
        
        return benchmarks
    
    def run_memory_efficiency_benchmark(self) -> Dict[str, PerformanceBenchmark]:
        """è¿è¡Œå†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•"""
        self.logger.info("å¼€å§‹å†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•")
        benchmarks = {}
        
        if not torch.cuda.is_available():
            self.logger.warning("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå†…å­˜æµ‹è¯•")
            return benchmarks
        
        # åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨
        memory_manager = MemoryManager({
            "monitoring_interval": 1,
            "enable_auto_adjustment": True,
            "initial_batch_size": 4
        })
        
        memory_manager.start()
        
        try:
            for config_name, config in self.benchmark_configs.items():
                self.logger.info(f"å†…å­˜æµ‹è¯•é…ç½®: {config_name}")
                
                # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
                memory_snapshots = []
                
                # åˆ›å»ºæµ‹è¯•å¼ é‡æ¨¡æ‹Ÿè®­ç»ƒ
                batch_size = config["batch_size"]
                seq_length = config["sequence_length"]
                
                test_tensors = []
                for i in range(10):  # æ¨¡æ‹Ÿ10ä¸ªè®­ç»ƒæ­¥éª¤
                    # åˆ›å»ºæ¨¡æ‹Ÿçš„æ¿€æ´»å¼ é‡
                    tensor = torch.randn(batch_size, seq_length, 3584, device='cuda')  # Qwen3-4B hidden size
                    test_tensors.append(tensor)
                    
                    # æ”¶é›†å†…å­˜å¿«ç…§
                    snapshot = memory_manager.get_current_memory_status()
                    if snapshot:
                        memory_snapshots.append(snapshot)
                    
                    time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…
                
                # æ¸…ç†å¼ é‡
                del test_tensors
                torch.cuda.empty_cache()
                
                # åˆ†æå†…å­˜ä½¿ç”¨
                if memory_snapshots:
                    peak_memory = max(s.allocated_memory for s in memory_snapshots)
                    avg_memory = sum(s.allocated_memory for s in memory_snapshots) / len(memory_snapshots)
                    memory_efficiency = avg_memory / peak_memory if peak_memory > 0 else 0
                    
                    benchmarks[f"{config_name}_peak_memory"] = PerformanceBenchmark(
                        benchmark_name="memory_efficiency",
                        metric_name=f"{config_name}_peak_memory_mb",
                        value=peak_memory,
                        unit="MB",
                        timestamp=datetime.now(),
                        environment_info=self._get_environment_info(),
                        test_config=config
                    )
                    
                    benchmarks[f"{config_name}_memory_efficiency"] = PerformanceBenchmark(
                        benchmark_name="memory_efficiency",
                        metric_name=f"{config_name}_memory_efficiency",
                        value=memory_efficiency,
                        unit="ratio",
                        timestamp=datetime.now(),
                        environment_info=self._get_environment_info(),
                        test_config=config
                    )
                    
                    self.logger.info(f"{config_name}: å³°å€¼å†…å­˜ {peak_memory:.2f}MB, æ•ˆç‡ {memory_efficiency:.3f}")
        
        finally:
            memory_manager.stop()
        
        return benchmarks
    
    def run_gpu_utilization_benchmark(self) -> Dict[str, PerformanceBenchmark]:
        """è¿è¡ŒGPUåˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•"""
        self.logger.info("å¼€å§‹GPUåˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•")
        benchmarks = {}
        
        if not torch.cuda.is_available():
            self.logger.warning("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUåˆ©ç”¨ç‡æµ‹è¯•")
            return benchmarks
        
        gpu_infos = self.gpu_detector.get_all_gpu_info()
        
        for gpu_info in gpu_infos:
            gpu_id = gpu_info.gpu_id
            self.logger.info(f"æµ‹è¯•GPU {gpu_id}: {gpu_info.name}")
            
            # è®¾ç½®è®¾å¤‡
            torch.cuda.set_device(gpu_id)
            
            # è¿è¡Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡
            utilization_samples = []
            start_time = time.time()
            
            for i in range(20):  # 20æ¬¡é‡‡æ ·
                # åˆ›å»ºè®¡ç®—ä»»åŠ¡
                a = torch.randn(1024, 1024, device=f'cuda:{gpu_id}')
                b = torch.randn(1024, 1024, device=f'cuda:{gpu_id}')
                
                # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
                c = torch.matmul(a, b)
                torch.cuda.synchronize()
                
                # è·å–GPUåˆ©ç”¨ç‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦nvidia-ml-pyï¼‰
                # è¿™é‡Œä½¿ç”¨å†…å­˜ä½¿ç”¨ç‡ä½œä¸ºä»£ç†æŒ‡æ ‡
                memory_info = torch.cuda.memory_stats(gpu_id)
                allocated = memory_info.get('allocated_bytes.all.current', 0)
                reserved = memory_info.get('reserved_bytes.all.current', 0)
                utilization = allocated / gpu_info.total_memory if gpu_info.total_memory > 0 else 0
                
                utilization_samples.append(utilization)
                time.sleep(0.1)
            
            total_time = time.time() - start_time
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            avg_utilization = statistics.mean(utilization_samples)
            max_utilization = max(utilization_samples)
            utilization_variance = statistics.variance(utilization_samples) if len(utilization_samples) > 1 else 0
            
            benchmarks[f"gpu_{gpu_id}_avg_utilization"] = PerformanceBenchmark(
                benchmark_name="gpu_utilization",
                metric_name=f"gpu_{gpu_id}_average_utilization",
                value=avg_utilization,
                unit="ratio",
                timestamp=datetime.now(),
                environment_info=self._get_environment_info(),
                test_config={"gpu_id": gpu_id, "test_duration": total_time}
            )
            
            benchmarks[f"gpu_{gpu_id}_max_utilization"] = PerformanceBenchmark(
                benchmark_name="gpu_utilization",
                metric_name=f"gpu_{gpu_id}_max_utilization",
                value=max_utilization,
                unit="ratio",
                timestamp=datetime.now(),
                environment_info=self._get_environment_info(),
                test_config={"gpu_id": gpu_id, "test_duration": total_time}
            )
            
            self.logger.info(f"GPU {gpu_id}: å¹³å‡åˆ©ç”¨ç‡ {avg_utilization:.3f}, æœ€å¤§åˆ©ç”¨ç‡ {max_utilization:.3f}")
        
        return benchmarks
    
    def run_communication_benchmark(self) -> Dict[str, PerformanceBenchmark]:
        """è¿è¡Œå¤šGPUé€šä¿¡æ•ˆç‡åŸºå‡†æµ‹è¯•"""
        self.logger.info("å¼€å§‹å¤šGPUé€šä¿¡æ•ˆç‡åŸºå‡†æµ‹è¯•")
        benchmarks = {}
        
        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        if gpu_count < 2:
            self.logger.warning("éœ€è¦è‡³å°‘2ä¸ªGPUè¿›è¡Œé€šä¿¡æµ‹è¯•")
            return benchmarks
        
        # æµ‹è¯•ä¸åŒå¤§å°çš„å¼ é‡é€šä¿¡
        tensor_sizes = [
            (1024, 1024),      # 4MB (float32)
            (2048, 2048),      # 16MB
            (4096, 4096),      # 64MB
        ]
        
        for size in tensor_sizes:
            size_name = f"{size[0]}x{size[1]}"
            self.logger.info(f"æµ‹è¯•å¼ é‡å¤§å°: {size_name}")
            
            # åˆ›å»ºæµ‹è¯•å¼ é‡
            tensor = torch.randn(size, device='cuda:0')
            tensor_size_mb = tensor.numel() * 4 / (1024 * 1024)  # float32 = 4 bytes
            
            # æµ‹è¯•ç‚¹å¯¹ç‚¹é€šä¿¡
            if gpu_count >= 2:
                start_time = time.time()
                for i in range(10):  # 10æ¬¡æµ‹è¯•
                    # ä»GPU 0å‘é€åˆ°GPU 1
                    tensor_copy = tensor.to('cuda:1')
                    torch.cuda.synchronize()
                    
                    # ä»GPU 1å‘é€å›GPU 0
                    tensor_back = tensor_copy.to('cuda:0')
                    torch.cuda.synchronize()
                
                p2p_time = (time.time() - start_time) / 20  # 20æ¬¡ä¼ è¾“ï¼ˆæ¥å›10æ¬¡ï¼‰
                p2p_bandwidth = tensor_size_mb / p2p_time if p2p_time > 0 else 0
                
                benchmarks[f"p2p_bandwidth_{size_name}"] = PerformanceBenchmark(
                    benchmark_name="communication",
                    metric_name=f"p2p_bandwidth_{size_name}",
                    value=p2p_bandwidth,
                    unit="MB/s",
                    timestamp=datetime.now(),
                    environment_info=self._get_environment_info(),
                    test_config={"tensor_size": size, "tensor_size_mb": tensor_size_mb}
                )
                
                self.logger.info(f"P2På¸¦å®½ {size_name}: {p2p_bandwidth:.2f} MB/s")
        
        return benchmarks
    
    def _generate_benchmark_data(self, config: Dict[str, Any]) -> List[Any]:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æ•°æ®"""
        data = []
        samples = config.get("samples", 100)
        enable_thinking = config.get("enable_thinking", False)
        enable_chinese = config.get("enable_chinese", False)
        
        for i in range(samples):
            if enable_thinking:
                # ç”Ÿæˆæ€è€ƒæ•°æ®
                example = ThinkingExample(
                    instruction=f"åˆ†æå¯†ç å­¦ç®—æ³• {i}",
                    thinking_process=f"<thinking>è¿™æ˜¯ç¬¬{i}ä¸ªæ€è€ƒè¿‡ç¨‹</thinking>",
                    final_response=f"è¿™æ˜¯ç¬¬{i}ä¸ªå›ç­”",
                    crypto_terms=["AES", "RSA", "SHA"],
                    reasoning_steps=[f"æ­¥éª¤{j}" for j in range(3)]
                )
            else:
                # ç”Ÿæˆæ™®é€šè®­ç»ƒæ•°æ®
                instruction = f"è§£é‡Šå¯†ç å­¦æ¦‚å¿µ {i}"
                if enable_chinese:
                    instruction = f"è¯·ç”¨ä¸­æ–‡è§£é‡Šå¯†ç å­¦æ¦‚å¿µ {i}"
                
                example = TrainingExample(
                    instruction=instruction,
                    input="",
                    output=f"è¿™æ˜¯ç¬¬{i}ä¸ªå¯†ç å­¦æ¦‚å¿µçš„è§£é‡Š",
                    thinking=None,
                    crypto_terms=["åŠ å¯†", "è§£å¯†", "å¯†é’¥"],
                    difficulty_level=1,
                    source_file=f"benchmark_{i}.md"
                )
            
            data.append(example)
        
        return data
    
    def _process_benchmark_data(self, data: List[Any], config: Dict[str, Any]) -> List[Any]:
        """å¤„ç†åŸºå‡†æµ‹è¯•æ•°æ®"""
        processed_data = []
        enable_chinese = config.get("enable_chinese", False)
        
        for item in data:
            if enable_chinese and hasattr(item, 'instruction'):
                # ä¸­æ–‡å¤„ç† - ç®€åŒ–å¤„ç†ï¼Œåªåšåˆ†è¯æµ‹è¯•
                tokens = self.chinese_processor.segment_text(item.instruction)
                processed_instruction = item.instruction  # ä¿æŒåŸæ–‡æœ¬
                
                if hasattr(item, 'output'):
                    output_tokens = self.chinese_processor.segment_text(item.output)
                    processed_output = item.output  # ä¿æŒåŸæ–‡æœ¬
                    # åˆ›å»ºå¤„ç†åçš„å‰¯æœ¬
                    if isinstance(item, TrainingExample):
                        processed_item = TrainingExample(
                            instruction=processed_instruction,
                            input=item.input,
                            output=processed_output,
                            thinking=item.thinking,
                            crypto_terms=item.crypto_terms,
                            difficulty_level=item.difficulty_level,
                            source_file=item.source_file
                        )
                    else:
                        processed_item = item
                else:
                    processed_item = item
            else:
                processed_item = item
            
            # å¯†ç å­¦æœ¯è¯­å¤„ç†
            if hasattr(processed_item, 'output'):
                term_annotations = self.crypto_processor.identify_crypto_terms(processed_item.output)
                crypto_terms = [ann.term for ann in term_annotations]
                if hasattr(processed_item, 'crypto_terms'):
                    processed_item.crypto_terms = crypto_terms
            
            processed_data.append(processed_item)
        
        return processed_data
    
    def _get_environment_info(self) -> Dict[str, Any]:
        """è·å–ç¯å¢ƒä¿¡æ¯"""
        gpu_infos = self.gpu_detector.get_all_gpu_info()
        
        return {
            "python_version": sys.version,
            "pytorch_version": torch.__version__,
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
            "gpu_count": len(gpu_infos),
            "gpu_info": [
                {
                    "gpu_id": gpu.gpu_id,
                    "name": gpu.name,
                    "total_memory": gpu.total_memory
                }
                for gpu in gpu_infos
            ],
            "cpu_count": psutil.cpu_count(),
            "total_memory": psutil.virtual_memory().total // (1024**3),  # GB
            "timestamp": datetime.now().isoformat()
        }


class PerformanceRegressionDetector:
    """æ€§èƒ½å›å½’æ£€æµ‹å™¨"""
    
    def __init__(self, baseline_file: str = "performance_baselines.json"):
        self.baseline_file = Path(baseline_file)
        self.logger = logging.getLogger(__name__)
        
        # å›å½’é˜ˆå€¼é…ç½®
        self.regression_thresholds = {
            "samples_per_second": -10.0,      # 10%ä¸‹é™
            "tokens_per_second": -10.0,       # 10%ä¸‹é™
            "memory_efficiency": -15.0,       # 15%ä¸‹é™
            "peak_memory_mb": 20.0,           # 20%å¢åŠ 
            "average_utilization": -15.0,     # 15%ä¸‹é™
            "p2p_bandwidth": -20.0            # 20%ä¸‹é™
        }
        
        # ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        self.severity_thresholds = {
            "minor": 15.0,     # 15%ä»¥å†…
            "major": 30.0,     # 30%ä»¥å†…
            "critical": 50.0   # 50%ä»¥ä¸Š
        }
    
    def load_baselines(self) -> Dict[str, PerformanceBenchmark]:
        """åŠ è½½æ€§èƒ½åŸºå‡†"""
        if not self.baseline_file.exists():
            self.logger.warning(f"åŸºå‡†æ–‡ä»¶ä¸å­˜åœ¨: {self.baseline_file}")
            return {}
        
        try:
            with open(self.baseline_file, 'r') as f:
                data = json.load(f)
            
            baselines = {}
            for key, value in data.items():
                baselines[key] = PerformanceBenchmark(
                    benchmark_name=value["benchmark_name"],
                    metric_name=value["metric_name"],
                    value=value["value"],
                    unit=value["unit"],
                    timestamp=datetime.fromisoformat(value["timestamp"]),
                    environment_info=value["environment_info"],
                    test_config=value["test_config"]
                )
            
            self.logger.info(f"åŠ è½½äº†{len(baselines)}ä¸ªæ€§èƒ½åŸºå‡†")
            return baselines
            
        except Exception as e:
            self.logger.error(f"åŠ è½½åŸºå‡†æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def save_baselines(self, benchmarks: Dict[str, PerformanceBenchmark]):
        """ä¿å­˜æ€§èƒ½åŸºå‡†"""
        try:
            data = {key: benchmark.to_dict() for key, benchmark in benchmarks.items()}
            
            with open(self.baseline_file, 'w') as f:
                json.dump(data, f, indent=2)
            
            self.logger.info(f"ä¿å­˜äº†{len(benchmarks)}ä¸ªæ€§èƒ½åŸºå‡†åˆ° {self.baseline_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜åŸºå‡†æ–‡ä»¶å¤±è´¥: {e}")
    
    def detect_regressions(self, current_benchmarks: Dict[str, PerformanceBenchmark],
                          baseline_benchmarks: Dict[str, PerformanceBenchmark]) -> List[PerformanceRegression]:
        """æ£€æµ‹æ€§èƒ½å›å½’"""
        regressions = []
        
        for metric_name, current_benchmark in current_benchmarks.items():
            if metric_name not in baseline_benchmarks:
                self.logger.warning(f"æœªæ‰¾åˆ°åŸºå‡†: {metric_name}")
                continue
            
            baseline_benchmark = baseline_benchmarks[metric_name]
            
            # è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
            current_value = current_benchmark.value
            baseline_value = baseline_benchmark.value
            
            if baseline_value == 0:
                self.logger.warning(f"åŸºå‡†å€¼ä¸º0ï¼Œè·³è¿‡: {metric_name}")
                continue
            
            change_percent = (current_value - baseline_value) / baseline_value * 100
            
            # ç¡®å®šå›å½’é˜ˆå€¼
            threshold = self._get_regression_threshold(metric_name)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå›å½’
            is_regression = False
            if threshold < 0:  # å€¼åº”è¯¥æ›´é«˜ï¼ˆå¦‚ååé‡ï¼‰
                is_regression = change_percent < threshold
            else:  # å€¼åº”è¯¥æ›´ä½ï¼ˆå¦‚å†…å­˜ä½¿ç”¨ï¼‰
                is_regression = change_percent > threshold
            
            # ç¡®å®šä¸¥é‡ç¨‹åº¦
            severity = self._determine_severity(abs(change_percent))
            
            if is_regression:
                regression = PerformanceRegression(
                    metric_name=metric_name,
                    current_value=current_value,
                    baseline_value=baseline_value,
                    change_percent=change_percent,
                    threshold_percent=threshold,
                    is_regression=True,
                    severity=severity
                )
                regressions.append(regression)
                
                self.logger.warning(f"æ£€æµ‹åˆ°å›å½’: {metric_name}, å˜åŒ–: {change_percent:.2f}%, ä¸¥é‡ç¨‹åº¦: {severity}")
        
        return regressions
    
    def _get_regression_threshold(self, metric_name: str) -> float:
        """è·å–å›å½’é˜ˆå€¼"""
        # æ ¹æ®æŒ‡æ ‡åç§°åŒ¹é…é˜ˆå€¼
        for pattern, threshold in self.regression_thresholds.items():
            if pattern in metric_name.lower():
                return threshold
        
        # é»˜è®¤é˜ˆå€¼
        return -10.0
    
    def _determine_severity(self, change_percent: float) -> str:
        """ç¡®å®šä¸¥é‡ç¨‹åº¦"""
        if change_percent <= self.severity_thresholds["minor"]:
            return "minor"
        elif change_percent <= self.severity_thresholds["major"]:
            return "major"
        else:
            return "critical"
    
    def generate_regression_report(self, regressions: List[PerformanceRegression],
                                 current_benchmarks: Dict[str, PerformanceBenchmark],
                                 baseline_benchmarks: Dict[str, PerformanceBenchmark]) -> Dict[str, Any]:
        """ç”Ÿæˆå›å½’æŠ¥å‘Š"""
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        regressions_by_severity = {
            "critical": [r for r in regressions if r.severity == "critical"],
            "major": [r for r in regressions if r.severity == "major"],
            "minor": [r for r in regressions if r.severity == "minor"]
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_metrics = len(current_benchmarks)
        regression_count = len(regressions)
        regression_rate = regression_count / total_metrics if total_metrics > 0 else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "summary": {
                "total_metrics": total_metrics,
                "regression_count": regression_count,
                "regression_rate": regression_rate,
                "critical_regressions": len(regressions_by_severity["critical"]),
                "major_regressions": len(regressions_by_severity["major"]),
                "minor_regressions": len(regressions_by_severity["minor"])
            },
            "regressions": {
                "critical": [r.to_dict() for r in regressions_by_severity["critical"]],
                "major": [r.to_dict() for r in regressions_by_severity["major"]],
                "minor": [r.to_dict() for r in regressions_by_severity["minor"]]
            },
            "recommendations": self._generate_recommendations(regressions),
            "baseline_info": {
                "baseline_count": len(baseline_benchmarks),
                "oldest_baseline": min(b.timestamp for b in baseline_benchmarks.values()).isoformat() if baseline_benchmarks else None,
                "newest_baseline": max(b.timestamp for b in baseline_benchmarks.values()).isoformat() if baseline_benchmarks else None
            },
            "current_test_info": {
                "test_time": datetime.now().isoformat(),
                "environment": list(current_benchmarks.values())[0].environment_info if current_benchmarks else {}
            }
        }
        
        return report
    
    def _generate_recommendations(self, regressions: List[PerformanceRegression]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # æŒ‰ç±»å‹åˆ†æå›å½’
        memory_regressions = [r for r in regressions if "memory" in r.metric_name.lower()]
        throughput_regressions = [r for r in regressions if any(x in r.metric_name.lower() for x in ["samples_per_second", "tokens_per_second"])]
        utilization_regressions = [r for r in regressions if "utilization" in r.metric_name.lower()]
        communication_regressions = [r for r in regressions if "bandwidth" in r.metric_name.lower()]
        
        if memory_regressions:
            recommendations.append("æ£€æµ‹åˆ°å†…å­˜ç›¸å…³å›å½’ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜ç®¡ç†ç­–ç•¥å’Œæ‰¹æ¬¡å¤§å°è®¾ç½®")
        
        if throughput_regressions:
            recommendations.append("æ£€æµ‹åˆ°ååé‡å›å½’ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ€§èƒ½")
        
        if utilization_regressions:
            recommendations.append("æ£€æµ‹åˆ°GPUåˆ©ç”¨ç‡å›å½’ï¼Œå»ºè®®æ£€æŸ¥è®¡ç®—è´Ÿè½½å’Œå¹¶è¡Œç­–ç•¥")
        
        if communication_regressions:
            recommendations.append("æ£€æµ‹åˆ°é€šä¿¡æ€§èƒ½å›å½’ï¼Œå»ºè®®æ£€æŸ¥å¤šGPUé€šä¿¡é…ç½®å’Œç½‘ç»œç¯å¢ƒ")
        
        # ä¸¥é‡ç¨‹åº¦å»ºè®®
        critical_regressions = [r for r in regressions if r.severity == "critical"]
        if critical_regressions:
            recommendations.append("å­˜åœ¨ä¸¥é‡æ€§èƒ½å›å½’ï¼Œå»ºè®®ç«‹å³è°ƒæŸ¥å¹¶ä¿®å¤")
        
        if not recommendations:
            recommendations.append("æœªæ£€æµ‹åˆ°æ˜¾è‘—æ€§èƒ½å›å½’ï¼Œç³»ç»Ÿæ€§èƒ½ç¨³å®š")
        
        return recommendations


class PerformanceBenchmarkTestSuite:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, output_dir: str = "performance_benchmark_output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        self.benchmark_runner = TrainingPerformanceBenchmark(str(self.output_dir))
        self.regression_detector = PerformanceRegressionDetector(
            str(self.output_dir / "performance_baselines.json")
        )
    
    def run_full_benchmark_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        self.logger.info("å¼€å§‹è¿è¡Œå®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶")
        start_time = time.time()
        
        all_benchmarks = {}
        
        # è¿è¡Œå„é¡¹åŸºå‡†æµ‹è¯•
        test_suites = [
            ("data_processing", self.benchmark_runner.run_data_processing_benchmark),
            ("memory_efficiency", self.benchmark_runner.run_memory_efficiency_benchmark),
            ("gpu_utilization", self.benchmark_runner.run_gpu_utilization_benchmark),
            ("communication", self.benchmark_runner.run_communication_benchmark)
        ]
        
        for suite_name, test_func in test_suites:
            self.logger.info(f"è¿è¡Œ {suite_name} åŸºå‡†æµ‹è¯•")
            try:
                benchmarks = test_func()
                all_benchmarks.update(benchmarks)
                self.logger.info(f"{suite_name} å®Œæˆï¼Œè·å¾— {len(benchmarks)} ä¸ªåŸºå‡†")
            except Exception as e:
                self.logger.error(f"{suite_name} æµ‹è¯•å¤±è´¥: {e}")
        
        total_time = time.time() - start_time
        
        # ä¿å­˜åŸºå‡†ç»“æœ
        self.regression_detector.save_baselines(all_benchmarks)
        
        # ç”ŸæˆåŸºå‡†æŠ¥å‘Š
        report = {
            "benchmark_summary": {
                "total_benchmarks": len(all_benchmarks),
                "test_duration_seconds": total_time,
                "test_suites": len(test_suites),
                "timestamp": datetime.now().isoformat()
            },
            "benchmarks": {key: benchmark.to_dict() for key, benchmark in all_benchmarks.items()},
            "environment_info": self.benchmark_runner._get_environment_info()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "benchmark_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼Œè€—æ—¶ {total_time:.2f}ç§’ï¼ŒæŠ¥å‘Šä¿å­˜è‡³ {report_file}")
        
        return report
    
    def run_regression_test(self) -> Dict[str, Any]:
        """è¿è¡Œå›å½’æµ‹è¯•"""
        self.logger.info("å¼€å§‹è¿è¡Œæ€§èƒ½å›å½’æµ‹è¯•")
        
        # åŠ è½½å†å²åŸºå‡†
        baseline_benchmarks = self.regression_detector.load_baselines()
        if not baseline_benchmarks:
            self.logger.warning("æœªæ‰¾åˆ°å†å²åŸºå‡†ï¼Œæ— æ³•è¿›è¡Œå›å½’æµ‹è¯•")
            return {"error": "æœªæ‰¾åˆ°å†å²åŸºå‡†"}
        
        # è¿è¡Œå½“å‰åŸºå‡†æµ‹è¯•
        current_report = self.run_full_benchmark_suite()
        current_benchmarks = {}
        
        for key, benchmark_dict in current_report["benchmarks"].items():
            current_benchmarks[key] = PerformanceBenchmark(
                benchmark_name=benchmark_dict["benchmark_name"],
                metric_name=benchmark_dict["metric_name"],
                value=benchmark_dict["value"],
                unit=benchmark_dict["unit"],
                timestamp=datetime.fromisoformat(benchmark_dict["timestamp"]),
                environment_info=benchmark_dict["environment_info"],
                test_config=benchmark_dict["test_config"]
            )
        
        # æ£€æµ‹å›å½’
        regressions = self.regression_detector.detect_regressions(
            current_benchmarks, baseline_benchmarks
        )
        
        # ç”Ÿæˆå›å½’æŠ¥å‘Š
        regression_report = self.regression_detector.generate_regression_report(
            regressions, current_benchmarks, baseline_benchmarks
        )
        
        # ä¿å­˜å›å½’æŠ¥å‘Š
        regression_file = self.output_dir / "regression_report.json"
        with open(regression_file, 'w') as f:
            json.dump(regression_report, f, indent=2)
        
        self.logger.info(f"å›å½’æµ‹è¯•å®Œæˆï¼Œå‘ç° {len(regressions)} ä¸ªå›å½’ï¼ŒæŠ¥å‘Šä¿å­˜è‡³ {regression_file}")
        
        return regression_report


# æµ‹è¯•ç”¨ä¾‹
class TestPerformanceBenchmarks(unittest.TestCase):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç”¨ä¾‹"""
    
    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»è®¾ç½®"""
        cls.benchmark_suite = PerformanceBenchmarkTestSuite("test_benchmark_output")
    
    def test_data_processing_benchmark(self):
        """æµ‹è¯•æ•°æ®å¤„ç†æ€§èƒ½åŸºå‡†"""
        benchmarks = self.benchmark_suite.benchmark_runner.run_data_processing_benchmark()
        self.assertGreater(len(benchmarks), 0, "åº”è¯¥ç”Ÿæˆæ•°æ®å¤„ç†åŸºå‡†")
        
        # æ£€æŸ¥åŸºå‡†æŒ‡æ ‡
        for benchmark in benchmarks.values():
            self.assertGreater(benchmark.value, 0, f"åŸºå‡†å€¼åº”è¯¥å¤§äº0: {benchmark.metric_name}")
    
    def test_memory_efficiency_benchmark(self):
        """æµ‹è¯•å†…å­˜æ•ˆç‡åŸºå‡†"""
        if not torch.cuda.is_available():
            self.skipTest("éœ€è¦CUDAæ”¯æŒ")
        
        benchmarks = self.benchmark_suite.benchmark_runner.run_memory_efficiency_benchmark()
        
        # æ£€æŸ¥å†…å­˜åŸºå‡†
        memory_benchmarks = [b for b in benchmarks.values() if "memory" in b.metric_name]
        self.assertGreater(len(memory_benchmarks), 0, "åº”è¯¥ç”Ÿæˆå†…å­˜åŸºå‡†")
    
    def test_gpu_utilization_benchmark(self):
        """æµ‹è¯•GPUåˆ©ç”¨ç‡åŸºå‡†"""
        if not torch.cuda.is_available():
            self.skipTest("éœ€è¦CUDAæ”¯æŒ")
        
        benchmarks = self.benchmark_suite.benchmark_runner.run_gpu_utilization_benchmark()
        
        # æ£€æŸ¥GPUåˆ©ç”¨ç‡åŸºå‡†
        gpu_benchmarks = [b for b in benchmarks.values() if "utilization" in b.metric_name]
        self.assertGreater(len(gpu_benchmarks), 0, "åº”è¯¥ç”ŸæˆGPUåˆ©ç”¨ç‡åŸºå‡†")
    
    def test_regression_detection(self):
        """æµ‹è¯•å›å½’æ£€æµ‹"""
        # åˆ›å»ºæ¨¡æ‹ŸåŸºå‡†
        baseline = PerformanceBenchmark(
            benchmark_name="test",
            metric_name="test_metric",
            value=100.0,
            unit="ops/sec",
            timestamp=datetime.now() - timedelta(days=1),
            environment_info={},
            test_config={}
        )
        
        # åˆ›å»ºå›å½’çš„å½“å‰åŸºå‡†
        current = PerformanceBenchmark(
            benchmark_name="test",
            metric_name="test_metric",
            value=80.0,  # 20%ä¸‹é™
            unit="ops/sec",
            timestamp=datetime.now(),
            environment_info={},
            test_config={}
        )
        
        # æ£€æµ‹å›å½’
        regressions = self.benchmark_suite.regression_detector.detect_regressions(
            {"test_metric": current},
            {"test_metric": baseline}
        )
        
        self.assertEqual(len(regressions), 1, "åº”è¯¥æ£€æµ‹åˆ°1ä¸ªå›å½’")
        self.assertTrue(regressions[0].is_regression, "åº”è¯¥æ ‡è®°ä¸ºå›å½’")
        self.assertEqual(regressions[0].change_percent, -20.0, "å˜åŒ–ç™¾åˆ†æ¯”åº”è¯¥æ˜¯-20%")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark_suite = PerformanceBenchmarkTestSuite()
    
    # è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
    print("å¼€å§‹è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶...")
    benchmark_report = benchmark_suite.run_full_benchmark_suite()
    
    print(f"åŸºå‡†æµ‹è¯•å®Œæˆï¼Œç”Ÿæˆäº† {benchmark_report['benchmark_summary']['total_benchmarks']} ä¸ªåŸºå‡†")
    
    # è¿è¡Œå›å½’æµ‹è¯•
    print("\nå¼€å§‹è¿è¡Œå›å½’æµ‹è¯•...")
    regression_report = benchmark_suite.run_regression_test()
    
    if "error" not in regression_report:
        regression_count = regression_report["summary"]["regression_count"]
        if regression_count > 0:
            print(f"âš ï¸  æ£€æµ‹åˆ° {regression_count} ä¸ªæ€§èƒ½å›å½’")
            critical_count = regression_report["summary"]["critical_regressions"]
            if critical_count > 0:
                print(f"ğŸš¨ å…¶ä¸­ {critical_count} ä¸ªä¸ºä¸¥é‡å›å½’")
        else:
            print("âœ“ æœªæ£€æµ‹åˆ°æ€§èƒ½å›å½’")
    else:
        print(f"å›å½’æµ‹è¯•è·³è¿‡: {regression_report['error']}")
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)