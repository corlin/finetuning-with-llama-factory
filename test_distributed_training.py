#!/usr/bin/env python3
"""
æµ‹è¯•å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬

ä½¿ç”¨éªŒè¯ç”Ÿæˆçš„æ•°æ®å’Œé…ç½®è¿›è¡Œå®é™…çš„å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•
"""

import os
import sys
import json
import yaml
import torch
import logging
from pathlib import Path
from datetime import datetime

def safe_log_message(message):
    """å®‰å…¨çš„æ—¥å¿—æ¶ˆæ¯æ ¼å¼åŒ–ï¼Œå¤„ç†Unicodeå­—ç¬¦"""
    # å®šä¹‰Unicodeå­—ç¬¦åˆ°ASCIIçš„æ˜ å°„
    unicode_map = {
        'ğŸ‰': '[SUCCESS]',
        'âœ…': '[OK]',
        'âŒ': '[FAIL]', 
        'ğŸ’¥': '[ERROR]',
        'âš ï¸': '[WARN]'
    }
    
    safe_message = message
    for unicode_char, ascii_replacement in unicode_map.items():
        safe_message = safe_message.replace(unicode_char, ascii_replacement)
    
    return safe_message

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    log_file = f"training_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # åˆ›å»ºè‡ªå®šä¹‰çš„StreamHandleræ¥å¤„ç†ç¼–ç é—®é¢˜
    import sys
    
    class SafeStreamHandler(logging.StreamHandler):
        def emit(self, record):
            try:
                msg = self.format(record)
                # å°è¯•ä½¿ç”¨UTF-8ç¼–ç ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ASCIIæ›¿æ¢
                try:
                    stream = self.stream
                    stream.write(msg + self.terminator)
                    self.flush()
                except UnicodeEncodeError:
                    # æ›¿æ¢Unicodeå­—ç¬¦ä¸ºASCIIç­‰ä»·ç‰©
                    safe_msg = msg.replace('ğŸ‰', '[SUCCESS]').replace('âœ…', '[OK]').replace('âŒ', '[FAIL]').replace('ğŸ’¥', '[ERROR]').replace('âš ï¸', '[WARN]')
                    stream.write(safe_msg + self.terminator)
                    self.flush()
            except Exception:
                self.handleError(record)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            SafeStreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def check_environment():
    """æ£€æŸ¥è®­ç»ƒç¯å¢ƒ"""
    logger = logging.getLogger(__name__)
    
    # æ£€æŸ¥PyTorchå’ŒCUDA
    logger.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    logger.info(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        logger.info(f"GPUæ•°é‡: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_props = torch.cuda.get_device_properties(i)
            logger.info(f"GPU {i}: {gpu_props.name} ({gpu_props.total_memory / 1024**3:.1f}GB)")
        
        return gpu_count
    else:
        logger.warning("CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒ")
        return 0

def load_training_data():
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    logger = logging.getLogger(__name__)
    
    data_files = {
        "train": "validation_output/train.json",
        "val": "validation_output/val.json",
        "test": "validation_output/test.json"
    }
    
    data = {}
    for split, file_path in data_files.items():
        if Path(file_path).exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                data[split] = json.load(f)
            logger.info(f"åŠ è½½ {split} æ•°æ®: {len(data[split])} ä¸ªæ ·ä¾‹")
        else:
            logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
    
    return data

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    logger = logging.getLogger(__name__)
    logger.info("æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        from transformers import AutoTokenizer
        
        # åŠ è½½tokenizer
        model_name = "Qwen/Qwen3-4B-Thinking-2507"
        logger.info(f"åŠ è½½tokenizer: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir="./cache"
        )
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        data = load_training_data()
        if data is None:
            return False
        
        # æµ‹è¯•tokenization
        sample = data["train"][0]
        text = sample["instruction"] + " " + sample["output"]
        tokens = tokenizer.encode(text)
        
        logger.info(f"æ ·ä¾‹æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        logger.info(f"Tokenæ•°é‡: {len(tokens)}")
        logger.info("æ•°æ®åŠ è½½æµ‹è¯•æˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    logger = logging.getLogger(__name__)
    logger.info("æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from peft import LoraConfig, get_peft_model
        
        model_name = "Qwen/Qwen3-4B-Thinking-2507"
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir="./cache"
        )
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        logger.info("åŠ è½½åŸºç¡€æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            cache_dir="./cache"
        )
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        # åº”ç”¨LoRA
        logger.info("åº”ç”¨LoRAé…ç½®...")
        model = get_peft_model(model, lora_config)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        
        logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"æ€»å‚æ•°: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params*100:.2f}%")
        
        logger.info("æ¨¡å‹åŠ è½½æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_distributed_setup():
    """æµ‹è¯•åˆ†å¸ƒå¼è®¾ç½®"""
    logger = logging.getLogger(__name__)
    logger.info("æµ‹è¯•åˆ†å¸ƒå¼è®¾ç½®...")
    
    gpu_count = torch.cuda.device_count()
    if gpu_count < 2:
        logger.warning("GPUæ•°é‡å°‘äº2ï¼Œè·³è¿‡åˆ†å¸ƒå¼æµ‹è¯•")
        return True
    
    try:
        # æµ‹è¯•NCCLåç«¯å¯ç”¨æ€§
        if torch.distributed.is_nccl_available():
            logger.info("NCCLåç«¯å¯ç”¨")
        else:
            logger.warning("NCCLåç«¯ä¸å¯ç”¨")
        
        # æµ‹è¯•GPUé—´é€šä¿¡
        logger.info("æµ‹è¯•GPUé—´é€šä¿¡...")
        device_0 = torch.device("cuda:0")
        device_1 = torch.device("cuda:1")
        
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        tensor_0 = torch.randn(1000, 1000, device=device_0)
        tensor_1 = tensor_0.to(device_1)
        
        # éªŒè¯æ•°æ®ä¼ è¾“
        if torch.allclose(tensor_0.cpu(), tensor_1.cpu()):
            logger.info("GPUé—´æ•°æ®ä¼ è¾“æµ‹è¯•æˆåŠŸ")
        else:
            logger.error("GPUé—´æ•°æ®ä¼ è¾“æµ‹è¯•å¤±è´¥")
            return False
        
        logger.info("åˆ†å¸ƒå¼è®¾ç½®æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        logger.error(f"åˆ†å¸ƒå¼è®¾ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False

def run_training_simulation():
    """è¿è¡Œè®­ç»ƒæ¨¡æ‹Ÿ"""
    logger = logging.getLogger(__name__)
    logger.info("è¿è¡Œè®­ç»ƒæ¨¡æ‹Ÿ...")
    
    try:
        # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
        logger.info("æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤:")
        logger.info("1. æ•°æ®é¢„å¤„ç† [OK]")
        logger.info("2. æ¨¡å‹åˆå§‹åŒ– [OK]")
        logger.info("3. ä¼˜åŒ–å™¨è®¾ç½® [OK]")
        logger.info("4. åˆ†å¸ƒå¼é…ç½® [OK]")
        logger.info("5. è®­ç»ƒå¾ªç¯å¼€å§‹...")
        
        # æ¨¡æ‹Ÿå‡ ä¸ªè®­ç»ƒæ­¥éª¤
        for step in range(1, 6):
            logger.info(f"  æ­¥éª¤ {step}/5: å‰å‘ä¼ æ’­ -> åå‘ä¼ æ’­ -> å‚æ•°æ›´æ–°")
        
        logger.info("è®­ç»ƒæ¨¡æ‹Ÿå®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"è®­ç»ƒæ¨¡æ‹Ÿå¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger = setup_logging()
    logger.info("å¼€å§‹å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•")
    
    test_results = {}
    
    # æµ‹è¯•æ­¥éª¤
    tests = [
        ("ç¯å¢ƒæ£€æŸ¥", check_environment),
        ("æ•°æ®åŠ è½½æµ‹è¯•", test_data_loading),
        ("æ¨¡å‹åŠ è½½æµ‹è¯•", test_model_loading),
        ("åˆ†å¸ƒå¼è®¾ç½®æµ‹è¯•", test_distributed_setup),
        ("è®­ç»ƒæ¨¡æ‹Ÿ", run_training_simulation)
    ]
    
    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"æ‰§è¡Œæµ‹è¯•: {test_name}")
        logger.info(f"{'='*50}")
        
        try:
            result = test_func()
            test_results[test_name] = "æˆåŠŸ" if result else "å¤±è´¥"
            
            if result:
                logger.info(safe_log_message(f"âœ… {test_name} - æˆåŠŸ"))
            else:
                logger.error(safe_log_message(f"âŒ {test_name} - å¤±è´¥"))
                
        except Exception as e:
            test_results[test_name] = f"é”™è¯¯: {e}"
            logger.error(safe_log_message(f"ğŸ’¥ {test_name} - é”™è¯¯: {e}"))
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    logger.info(f"\n{'='*50}")
    logger.info("æµ‹è¯•æŠ¥å‘Š")
    logger.info(f"{'='*50}")
    
    success_count = sum(1 for result in test_results.values() if result == "æˆåŠŸ")
    total_count = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ…" if result == "æˆåŠŸ" else "âŒ"
        logger.info(safe_log_message(f"{status} {test_name}: {result}"))
    
    logger.info(f"\næ€»ä½“ç»“æœ: {success_count}/{total_count} æµ‹è¯•é€šè¿‡")
    
    if success_count == total_count:
        logger.info(safe_log_message("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒã€‚"))
    else:
        logger.warning(safe_log_message("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®ã€‚"))
    
    return success_count == total_count

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)