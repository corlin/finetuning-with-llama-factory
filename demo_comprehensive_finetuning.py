#!/usr/bin/env python3
"""
ç»¼åˆå¾®è°ƒæ¼”ç¤ºç¨‹åº

åŸºäºå½“å‰å·²å®Œæˆçš„åŠŸèƒ½ï¼Œä½¿ç”¨ data/raw æ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒçš„å®Œæ•´æ¼”ç¤ºã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- è‡ªåŠ¨åŠ è½½å’Œå¤„ç† data/raw ä¸­çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶
- æ™ºèƒ½æ•°æ®é¢„å¤„ç†å’Œæ ¼å¼è½¬æ¢
- è‡ªåŠ¨é…ç½® GPU å¹¶è¡Œç­–ç•¥å’Œ LoRA å‚æ•°
- é›†æˆåŸç”ŸPyTorchè¿›è¡Œæ¨¡å‹å¾®è°ƒ
- å®æ—¶è®­ç»ƒç›‘æ§å’Œè¿›åº¦è·Ÿè¸ª
- å®Œæ•´çš„è®­ç»ƒæµæ°´çº¿ç®¡ç†

ä½¿ç”¨æ–¹æ³•ï¼š
    uv run python demo_comprehensive_finetuning.py
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from data_models import TrainingExample, ThinkingExample, DifficultyLevel
from config_manager import TrainingConfig, DataConfig, SystemConfig
from lora_config_optimizer import LoRAConfigOptimizer, LoRAMemoryProfile
from parallel_config import ParallelConfig, ParallelStrategy
from gpu_utils import GPUDetector
# LlamaFactory adapter removed - using direct training engine
from training_pipeline import TrainingPipelineOrchestrator, PipelineState
from thinking_generator import ThinkingDataGenerator


class ComprehensiveFinetuningDemo:
    """ç»¼åˆå¾®è°ƒæ¼”ç¤ºç±»"""
    
    def __init__(self, output_dir: str = "demo_output"):
        """
        åˆå§‹åŒ–æ¼”ç¤ºç¨‹åº
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.gpu_detector = GPUDetector()
        self.lora_optimizer = LoRAConfigOptimizer()
        # LlamaFactory adapter removed - using direct training engine
        self.thinking_processor = ThinkingDataGenerator()
        
        # æ•°æ®å­˜å‚¨
        self.training_data: List[TrainingExample] = []
        self.thinking_data: List[ThinkingExample] = []
        
        self.logger.info("ç»¼åˆå¾®è°ƒæ¼”ç¤ºç¨‹åºåˆå§‹åŒ–å®Œæˆ")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_file = self.output_dir / f"demo_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
    
    def load_raw_data(self, data_dir: str = "data/raw") -> bool:
        """
        åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½æ•°æ®
        """
        try:
            data_path = Path(data_dir)
            if not data_path.exists():
                self.logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}")
                return False
            
            # æŸ¥æ‰¾æ‰€æœ‰markdownæ–‡ä»¶
            md_files = list(data_path.glob("*.md"))
            if not md_files:
                self.logger.error(f"åœ¨ {data_path} ä¸­æœªæ‰¾åˆ°markdownæ–‡ä»¶")
                return False
            
            self.logger.info(f"æ‰¾åˆ° {len(md_files)} ä¸ªæ•°æ®æ–‡ä»¶")
            
            total_examples = 0
            
            for md_file in md_files:
                self.logger.info(f"å¤„ç†æ–‡ä»¶: {md_file.name}")
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # è§£æQAå¯¹
                examples = self.parse_qa_content(content, str(md_file))
                
                if examples:
                    self.training_data.extend(examples)
                    total_examples += len(examples)
                    self.logger.info(f"ä» {md_file.name} è§£æå‡º {len(examples)} ä¸ªè®­ç»ƒæ ·ä¾‹")
                else:
                    self.logger.warning(f"ä» {md_file.name} æœªè§£æå‡ºä»»ä½•è®­ç»ƒæ ·ä¾‹")
            
            self.logger.info(f"æ€»å…±åŠ è½½äº† {total_examples} ä¸ªè®­ç»ƒæ ·ä¾‹")
            
            # å¤„ç†thinkingæ•°æ®
            self.process_thinking_data()
            
            return total_examples > 0
            
        except Exception as e:
            self.logger.error(f"åŠ è½½åŸå§‹æ•°æ®å¤±è´¥: {e}")
            return False
    
    def parse_qa_content(self, content: str, source_file: str) -> List[TrainingExample]:
        """
        è§£æQAå†…å®¹ä¸ºè®­ç»ƒæ ·ä¾‹
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            source_file: æºæ–‡ä»¶è·¯å¾„
            
        Returns:
            List[TrainingExample]: è®­ç»ƒæ ·ä¾‹åˆ—è¡¨
        """
        examples = []
        
        try:
            # ç›´æ¥ä½¿ç”¨ç®€å•è§£ææ–¹æ³•
            examples = self.simple_parse_qa(content, source_file)
                
        except Exception as e:
            self.logger.error(f"è§£æQAå†…å®¹å¤±è´¥: {e}")
            examples = []
        
        return examples
    
    def simple_parse_qa(self, content: str, source_file: str) -> List[TrainingExample]:
        """
        ç®€å•çš„QAè§£ææ–¹æ³•
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            source_file: æºæ–‡ä»¶è·¯å¾„
            
        Returns:
            List[TrainingExample]: è®­ç»ƒæ ·ä¾‹åˆ—è¡¨
        """
        examples = []
        
        # åˆ†å‰²å†…å®¹ä¸ºæ®µè½
        sections = content.split('\n\n')
        
        current_question = ""
        current_thinking = ""
        current_answer = ""
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é—®é¢˜
            if section.startswith('### Q') or section.startswith('##') and '?' in section:
                # ä¿å­˜ä¹‹å‰çš„QAå¯¹
                if current_question and current_answer:
                    example = self.create_training_example(
                        current_question, current_answer, current_thinking, source_file
                    )
                    if example:
                        examples.append(example)
                
                # å¼€å§‹æ–°çš„QAå¯¹
                current_question = section
                current_thinking = ""
                current_answer = ""
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯thinkingå†…å®¹
            elif '<thinking>' in section:
                current_thinking = section
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç­”æ¡ˆ
            elif section.startswith('A') and ':' in section:
                current_answer = section
        
        # å¤„ç†æœ€åä¸€ä¸ªQAå¯¹
        if current_question and current_answer:
            example = self.create_training_example(
                current_question, current_answer, current_thinking, source_file
            )
            if example:
                examples.append(example)
        
        return examples
    
    def create_training_example(self, question: str, answer: str, thinking: str, source_file: str) -> Optional[TrainingExample]:
        """
        åˆ›å»ºè®­ç»ƒæ ·ä¾‹
        
        Args:
            question: é—®é¢˜
            answer: ç­”æ¡ˆ
            thinking: æ€è€ƒè¿‡ç¨‹
            source_file: æºæ–‡ä»¶
            
        Returns:
            Optional[TrainingExample]: è®­ç»ƒæ ·ä¾‹
        """
        try:
            # æ¸…ç†é—®é¢˜æ–‡æœ¬
            instruction = question.replace('###', '').replace('##', '').strip()
            if ':' in instruction:
                instruction = instruction.split(':', 1)[1].strip()
            
            # æ¸…ç†ç­”æ¡ˆæ–‡æœ¬
            output = answer
            if ':' in output and output.startswith('A'):
                output = output.split(':', 1)[1].strip()
            
            # å¤„ç†thinkingå†…å®¹
            thinking_content = None
            if thinking and '<thinking>' in thinking:
                thinking_content = thinking
            
            if not instruction or not output:
                return None
            
            # æå–å¯†ç å­¦æœ¯è¯­
            crypto_terms = self.extract_crypto_terms(instruction + " " + output)
            
            # åˆ¤æ–­éš¾åº¦çº§åˆ«
            difficulty = self.determine_difficulty(instruction, output)
            
            return TrainingExample(
                instruction=instruction,
                input="",
                output=output,
                thinking=thinking_content,
                crypto_terms=crypto_terms,
                difficulty_level=difficulty,
                source_file=source_file
            )
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºè®­ç»ƒæ ·ä¾‹å¤±è´¥: {e}")
            return None
    
    def extract_crypto_terms(self, text: str) -> List[str]:
        """
        æå–å¯†ç å­¦æœ¯è¯­
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            List[str]: å¯†ç å­¦æœ¯è¯­åˆ—è¡¨
        """
        # å¸¸è§å¯†ç å­¦æœ¯è¯­åˆ—è¡¨
        crypto_keywords = [
            "å¯†ç å­¦", "åŠ å¯†", "è§£å¯†", "å“ˆå¸Œ", "æ•°å­—ç­¾å", "å…¬é’¥", "ç§é’¥", "å¯¹ç§°åŠ å¯†", "éå¯¹ç§°åŠ å¯†",
            "AES", "RSA", "SHA", "MD5", "DES", "3DES", "ECC", "DSA", "ECDSA",
            "å¯†é’¥ç®¡ç†", "è¯ä¹¦", "PKI", "CA", "æ•°å­—è¯ä¹¦", "èº«ä»½è®¤è¯", "è®¿é—®æ§åˆ¶",
            "å®Œæ•´æ€§", "æœºå¯†æ€§", "çœŸå®æ€§", "ä¸å¯å¦è®¤æ€§", "éšæœºæ•°", "å¯†é’¥äº¤æ¢",
            "åŒºå—é“¾", "æ¯”ç‰¹å¸", "ä»¥å¤ªåŠ", "æ™ºèƒ½åˆçº¦", "å…±è¯†ç®—æ³•", "å·¥ä½œé‡è¯æ˜"
        ]
        
        found_terms = []
        text_lower = text.lower()
        
        for term in crypto_keywords:
            if term.lower() in text_lower or term in text:
                found_terms.append(term)
        
        return list(set(found_terms))  # å»é‡
    
    def determine_difficulty(self, instruction: str, output: str) -> DifficultyLevel:
        """
        åˆ¤æ–­éš¾åº¦çº§åˆ«
        
        Args:
            instruction: æŒ‡ä»¤
            output: è¾“å‡º
            
        Returns:
            DifficultyLevel: éš¾åº¦çº§åˆ«
        """
        text = instruction + " " + output
        text_len = len(text)
        
        # åŸºäºæ–‡æœ¬é•¿åº¦å’Œå¤æ‚åº¦åˆ¤æ–­
        if text_len < 200:
            return DifficultyLevel.BEGINNER
        elif text_len < 500:
            return DifficultyLevel.INTERMEDIATE
        elif text_len < 1000:
            return DifficultyLevel.ADVANCED
        else:
            return DifficultyLevel.EXPERT
    
    def process_thinking_data(self):
        """å¤„ç†thinkingæ•°æ®"""
        thinking_examples = []
        
        for example in self.training_data:
            if example.has_thinking():
                thinking_example = example.to_thinking_example()
                if thinking_example:
                    thinking_examples.append(thinking_example)
        
        self.thinking_data = thinking_examples
        self.logger.info(f"å¤„ç†äº† {len(thinking_examples)} ä¸ªthinkingæ ·ä¾‹")
    
    def auto_configure_training(self) -> tuple:
        """
        è‡ªåŠ¨é…ç½®è®­ç»ƒå‚æ•°
        
        Returns:
            tuple: (training_config, data_config, lora_config, parallel_config, system_config)
        """
        self.logger.info("å¼€å§‹è‡ªåŠ¨é…ç½®è®­ç»ƒå‚æ•°...")
        
        # æ£€æµ‹GPUç¯å¢ƒ
        gpu_infos = self.gpu_detector.get_all_gpu_info()
        gpu_count = len(gpu_infos) if gpu_infos else 1
        
        self.logger.info(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        
        # é…ç½®è®­ç»ƒå‚æ•°
        training_config = TrainingConfig(
            output_dir=str(self.output_dir / "model_output"),
            num_train_epochs=2,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=4,
            learning_rate=2e-4,
            lr_scheduler_type="cosine",
            warmup_ratio=0.1,
            weight_decay=0.01,
            save_steps=100,
            eval_steps=100,
            logging_steps=10,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            bf16=True,
            fp16=False,
            seed=42
        )
        
        # é…ç½®æ•°æ®å‚æ•°
        data_config = DataConfig(
            max_samples=len(self.training_data),
            train_split_ratio=0.9,
            eval_split_ratio=0.1,
            test_split_ratio=0.0,
            shuffle_data=True
        )
        
        # é…ç½®å¹¶è¡Œç­–ç•¥
        if gpu_count > 1:
            parallel_config = ParallelConfig(
                strategy=ParallelStrategy.DATA_PARALLEL,
                data_parallel_size=gpu_count,
                tensor_parallel_size=1,
                pipeline_parallel_size=1,
                master_addr="localhost",
                master_port=29500
            )
        else:
            parallel_config = ParallelConfig(
                strategy=ParallelStrategy.DATA_PARALLEL,
                data_parallel_size=1,
                tensor_parallel_size=1,
                pipeline_parallel_size=1
            )
        
        # é…ç½®LoRAå‚æ•°
        if gpu_infos:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUçš„ä¿¡æ¯æ¥é…ç½®LoRA
            gpu_memory = gpu_infos[0].total_memory  # ä»¥MBä¸ºå•ä½
            lora_config = self.lora_optimizer.optimize_for_single_gpu(
                available_memory_mb=gpu_memory,
                batch_size=training_config.per_device_train_batch_size,
                sequence_length=2048
            )
        else:
            # é»˜è®¤LoRAé…ç½®
            lora_config = LoRAMemoryProfile(
                rank=8,
                alpha=16,
                dropout=0.1,
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
            )
        
        # ç³»ç»Ÿé…ç½®
        system_config = SystemConfig(
            mixed_precision="bf16" if gpu_count > 0 else "no",
            cuda_visible_devices=",".join(str(i) for i in range(gpu_count)) if gpu_count > 0 else None,
            log_level="INFO"
        )
        
        self.logger.info("è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ")
        self.logger.info(f"- è®­ç»ƒè½®æ•°: {training_config.num_train_epochs}")
        self.logger.info(f"- æ‰¹æ¬¡å¤§å°: {training_config.per_device_train_batch_size}")
        self.logger.info(f"- å­¦ä¹ ç‡: {training_config.learning_rate}")
        self.logger.info(f"- LoRA rank: {lora_config.rank}")
        self.logger.info(f"- å¹¶è¡Œç­–ç•¥: {parallel_config.strategy.value}")
        
        return training_config, data_config, lora_config, parallel_config, system_config
    
    def run_training_pipeline(self, 
                            training_config: TrainingConfig,
                            data_config: DataConfig,
                            lora_config: LoRAMemoryProfile,
                            parallel_config: ParallelConfig,
                            system_config: SystemConfig) -> bool:
        """
        è¿è¡Œè®­ç»ƒæµæ°´çº¿
        
        Args:
            training_config: è®­ç»ƒé…ç½®
            data_config: æ•°æ®é…ç½®
            lora_config: LoRAé…ç½®
            parallel_config: å¹¶è¡Œé…ç½®
            system_config: ç³»ç»Ÿé…ç½®
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå®Œæˆè®­ç»ƒ
        """
        try:
            # åˆ›å»ºæµæ°´çº¿ç¼–æ’å™¨
            pipeline_id = f"demo_finetuning_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            orchestrator = TrainingPipelineOrchestrator(
                pipeline_id=pipeline_id,
                output_dir=str(self.output_dir / "pipeline"),
                logger=self.logger
            )
            
            # é…ç½®æµæ°´çº¿
            orchestrator.configure_pipeline(
                training_data=self.training_data,
                training_config=training_config,
                data_config=data_config,
                lora_config=lora_config,
                parallel_config=parallel_config,
                system_config=system_config
            )
            
            # æ·»åŠ è¿›åº¦å›è°ƒ
            def progress_callback(state: PipelineState):
                self.logger.info(f"è®­ç»ƒè¿›åº¦: {state.progress:.1f}% - å½“å‰é˜¶æ®µ: {state.current_stage.value}")
                if state.current_stage_runtime:
                    self.logger.info(f"å½“å‰é˜¶æ®µè¿è¡Œæ—¶é—´: {state.current_stage_runtime}")
            
            orchestrator.add_progress_callback(progress_callback)
            
            # è¿è¡Œæµæ°´çº¿
            self.logger.info("å¼€å§‹æ‰§è¡Œè®­ç»ƒæµæ°´çº¿...")
            success = orchestrator.run_pipeline()
            
            if success:
                self.logger.info("è®­ç»ƒæµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
                
                # ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
                self.generate_training_report(orchestrator)
                
            else:
                self.logger.error("è®­ç»ƒæµæ°´çº¿æ‰§è¡Œå¤±è´¥ï¼")
                if orchestrator.state.error_message:
                    self.logger.error(f"é”™è¯¯ä¿¡æ¯: {orchestrator.state.error_message}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"è¿è¡Œè®­ç»ƒæµæ°´çº¿å¤±è´¥: {e}")
            return False
    
    def generate_training_report(self, orchestrator: TrainingPipelineOrchestrator):
        """
        ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        
        Args:
            orchestrator: æµæ°´çº¿ç¼–æ’å™¨
        """
        try:
            state = orchestrator.get_state()
            
            report = {
                "demo_info": {
                    "name": "ç»¼åˆå¾®è°ƒæ¼”ç¤ºç¨‹åº",
                    "version": "1.0.0",
                    "execution_time": datetime.now().isoformat()
                },
                "data_summary": {
                    "total_training_examples": len(self.training_data),
                    "thinking_examples": len(self.thinking_data),
                    "data_sources": list(set(ex.source_file for ex in self.training_data)),
                    "difficulty_distribution": self.get_difficulty_distribution(),
                    "crypto_terms_count": len(set(term for ex in self.training_data for term in ex.crypto_terms))
                },
                "training_summary": state.to_dict(),
                "output_files": {
                    "model_output": str(self.output_dir / "model_output"),
                    "pipeline_output": str(self.output_dir / "pipeline"),
                    "logs": str(self.output_dir)
                }
            }
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = self.output_dir / f"training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            
            # æ‰“å°æ‘˜è¦
            self.print_training_summary(report)
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå¤±è´¥: {e}")
    
    def get_difficulty_distribution(self) -> Dict[str, int]:
        """è·å–éš¾åº¦åˆ†å¸ƒ"""
        distribution = {}
        for example in self.training_data:
            level = example.difficulty_level.name
            distribution[level] = distribution.get(level, 0) + 1
        return distribution
    
    def print_training_summary(self, report: Dict[str, Any]):
        """æ‰“å°è®­ç»ƒæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ‰ ç»¼åˆå¾®è°ƒæ¼”ç¤ºç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        print("="*60)
        
        data_summary = report["data_summary"]
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   - è®­ç»ƒæ ·ä¾‹æ€»æ•°: {data_summary['total_training_examples']}")
        print(f"   - Thinkingæ ·ä¾‹: {data_summary['thinking_examples']}")
        print(f"   - æ•°æ®æºæ–‡ä»¶: {len(data_summary['data_sources'])}")
        print(f"   - å¯†ç å­¦æœ¯è¯­: {data_summary['crypto_terms_count']}")
        
        print(f"\nğŸ“ˆ éš¾åº¦åˆ†å¸ƒ:")
        for level, count in data_summary["difficulty_distribution"].items():
            print(f"   - {level}: {count}")
        
        training_summary = report["training_summary"]
        print(f"\nğŸš€ è®­ç»ƒçŠ¶æ€:")
        print(f"   - çŠ¶æ€: {training_summary['status']}")
        print(f"   - è¿›åº¦: {training_summary['progress']:.1f}%")
        print(f"   - å½“å‰é˜¶æ®µ: {training_summary['current_stage']}")
        
        if training_summary.get('runtime_seconds'):
            runtime = training_summary['runtime_seconds']
            print(f"   - è¿è¡Œæ—¶é—´: {runtime//3600:.0f}h {(runtime%3600)//60:.0f}m {runtime%60:.0f}s")
        
        output_files = report["output_files"]
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   - æ¨¡å‹è¾“å‡º: {output_files['model_output']}")
        print(f"   - æµæ°´çº¿è¾“å‡º: {output_files['pipeline_output']}")
        print(f"   - æ—¥å¿—æ–‡ä»¶: {output_files['logs']}")
        
        print("\n" + "="*60)
    
    def run_demo(self, data_dir: str = "data/raw") -> bool:
        """
        è¿è¡Œå®Œæ•´æ¼”ç¤º
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        try:
            print("ğŸš€ å¯åŠ¨ç»¼åˆå¾®è°ƒæ¼”ç¤ºç¨‹åº...")
            print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            
            # 1. åŠ è½½åŸå§‹æ•°æ®
            print("\nğŸ“– æ­¥éª¤ 1: åŠ è½½åŸå§‹æ•°æ®...")
            if not self.load_raw_data(data_dir):
                print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
                return False
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.training_data)} ä¸ªè®­ç»ƒæ ·ä¾‹")
            
            # 2. è‡ªåŠ¨é…ç½®è®­ç»ƒå‚æ•°
            print("\nâš™ï¸ æ­¥éª¤ 2: è‡ªåŠ¨é…ç½®è®­ç»ƒå‚æ•°...")
            configs = self.auto_configure_training()
            training_config, data_config, lora_config, parallel_config, system_config = configs
            
            print("âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ")
            
            # 3. è¿è¡Œè®­ç»ƒæµæ°´çº¿
            print("\nğŸ¯ æ­¥éª¤ 3: è¿è¡Œè®­ç»ƒæµæ°´çº¿...")
            success = self.run_training_pipeline(
                training_config, data_config, lora_config, parallel_config, system_config
            )
            
            if success:
                print("âœ… è®­ç»ƒæµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")
                return True
            else:
                print("âŒ è®­ç»ƒæµæ°´çº¿æ‰§è¡Œå¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"æ¼”ç¤ºç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
            print(f"âŒ æ¼”ç¤ºç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç»¼åˆå¾®è°ƒæ¼”ç¤ºç¨‹åº")
    parser.add_argument("--data-dir", default="data/raw", help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output-dir", default="demo_output", help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºæ¼”ç¤ºç¨‹åº
    demo = ComprehensiveFinetuningDemo(output_dir=args.output_dir)
    
    # è¿è¡Œæ¼”ç¤º
    success = demo.run_demo(data_dir=args.data_dir)
    
    if success:
        print("\nğŸ‰ æ¼”ç¤ºç¨‹åºæ‰§è¡ŒæˆåŠŸï¼")
        sys.exit(0)
    else:
        print("\nğŸ’¥ æ¼”ç¤ºç¨‹åºæ‰§è¡Œå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == "__main__":
    main()