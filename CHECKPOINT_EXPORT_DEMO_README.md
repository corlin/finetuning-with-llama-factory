# Qwen3-4B-Thinking æ£€æŸ¥ç‚¹åˆå¹¶å’Œé‡åŒ–å¯¼å‡ºæ¼”ç¤º

æœ¬ç›®å½•åŒ…å«ä¸‰ä¸ªæ¼”ç¤ºç¨‹åºï¼Œå±•ç¤ºå¦‚ä½•å°†å¾®è°ƒæ£€æŸ¥ç‚¹åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹å¹¶å¯¼å‡ºå¤šç§æ ¼å¼çš„é‡åŒ–æ¨¡å‹ã€‚

## æ¼”ç¤ºç¨‹åºæ¦‚è§ˆ

### 1. `demo_checkpoint_merge_and_export.py` - å®Œæ•´åŠŸèƒ½æ¼”ç¤º
**åŠŸèƒ½æœ€å…¨é¢çš„æ¼”ç¤ºç¨‹åº**

- âœ… å®Œæ•´çš„LoRAæ£€æŸ¥ç‚¹åˆå¹¶æµç¨‹
- âœ… å¤šæ ¼å¼é‡åŒ–å¯¼å‡ºï¼ˆINT8, INT4, GPTQ, FP16ï¼‰
- âœ… ä¸­æ–‡å¤„ç†èƒ½åŠ›éªŒè¯
- âœ… è‡ªåŠ¨ç”Ÿæˆéƒ¨ç½²åŒ…å’Œæ–‡æ¡£
- âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•
- âš ï¸ éœ€è¦å®Œæ•´çš„ä¾èµ–ç¯å¢ƒ

**é€‚ç”¨åœºæ™¯**: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Œéœ€è¦å®Œæ•´åŠŸèƒ½

### 2. `simple_checkpoint_merge_demo.py` - ç®€åŒ–æ¼”ç¤º
**æœ€å®¹æ˜“è¿è¡Œçš„æ¼”ç¤ºç¨‹åº**

- âœ… æ£€æŸ¥ç‚¹ç»“æ„åˆ†æ
- âœ… æ¨¡æ‹Ÿåˆå¹¶è¿‡ç¨‹ï¼ˆæ— éœ€å¤æ‚ä¾èµ–ï¼‰
- âœ… ç”Ÿæˆå¤šç²¾åº¦ç‰ˆæœ¬ç›®å½•ç»“æ„
- âœ… è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£å’Œéƒ¨ç½²æŒ‡å—
- âœ… å¿«é€Ÿæµ‹è¯•è„šæœ¬
- ğŸ”§ é€‚åˆå­¦ä¹ å’Œç†è§£æµç¨‹

**é€‚ç”¨åœºæ™¯**: å­¦ä¹ æ¼”ç¤ºï¼Œç¯å¢ƒå—é™æ—¶ä½¿ç”¨

### 3. `practical_model_export_demo.py` - å®ç”¨å¯¼å‡º
**å¹³è¡¡åŠŸèƒ½å’Œå®ç”¨æ€§**

- âœ… ä½¿ç”¨ç°æœ‰è‡ªç ”æ¨¡å—
- âœ… æ™ºèƒ½é™çº§å¤„ç†ï¼ˆä¾èµ–ä¸å¯ç”¨æ—¶ï¼‰
- âœ… å®é™…é‡åŒ–å¯¼å‡º
- âœ… ä¸­æ–‡èƒ½åŠ›éªŒè¯
- âœ… ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹
- ğŸ¯ æ¨èæ—¥å¸¸ä½¿ç”¨

**é€‚ç”¨åœºæ™¯**: æ—¥å¸¸å¼€å‘ï¼Œå®é™…æ¨¡å‹å¯¼å‡º

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

**åŸºç¡€è¦æ±‚**:
```bash
pip install torch>=2.0.0
```

**å®Œæ•´åŠŸèƒ½**:
```bash
pip install torch>=2.0.0 transformers>=4.35.0 peft>=0.6.0 bitsandbytes>=0.41.0 accelerate>=0.20.0
```

### è¿è¡Œæ¼”ç¤º

#### æ–¹æ³•1: æ¨èä½¿ç”¨å®ç”¨å¯¼å‡ºæ¼”ç¤º
```bash
python practical_model_export_demo.py
```

#### æ–¹æ³•2: ç®€åŒ–æ¼”ç¤ºï¼ˆæ— éœ€å¤æ‚ä¾èµ–ï¼‰
```bash
python simple_checkpoint_merge_demo.py
```

#### æ–¹æ³•3: å®Œæ•´åŠŸèƒ½æ¼”ç¤º
```bash
python demo_checkpoint_merge_and_export.py
```

### è‡ªå®šä¹‰å‚æ•°

æ‰€æœ‰æ¼”ç¤ºç¨‹åºéƒ½æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼š

```bash
# æŒ‡å®šæ£€æŸ¥ç‚¹è·¯å¾„
python practical_model_export_demo.py --checkpoint_path "your/checkpoint/path"

# æŒ‡å®šåŸºç¡€æ¨¡å‹
python practical_model_export_demo.py --base_model "Qwen/Qwen3-4B-Thinking-2507"

# æŒ‡å®šè¾“å‡ºç›®å½•
python practical_model_export_demo.py --output_dir "custom_output"
```

## æ£€æŸ¥ç‚¹è¦æ±‚

### LoRAæ£€æŸ¥ç‚¹ç»“æ„
```
qwen3_4b_thinking_output/final_model/
â”œâ”€â”€ adapter_config.json      # LoRAé…ç½®
â”œâ”€â”€ adapter_model.safetensors # LoRAæƒé‡
â”œâ”€â”€ tokenizer.json           # åˆ†è¯å™¨
â”œâ”€â”€ tokenizer_config.json    # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ special_tokens_map.json  # ç‰¹æ®Štokenæ˜ å°„
â””â”€â”€ ...                      # å…¶ä»–é…ç½®æ–‡ä»¶
```

### å®Œæ•´æ¨¡å‹æ£€æŸ¥ç‚¹ç»“æ„
```
checkpoint_directory/
â”œâ”€â”€ pytorch_model.bin        # æ¨¡å‹æƒé‡
â”œâ”€â”€ config.json             # æ¨¡å‹é…ç½®
â”œâ”€â”€ tokenizer.json          # åˆ†è¯å™¨
â””â”€â”€ ...                     # å…¶ä»–æ–‡ä»¶
```

## è¾“å‡ºç»“æœ

### ç›®å½•ç»“æ„
```
output_directory/
â”œâ”€â”€ merged_model/           # åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
â”œâ”€â”€ fp16/                  # FP16ç²¾åº¦ç‰ˆæœ¬
â”œâ”€â”€ int8/                  # INT8é‡åŒ–ç‰ˆæœ¬
â”œâ”€â”€ int4/                  # INT4é‡åŒ–ç‰ˆæœ¬
â”œâ”€â”€ export_report.json     # å¯¼å‡ºæŠ¥å‘Š
â”œâ”€â”€ deployment_guide.md    # éƒ¨ç½²æŒ‡å—
â”œâ”€â”€ usage_examples.py      # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â””â”€â”€ test_model.py         # å¿«é€Ÿæµ‹è¯•è„šæœ¬
```

### é‡åŒ–æ ¼å¼è¯´æ˜

| æ ¼å¼ | å†…å­˜ä½¿ç”¨ | æ¨ç†é€Ÿåº¦ | ç²¾åº¦ä¿æŒ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|----------|----------|
| FP16 | ~8GB | åŸºå‡† | 100% | é«˜ç²¾åº¦è¦æ±‚ |
| INT8 | ~4GB | 1.5-2x | ~98% | å¹³è¡¡æ€§èƒ½ |
| INT4 | ~2GB | 2-3x | ~95% | èµ„æºå—é™ |

## ä½¿ç”¨å¯¼å‡ºçš„æ¨¡å‹

### åŸºç¡€åŠ è½½
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½FP16æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "output_directory/fp16",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "output_directory/fp16",
    trust_remote_code=True
)
```

### é‡åŒ–æ¨¡å‹åŠ è½½
```python
from transformers import BitsAndBytesConfig

# INT8é‡åŒ–
int8_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(
    "output_directory/int8",
    quantization_config=int8_config,
    device_map="auto",
    trust_remote_code=True
)

# INT4é‡åŒ–
int4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
model = AutoModelForCausalLM.from_pretrained(
    "output_directory/int4",
    quantization_config=int4_config,
    device_map="auto",
    trust_remote_code=True
)
```

### æ¨ç†ç¤ºä¾‹
```python
# åŸºç¡€å¯¹è¯
prompt = "ä»€ä¹ˆæ˜¯AESåŠ å¯†ç®—æ³•ï¼Ÿ"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

# æ·±åº¦æ€è€ƒæ¨ç†
thinking_prompt = "<thinking>æˆ‘éœ€è¦åˆ†æè¿™ä¸ªå¯†ç å­¦é—®é¢˜</thinking>è¯·è§£é‡ŠRSAç®—æ³•çš„å·¥ä½œåŸç†ã€‚"
inputs = tokenizer(thinking_prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=300, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. ä¾èµ–åº“ç¼ºå¤±
```bash
# é”™è¯¯: ModuleNotFoundError: No module named 'transformers'
pip install transformers peft torch bitsandbytes accelerate
```

#### 2. CUDAå†…å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆ1: ä½¿ç”¨æ›´å°çš„é‡åŒ–æ ¼å¼
python practical_model_export_demo.py --formats int4

# è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨CPU
export CUDA_VISIBLE_DEVICES=""
python practical_model_export_demo.py
```

#### 3. æ£€æŸ¥ç‚¹è·¯å¾„é”™è¯¯
```bash
# æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
ls -la qwen3_4b_thinking_output/final_model/

# ä½¿ç”¨ç»å¯¹è·¯å¾„
python practical_model_export_demo.py --checkpoint_path "/absolute/path/to/checkpoint"
```

#### 4. æƒé™é—®é¢˜
```bash
# ç¡®ä¿è¾“å‡ºç›®å½•æœ‰å†™æƒé™
chmod 755 output_directory
```

### è°ƒè¯•æ¨¡å¼

å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### æ€§èƒ½ä¼˜åŒ–

#### å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–ï¼ˆINT4è€ŒéINT8ï¼‰
- å¯ç”¨CPU offloading
- å‡å°batch_size

#### é€Ÿåº¦ä¼˜åŒ–
- ä½¿ç”¨GPUæ¨ç†
- å¯ç”¨æ··åˆç²¾åº¦
- è€ƒè™‘TensorRTä¼˜åŒ–

## æŠ€æœ¯ç»†èŠ‚

### LoRAåˆå¹¶åŸç†
1. åŠ è½½åŸºç¡€æ¨¡å‹æƒé‡
2. åŠ è½½LoRAé€‚é…å™¨æƒé‡
3. å°†LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹
4. ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹

### é‡åŒ–æŠ€æœ¯
- **INT8**: ä½¿ç”¨BitsAndBytesåº“è¿›è¡Œ8ä½é‡åŒ–
- **INT4**: ä½¿ç”¨NF4é‡åŒ–ç®—æ³•
- **GPTQ**: åŸºäºæ¢¯åº¦çš„åè®­ç»ƒé‡åŒ–
- **åŠ¨æ€é‡åŒ–**: PyTorchåŸç”ŸåŠ¨æ€é‡åŒ–

### ä¸­æ–‡èƒ½åŠ›éªŒè¯
- ä¸­æ–‡å­—ç¬¦ç¼–ç å‡†ç¡®æ€§æµ‹è¯•
- å¯†ç å­¦ä¸“ä¸šæœ¯è¯­ä¿æŒæµ‹è¯•
- æ€è€ƒç»“æ„å®Œæ•´æ€§éªŒè¯
- è¯­ä¹‰è¿è´¯æ€§è¯„ä¼°

## æ‰©å±•åŠŸèƒ½

### è‡ªå®šä¹‰é‡åŒ–é…ç½®
```python
from src.model_exporter import QuantizationConfig, QuantizationFormat, QuantizationBackend

custom_config = QuantizationConfig(
    format=QuantizationFormat.INT4,
    backend=QuantizationBackend.BITSANDBYTES,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)
```

### æ‰¹é‡å¤„ç†
```python
# æ‰¹é‡å¯¼å‡ºå¤šä¸ªæ£€æŸ¥ç‚¹
checkpoints = [
    "checkpoint1/path",
    "checkpoint2/path", 
    "checkpoint3/path"
]

for i, checkpoint in enumerate(checkpoints):
    output_dir = f"batch_export_{i}"
    # è¿è¡Œå¯¼å‡º...
```

### APIæœåŠ¡éƒ¨ç½²
```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class GenerationRequest(BaseModel):
    prompt: str
    max_length: int = 200

@app.post("/generate")
async def generate_text(request: GenerationRequest):
    # ä½¿ç”¨å¯¼å‡ºçš„æ¨¡å‹è¿›è¡Œæ¨ç†
    pass
```

## è´¡çŒ®æŒ‡å—

### æ·»åŠ æ–°çš„é‡åŒ–æ ¼å¼
1. åœ¨`QuantizationFormat`æšä¸¾ä¸­æ·»åŠ æ–°æ ¼å¼
2. åœ¨`ModelQuantizer`ä¸­å®ç°é‡åŒ–é€»è¾‘
3. æ·»åŠ ç›¸åº”çš„é…ç½®ç±»
4. æ›´æ–°æ–‡æ¡£å’Œæµ‹è¯•

### æ”¹è¿›ä¸­æ–‡éªŒè¯
1. æ‰©å±•æµ‹è¯•ç”¨ä¾‹é›†
2. æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡
3. ä¼˜åŒ–éªŒè¯ç®—æ³•
4. å¢åŠ ä¸“ä¸šé¢†åŸŸæµ‹è¯•

## è®¸å¯è¯

æœ¬æ¼”ç¤ºç¨‹åºéµå¾ªé¡¹ç›®ä¸»è®¸å¯è¯ã€‚

## æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹ç”Ÿæˆçš„`export_report.json`
2. æ£€æŸ¥å„ç›®å½•ä¸‹çš„READMEæ–‡ä»¶
3. è¿è¡Œ`test_model.py`è¿›è¡Œè¯Šæ–­
4. æäº¤Issueå¹¶é™„ä¸Šé”™è¯¯æ—¥å¿—

---

**æœ€åæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ç‰ˆæœ¬**: v1.0.0